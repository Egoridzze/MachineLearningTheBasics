% !Mode:: "TeX:DE:UTF-8:Main"
%
%
%JOURNAL CODE  SEE DOCUMENTATION

\documentclass[12pt]{report} 
%\usepackage[demo]{graphicx} 
\usepackage{bmpsize}
\usepackage{setspace}
\usepackage{bm}
\usepackage{amsmath}
\usepackage[square,numbers]{natbib}
\bibliographystyle{abbrv}
\usepackage[utf8]{inputenc}
\usepackage{fullpage}
\usepackage{url}
\usepackage[symbols]{glossaries-extra}
\usepackage{amsmath}
\usepackage{subfigure}
\usepackage{pgfplotstable}
\usetikzlibrary{matrix,chains,positioning,decorations.pathreplacing,arrows}
\usepackage{listings}
\usepackage{bm,amssymb}
\usepackage{algorithm, float}
%\usepackage{hyperlink}
\usepackage{algpseudocode}
\floatname{algorithm}{Algorithm}
\usepackage[pdf]{pstricks}
\usepackage{pst-plot}
\usepackage{pst-node}
\usepackage{pst-tree}
\usepackage{pstricks-add}
\usepackage{csvsimple}
\usepackage{xinttools} % for the \xintFor***
\usepackage{tikz}
\usetikzlibrary{calc}
\usetikzlibrary{shapes.misc}
\usepackage{booktabs}
\usepackage{hyperref}
%\usepackage[demo]{graphicx}
\pdfminorversion=5 
\pdfcompresslevel=9
\pdfobjcompresslevel=5

\tikzset{cross/.style={cross out, draw=black, minimum size=2*(#1-\pgflinewidth), inner sep=0pt, outer sep=0pt},
%default radius will be 1pt. 
cross/.default={1pt}}

\makeglossaries
\glsxtrnewsymbol % requires glossaries-extra.sty 'symbols' option
 [description={the set containing no elements}]
 {emptyset}% label (and sort value)
 {\ensuremath{\emptyset}}% symbol
%\usepackage[applemac]{inputenc}

%\newtheorem{theorem}{Theorem}%[section]
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{example}[theorem]{Example}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{postulate}[theorem]{Postulate}
%\newtheorem{corollary}[theorem]{Corollary}
\usetikzlibrary{trees}
\newcommand{\sizehypospace}{D}
\newcommand{\featurelen}{\featuredim}
\usetikzlibrary{arrows,automata,snakes}
\usetikzlibrary{calc,trees,positioning,arrows,chains,shapes.geometric,%
decorations.pathreplacing,decorations.pathmorphing,shapes,%
matrix,shapes.symbols,automata}

\lstdefinelanguage{Python}{
numbers=left,
numberstyle=\footnotesize,
numbersep=1em,
xleftmargin=1em,
framextopmargin=2em,
framexbottommargin=2em,
showspaces=false,
showtabs=false,
showstringspaces=false,
frame=l,
tabsize=4,
% Basic
basicstyle=\ttfamily\small\setstretch{1},
backgroundcolor=\color{Background},
% Comments
commentstyle=\color{Comments}\slshape,
% Strings
stringstyle=\color{Strings},
morecomment=[s][\color{Strings}]{"""}{"""},
morecomment=[s][\color{Strings}]{'''}{'''},
% keywords
morekeywords={import,from,class,def,for,while,if,is,in,elif,else,not,and,or,print,break,continue,return,True,False,None,access,as,,del,except,exec,finally,global,import,lambda,pass,print,raise,try,assert},
keywordstyle={\color{Keywords}\bfseries},
% additional keywords
morekeywords={[2]@invariant,pylab,numpy,np,scipy},
keywordstyle={[2]\color{Decorators}\slshape},
emph={self},
emphstyle={\color{self}\slshape},
%
}

% Python style for highlighting
\newcommand\pythonstyle{\lstset{
language=Python,
basicstyle=\ttm,
otherkeywords={self},             % Add keywords here
keywordstyle=\ttb\color{deepblue},
emph={MyClass,__init__},          % Custom highlighting
emphstyle=\ttb\color{deepred},    % Custom highlighting style
stringstyle=\color{deepgreen},
frame=tb,                         % Any extra options here
showstringspaces=false            % 
}}


\newcommand{\boundellipse}[3]% center, xdim, ydim
{(#1) ellipse (#2 and #3)
}
\def\biglen{20cm} % playing role of infinity (should be < .25\maxdimen)
% define the "half plane" to be clipped (#1 = half the distance between cells)
\tikzset{
  half plane/.style={ to path={
       ($(\tikztostart)!.5!(\tikztotarget)!#1!(\tikztotarget)!\biglen!90:(\tikztotarget)$)
    -- ($(\tikztostart)!.5!(\tikztotarget)!#1!(\tikztotarget)!\biglen!-90:(\tikztotarget)$)
    -- ([turn]0,2*\biglen) -- ([turn]0,2*\biglen) -- cycle}},
  half plane/.default={1pt}
}

\def\n{23} % number of random points
\def\maxxy{4} % random points are in [-\maxxy,\maxxy]x[-\maxxy,\maxxy]


% Python environment
\lstnewenvironment{python}[1][]
{
\pythonstyle
\lstset{#1}
}
{}

% Python for external files
\newcommand\pythonexternal[2][]{{
\pythonstyle
\lstinputlisting[#1]{#2}}}

% Python for inline
\newcommand\pythoninline[1]{{\pythonstyle\lstinline!#1!}}

\tikzstyle{mybox} = [draw=red,very thick,
    rectangle, rounded corners, inner sep=10pt, inner ysep=20pt]
\tikzstyle{fancytitle} =[fill=red, text=black]

%\tikzset{
% >=stealth',
% punktchain/.style={
% rectangle, 
% rounded corners, 
%% fill=black!10,
%draw=black, very thick,
%text width=10em, 
%minimum height=3em, 
%text centered, 
%on chain},
% line/.style={draw, thick, <-},
%  element/.style={
%    tape,
%    top color=white,
%    bottom color=blue!50!black!60!,
%    minimum width=8em,
%      draw=blue!40!black!90, very thick,
%      text width=10em, 
%      minimum height=3.5em, 
%      text centered, 
%      on chain},
%      every join/.style={->, thick,shorten >=1pt},
%      decoration={brace},
%      tuborg/.style={decorate},
%      tubnode/.style={midway, right=2pt},
%      every rectangle node/.style={color=black,thick}
%     treenode/.style = {shape=rectangle, rounded corners,
%                     draw, align=center,
%                     top color=white, bottom color=blue!20},
%  root/.style     = {treenode, font=\Large, bottom color=red!30},
%  env/.style      = {treenode, font=\ttfamily\normalsize},
%  dummy/.style    = {circle,draw}
%    }
    
\tikzset{
  treenode/.style = {shape=rectangle, rounded corners,
                     draw, align=center,
                     top color=white, bottom color=blue!20},
  root/.style     = {treenode, font=\Large, bottom color=red!30},
  env/.style      = {treenode, font=\ttfamily\normalsize},
  dummy/.style    = {circle,draw}
}

 \newglossaryentry{cabbage}{name={cabbage},
  description={vegetable with thick green or purple leaves}}
 \newglossaryentry{turnip}{name={turnip},
  description={round pale root vegetable}}
 \newglossaryentry{carrot}{name={carrot},
  description={orange root}}

%\begin{filecontents*}{data.csv}
%a,b
%-3,0
%-1,0
%0,1
%1,0
%3,0
%\end{filecontents*}

\tikzset{
	%Define standard arrow tip
	>=stealth',
	%Define style for boxes
	punkt/.style={
		rectangle,
		rounded corners,
		draw=black, very thick,
		text width=3cm,
		minimum height=2em,
		text centered},
	% Define arrow style
	pil/.style={
		->,
		thick,
		shorten <=2pt,
		shorten >=2pt,}
}

\usetikzlibrary{arrows,automata,snakes}
\usetikzlibrary{calc,trees,positioning,arrows,chains,shapes.geometric,%
	decorations.pathreplacing,decorations.pathmorphing,shapes,%
	matrix,shapes.symbols,automata}

\tikzset{
	treenode/.style = {shape=rectangle, rounded corners,
		draw, align=center,
		top color=white, bottom color=blue!20},
	root/.style     = {treenode, font=\Large, bottom color=red!30},
	env/.style      = {treenode, font=\ttfamily\normalsize},
	dummy/.style    = {circle,draw}
}


\input{mlbpbook_macros}



% a few definitions that are *not* needed in general:
\newcommand{\ie}{\emph{i.e.}}
\newcommand{\eg}{\emph{e.g.}}
\newcommand{\etc}{\emph{etc}}
\newcommand{\now}{\textsc{now}}
\newcommand{\vh}{\mathbf{h}}

%ARTICLE TITLE
%\title{Machine Learning: Basic Principles}


%AUTHORS FOR COVER PAGE 
% separate authors by \and, item by \\
% Don't use verbatim or problematic symbols.
% _ in mail address should be entered as \_
% Pay attention to large mail-addresses ...

\font\myfont=cmr12 at 30pt
\font\myfonta=cmr12 at 20pt


%\title{\myfont Machine Learning: Basic Principles}  % Supply information
%\author{Alexander Jung}              %   for the title page.
%\date{\today}                           %   Use current date.

\onehalfspacing
\begin{document}

%\maketitle                              % Print title page.
\pagestyle{plain}
\vspace*{5mm}
\begin{center}
\noindent {\bf {\myfont Machine Learning: The Basics}} \\[2mm]
\noindent {\bf {\myfonta !! ROUGH DRAFT !!}} \\[2mm]
%\noindent {\bf { Department of Computer Science, Aalto University} } \\[5mm]
\noindent {Alexander Jung} \\[2mm]
\noindent {\today} \\[20mm]
%\begin{figure}[ht] 
%\centering
%%% \includegraphics[height=5.2cm,width=9.5cm]{numerical_results.eps}
%\includegraphics[width=0.9\textwidth]{ThePrinciple.eps}
% \caption{Machine learning methods implement the scientific principle of 
% 	iteratively validating and refining a hypothesis (or model) about 
% 	some ``real-world'' phenomenon.} %%  and $\sigma^{2}=1$
%%\label{fig_ROC}
%\label{fig_AlexMLBP}
%\end{figure}
\begin{figure}[htbp]
	\centering
	\hspace*{5mm}
	\begin{tikzpicture}[node distance=1cm, auto,]
	\coordinate (OR) at (0.00, 1.50);
	%\cercle {OR} {3cm} {-150} {120} {1.0} {red};
	\node[circle,inner sep=0,minimum size={6cm}](a) at (OR) {};
	\node[circle,inner sep=0,minimum size={8cm}](b) at (OR) {};
	\node[circle,inner sep=0,minimum size={8cm}](c) at (OR) {};
	\draw[red,line width=1,dashed] (a.-150) arc (-150:{-150+120}:3cm);
	\draw[blue,line width=1,dashed] (b.160) arc (160:{160+50}:4cm);
	\draw[black!30!green,line width=2,dashed] (c.20) arc (20:{160}:4cm);
	\draw[black!30!blue,line width=2,dashed] ([shift={(-30:6cm)}]OR) arc (-30:{20}:6cm);
	\draw[black!30!blue,line width=2,dashed] (OR) --([shift={(-30:6cm)}]OR);
	\draw[black!30!blue,line width=2,dashed] (OR) --([shift={(20:6cm)}]OR);
	\draw[black!30!green,line width=2,dashed] (OR) -- (c.20);
	\draw[black!30!green,line width=2,dashed] (OR) -- (c.160);
	\draw[blue,thin,dashed] (OR) -- (b.160);
	\draw[blue,thin,dashed] (OR) -- (b.210);
	\draw[red,thin,dashed] (OR) -- (a.-150);
	\draw[red,thin,dashed] (OR) -- (a.-30);   
	\node[punkt] (data) {observations};
	\node[red,below=0.5cm of data] (data1) {data};
	\node[above=of data] (dummy) {};
	\node[punkt,above=1cm of dummy] (hypothesis) {hypothesis};
	\node[right=1.4cm of dummy] (t) {make prediction} ; 
	\node[left=1.4cm of dummy] (g) {validate/adapt} ;
	\node[blue,below=0.5cm of g,anchor=east] (g1) {loss} ; 
	\node[black!30!blue,below=1cm of t,anchor=west] (g6) {inference} ; 
	\node[black!30!green,above=1cm of hypothesis,anchor=south] (g3) {model} ; 
	\draw [->,line width=0.5mm] (hypothesis.east) to [out=0,in=90] (t.north);
	\draw [->,line width=0.5mm] (t.south) to [out=270,in=0] (data.east);
	\draw [->,line width=0.5mm] (data.west) to [out=180,in=270] (g.south);
	\draw [->,line width=0.5mm] (g.north) to [out=90,in=180] (hypothesis.west);
	%   edge[pil, bend right=45] (data.west)
	%edge[pil, bend right=45] (formidler.west)
	%  edge[pil,<->, bend left=45] node[auto] {Direct (a)} (t)
	\end{tikzpicture}
	\vspace*{-13mm}
	\caption{
		Machine learning methods implement the scientific principle of 
		iteratively validating and refining a hypothesis (or model) about 
		some ``real-world'' phenomenon. This principle involves 
		some data, some model (or hypothesis space) and some loss function.}
	\label{fig_AlexMLBP}
\end{figure}
\end{center}



\newpage
\chapter*{Preface}

Machine learning (ML) has become a commodity in our every-day lives. 
We routinely ask ML empowered smartphones to suggest lovely food places 
or to guide us through a strange place. ML methods have also 
become standard tools in many fields of science and engineering. A plethora 
of ML applications is driving a revolution of human lives at 
unprecedented pace and scale. 

This book portrays machine learning methods as the combination of three basic 
components: data, model and loss. ML methods combine these three components 
as computationally efficient implementations of the basic scientific principle ``trial and error''. 
Trial and error amounts to continuously adapting a model about some data generating 
phenomenon by minimizing some notion of loss. A plethora of different ML methods 
is obtained by combining different design choices for the data representation, model and loss.  

Machine learning methods differ vastly in their actual implementations 
which might obscure their unifying basic principles. Deep learning methods 
use cloud computing frameworks to train large models on huge online 
datasets. Operating on a much finer granularity for data and computation, 
linear least squares regression can be implemented on small embedded 
systems. However, deep learning methods and linear regression use 
the same principle of iteratively updating a model based on the discrepancy 
between model predictions and actual observed data. 

The three component picture of ML championed in this book allows a 
unified treatment of a wide range of concepts and techniques 
which seem quite unrelated at first sight. On a low-level, we discuss 
the regularization effect of early stopping in terms of adjusting the effective 
model space. On a higher-level, we can interpret privacy-preserving 
and explainable machine learning as particular design choices for the model, data and loss. 

To make good use of widely available ML tools it is instrumental 
to understand its underlying principles at different levels of detail. 
On a lower-level, this tutorial helps machine learning engineers to 
choose suitable methods for the application at hand. The book also 
provides leaders a higher-level view on the development of ML 
which is required to manage a machine learning team. 
We believe that thinking about ML using this three components 
helps to navigate the steadily growing offer for ready-to-use ML methods. 

\section*{Acknowledgement}
This tutorial is based on lecture notes prepared for the 
courses CS-E3210 ``Machine Learning: Basic Principles'', 
CS-E4800 ``Artificial Intelligence'', CS-EJ3211 ``Machine 
Learning with Python'', CS-EJ3311 ``Deep Learning with Python'' 
and CS-C3240 ``Machine Learning'' offered at Aalto 
University and within the Finnish university network 
\url{fitech.io}. This tutorial is accompanied by practical 
implementations of ML methods in MATLAB and 
Python \url{https://github.com/alexjungaalto/}. 

This text benefited from the numerous feedback of the 
students within the courses that have been (co-)taught 
by the author. The author is indebted to Shamsiiat Abdurakhmanova, 
Tomi Janhunen, Yu Tian, Natalia Vesselinova, Ekaterina Voskoboinik, 
Buse Atli, Stefan Mojsilovic for carefully reviewing early 
drafts of this tutorial. Some of the figures have been 
generated with the help of Eric Bach. The author is grateful 
for the feedback received from Jukka Suomela, Oleg Vlasovetc, 
Georgios Karakasidis, Joni P{\"a}{\"a}kk{\"o}, Harri Wallenius and 
Satu Korhonen. 

%\end{abstract}

\tableofcontents                        % Print table of contents


\chapter{Introduction}
Consider waking up some morning during winter in Finland 
and taking a look outside the window (see Figure \ref{fig:skiday}). 
It seems to become a nice sunny day which is ideal for a 
ski trip. To choose the right gear (clothing, wax) it is 
vital to have some idea for the maximum daytime temperature 
which is typically reached around early afternoon. If we 
expect a maximum daytime temperature of around plus 
$10$ degrees, we might not put on the extra warm jacket 
but rather take only some extra shirt for change. 


\begin{figure}[htbp]
	\centering
	\includegraphics[width=8cm]{SkiDay2.jpeg}
	\caption{Looking outside the window during the morning of a winter day in Finland.}
	\label{fig:skiday}
\end{figure}

How can we predict the maximum daytime temperature for 
the specific day depicted in Figure \ref{fig:skiday}? Let us now 
show how this can be done via ML. In a nutshell, 
ML methods are computational implementations of a simple 
(scientific) principle. 

\begin{quote}
Find a good hypothesis based on {\bf a model} for the phenomenon of interest by 
using {\bf observed data} in order to minimize {\bf a loss function}. 
\end{quote}

This principle contains three components: data, a model and a 
loss function. Any machine learning method, including linear 
regression and deep reinforcement learning, combines these 
three components. We will discuss in Chapter \ref{ch_Elements_ML} 
that each of these components incur design choices such as 
which data representation or model class to use. One such 
choice for data and model representation is to use vectors 
and matrices. The seemingly overwhelming offer for different 
ML methods is based on different design choices for data 
representation, models and loss function. 

Let us now illustrate the (rather abstract) concepts behind the main components 
of ML with the above problem of predicting the maximum daytime temperature. 
The data are weather observations collected by the Finnish metrological institute. 
We depict such data in Figure \ref{fig_scatterplot_temp_FMI}, with each dot 
representing some previous day. Each day is characterized by the minimum daytime 
temperature $x$ and the maximum daytime temperature $y$. 

\begin{figure}[htbp]
	\begin{center}
		\begin{tikzpicture}
		\tikzset{x=0.35cm,y=2cm,every path/.style={>=latex},node style/.style={circle,draw}}
		%		\csvreader[ head to column names,%
		%		late after head=\xdef\iold{\maxtmp}\xdef\xold{\mintmp},,%
		%		after line=\xdef\iold{\maxtmp}\xdef\xold{\mintmp}]%
		%		{FMIData.csv}{}
		%		{\draw [line width=0.0mm] (\iold, \xold) (\i,\x) node {\large $\circ$};
		%		}
		%	
		\begin{axis}[axis x line=none,
		axis y line=none,
		ylabel near ticks,
		xlabel near ticks,
		enlarge y limits=true,
		xmin=-40, xmax=40,
		ymin=-40, ymax=40,
		width=10cm, height=10cm, ]
		\addplot[only marks] table [x=mintmp, y=maxtmp, col sep = comma] {FMIData.csv};
		\node at (axis cs:30,1) [anchor=west] {$x$};
		\node at (axis cs:0,30) [anchor=west] {$y$};
		\draw[->] (axis cs:-30,0) -- (axis cs:32,0);
		\draw[->] (axis cs:0,-30) -- (axis cs:0,30);
		\end{axis}
		\end{tikzpicture}
		\vspace*{-14mm}
	\end{center}
	\caption{A scatterplot where each dot represents some day that is characterized 
		by its minimum daytime temperature $x$ and its maximum daytime temperature $y$.}
	\label{fig_scatterplot_temp_FMI}
	\vspace*{-3mm}
\end{figure}

ML methods allow to learn a predictor $\hat{y} = h(x)$, reading in the minimum temperature 
$x$ and delivering a prediction/estimate/approximation $\hat{y} = h(x)$ for the actual 
maximum daytime temperature $y$. This prediction is based on a simple model for how 
the minimum and maximum daytime temperature during some day are related. We assume 
that they are related approximately by a linear model: $y \approx w_{1} x + w_{0}$. Based on 
this model, it seems reasonable to restrict the ML method to only consider linear 
predictor maps $h(x) = w_{1}x +w_{0}$. 



??? Work out this prinicple for linear regression using FMI data pionts ???

Figure \ref{fig_AlexMLBP} illustrates the typical workflow of a 
ML method. Any ML method must start with some initial guess 
or choice for a good hypothesis. This initial guess might not be 
made explicit but each ML method must use such an initial guess. 
This initial guess is typically based on some prior knowledge or 
domain expertise \cite{MitchellBias1980}. 

Based on a current hypothesis, ML methods make predictions or forecasts about future 
observations. These observations are in the form of data collected by some hard or software. 
The discrepancy between the predictions and the actual observations is used to improve the
hypothesis. Learning happens during improving the current hypothesis based on the 
discrepancy between its predictions and the actual observations. 


 
A prominent example of a phenomenon is the weather. Forecasting the weather is 
based on learning or fitting models to massive amounts of weather observations.  
The Finnish Meteorological Institute collects weather measurements at different 
observation stations distributed all over Finland. We can download the history of 
minimum and maximum daytime temperature recorded by the station nearest to 
our home location. Let us denote the resulting dataset by 
\begin{equation}
 z^{(1)},\ldots,z^{(\samplesize)}\}.  
\end{equation} 
Each data point $\vz^{(i)}$ represents some previous day for which the 
minimum and maximum daytime temperature $x^{(i)}$ and $y^{(i)}$ has been recorded. 
Thus, we can represent such a day by these measurements only. We 
will use the minimum daytime temperature $x^{(i)}$ as the feature and the 
maximum daytime temperature $y^{(i)}$ as the label of the data point $\vz^{(i)}$. 
%In general we consider features being any property of a datapoint that can be 
%determined without human intervention. In contrast, labels are properties of 
%datapoints that require human expert knowledge or domain expertise. 

Consider waking up on some day $i$. We can easily determine the minimum temperature 
since this is what we can measure in the morning. However, we cannot easily determine 
the maximum day temperature as this will be attained only a few hours later in 
early afternoon. However, for this application we can get labeled datapoint, i.e., for 
which we know the true label by using historic recordings. One main source for 
labeled data is by looking into the past. 




Machine learning (ML) uses data to learn models that allow, in turn, to make predictions of forecasts. 
The models used in ML come in the form of assumptions or 
hypothesis for the properties of observed data. Consider the data point $\vz$ representing some 
day with minimum temperature $x$ and maximum temperature $y$. Then, 
it seems reasonable to assume that these two properties of the same day are 
correlated. 

For a higher minimum temperature $x^{(i)}$, we typically expect a higher 
maximum daytime temperature $y$. We can formalize this assumption by 
modelling the maximum daytime temperature as $y \approx h(x)$ with some 
hypothesis map $h$ which is monotonically increasing. One candidate for 
such a map is a linear function of the form
\begin{equation} 
\label{equ_def_linear_map}
h(x) = w_{1} x + w_{0} \mbox{ with some weights } w_{1},w_{0} \in \mathbb{R}. 
\end{equation}
The map \eqref{equ_def_linear_map} is monotonically increasing whenever $w_{1}\!>\!0$. 
Note that \eqref{equ_def_linear_map} defines a whole ensemble of hypothesis maps, 
each individual map corresponding to a particular choice for $w_{1} \geq 0$ and $w_{0}$. 
We say that the map \eqref{equ_def_linear_map} is parametrized by the weight vector 
$\mathbf{w}= \big(w_{1},w_{0}\big)$ and indicate this by writing $h^{(\mathbf{w})}$. 
For a given weight vector $\mathbf{w}= \big(w_{1},w_{0}\big)$, we obtain the map 
$h^{(\mathbf{w})}(x) = w_{1}x +w_{0}$. 
Figure \ref{fig_three_maps_example} depicts three particular maps $h^{(\mathbf{w})}$ obtained 
for three different choices for the weights $\mathbf{w}$.  
\begin{figure}[htbp]
	\begin{center}
		\begin{tikzpicture}
		\begin{axis}
		[ylabel=$h(x)$,
		xscale=1,
		xlabel=$x$, 
		axis x line=center,
		axis y line=center,
		xtick={-3,-2,-1,0,1,2,3},
		ytick={-3,-2,-1,0,1,2,3},
		xlabel={feature $x$},
		ylabel={$h^{(\mathbf{w})}(x)$},
		xlabel style={right},
		ylabel style={above},
		xmin=-5,
		xmax=5,
		ymin=-5,
		ymax=5
		]
	  \addplot[green, ultra thick] (x,4*x);
		  \addplot[red, ultra thick] (x,1*x+2);	
		    \addplot[blue, ultra thick] (x,2*x+3);	
	%	\addplot [smooth, color=red, ultra thick] table [x=a, y=b, col sep=comma] {polynomials.csv};    
	%	\addplot [color=blue, ultra thick] table [x=a, y=c, col sep=comma] {polynomials.csv};     
	%	\addplot [color=black, thick] table [x=a, y=d, col sep=comma] {polynomials.csv};      
		\end{axis}
	%	\node [right,color=red] at (5.5,1.5) {$h^{(\mathbf{w}_{1})}(x) = 7(x^2 - x^3)$};
%		\node [right,color=blue] at (2.3,0.5) {$h^{(\mathbf{w}_{2})}(x) =x^2$};
%		\node [right] at (2.5,2.8) {$h^{(\mathbf{w}_{3})}(x)\!=\!x$};
		\end{tikzpicture}
		\vspace*{-4mm}
	\end{center}
	\caption{Three particular hypothesis maps of the form \eqref{equ_def_linear_map}.}
	\label{fig_three_maps_example}
\end{figure}


ML would be trivial if there is only one single hypothesis. Having 
only a single hypothesis means that there is no need to try out 
different hypotheses to find the best one. To enable (machine) 
learning we need to allow for more than one hypothesis. ML allows 
to choose (learn) a good hypothesis out of (typically very large) 
hypothesis spaces. The hypothesis space constituted by the 
maps \eqref{equ_def_linear_map} for different weights is uncountably 
infinite. 

To find, or {\bf learn}, a good hypothesis we need to somehow 
assess the quality of a given hypothesis map. ML methods use 
data for this purpose. These methods assess the quality of a 
hypothesis by comparing its predictions with actual observed 
data. 

Consider the hypothesis map \eqref{equ_def_linear_map} for 
predicting the maximum daytime temperature based on the 
minimum (morning) daytime temperature $x$. We can evaluate 
the quality of this map by comparing its predictions with actual 
observed minimum and maximum temperature for previous days. 
Figure \ref{fig_scatterplot_temp_FMI} depicts a set of data points 
represented earlier days characterized by minimum and maximum 
temperature. These data points can been downloaded from the 
Finnish meteorological institute (FMI). 


\section{Linear Algebra} 

Modern ML methods are computationally efficient methods to fit high-dimensional 
models to large amounts of data. The models underlying state-of-the art 
ML methods can contain billions of tunable or learnable parameters. To 
make ML methods computationally efficient we need to use suitable representations 
for data and models. 

Maybe the most important mathematical structure that provides such a 
representation is the Euclidean space $\mathbb{R}^{n}$ with some dimension 
$n$. The rich algebraic and geometric structure of $\mathbb{R}^{n}$ allows 
for the design of ML algorithms that can process vast amounts of data to 
quickly update a model (parameters). 

The scatter plot in Figure \ref{fig_scatterplot_temp_FMI} depicts 
data points (individual days) using vectors $\vz \in \mathbb{R}^{2}$. 
We obtain the vector representation $\vz=(x,y)^{T}$ of a particular 
day is obtained by stacking the minimum daytime temperature $x$ 
and the maximum daytime temperature $y$ into a vector $\vz$ of 
length two. 

We can use the Euclidean space $\mathbb{R}^{n}$ not only to represent 
data points but also models for the data. One such class of models is 
obtained by linear subsets of $\mathbb{R}^{n}$, such as those depicted 
in Figure \ref{fig_three_maps_example}. We can then use the geometric 
structure of $\mathbb{R}^{n}$, defined by the Euclidean norm, to search 
for the best model. As an example, we could search for the linear model, 
represented by a straight line, such thatthe average distance to the data 
points in Figure \ref{fig_scatterplot_temp_FMI} is smallest 
(see Figure \ref{fig_scatterplot_temp_FMI_linalg}). 

The properties of linear structures, such as straight lines, is studied within linear 
algebra \cite{StrangLinAlg2016}. The basic principles behind important ML methods, 
such as linear regression or principal component analysis, are deeply rooted in the 
theory of linear algebra (see Sections \ref{sec_lin_regression} and \ref{sec_pca}).


\begin{figure}[htbp]
	\begin{center}
		\begin{tikzpicture}
		\tikzset{x=0.35cm,y=2cm,every path/.style={>=latex},node style/.style={circle,draw}}
		%		\csvreader[ head to column names,%
		%		late after head=\xdef\iold{\maxtmp}\xdef\xold{\mintmp},,%
		%		after line=\xdef\iold{\maxtmp}\xdef\xold{\mintmp}]%
		%		{FMIData.csv}{}
		%		{\draw [line width=0.0mm] (\iold, \xold) (\i,\x) node {\large $\circ$};
		%		}
		%	
		\begin{axis}[axis x line=none,
		axis y line=none,
		ylabel near ticks,
		xlabel near ticks,
		enlarge y limits=true,
		xmin=-40, xmax=40,
		ymin=-40, ymax=40,
		width=10cm, height=10cm, ]
		\addplot[only marks] table [x=mintmp, y=maxtmp, col sep = comma] {FMIData.csv};
		\node at (axis cs:30,1) [anchor=west] {$x$};
		\node at (axis cs:0,30) [anchor=west] {$y$};
		\draw[->] (axis cs:-30,0) -- (axis cs:32,0);
		\draw[->] (axis cs:0,-30) -- (axis cs:0,30);
		\draw[red,line width=3] (axis cs:-30,-10) -- (axis cs:20,25);
	%	\draw [red,rotate=40,thick] \boundellipse{axis cs:10,-36}{300}{50} node[left,xshift=3mm,yshift=10mm]  {$\,\,p(\vz)$}; 
		%	\fill (axis cs:0,0) circle (2pt) ; 
		%	\node [right] at (axis cs:-9,8) {$ {\bm \Sigma}^{(3)}$} ; 
		\end{axis}
		\end{tikzpicture}
		\vspace*{-14mm}
	\end{center}
	\caption{Dots represent earlier days characterized by its minimum daytime temperature $x$ 
		and its maximum daytime temperature $y$. We also depict a straight line as a simple model 
		for the inherent structure in the data. 
	  }
	\label{fig_scatterplot_temp_FMI_linalg}
	\vspace*{-3mm}
\end{figure}


\newpage
\section{Optimization} 

A main design principle for ML methods is to formulate learning tasks as  
optimization problems \cite{OptMLBook}. The weather prediction problem 
above can be formulated as the problem of optimizing (minimizing) the 
prediction error for the maximum daytime temperature.  ML methods are 
then obtained by applying optimization methods to these learning problems. 
The statistical and computational properties of such ML methods can be 
studied using tools from the theory of optimization. What sets the optimization 
problems arising in ML apart from ``standard'' optimization problems is that 
we do not have full access to the objective function to be minimized. 
Section \ref{ch_Optimization} discusses methods that are based on estimating 
the correct objective function by empirical averages that 
are computed over subsets of data points (the training set). 

\section{Theoretical Computer Science} 

On a high level, ML methods take data as input and produce some prediction as the output. 
The predictions are computed using algorithms such as linear solvers or optimization methods. 
These algorithms are implemented using some finite computational infrastructure. One example 
for such a computational infrastructure is a single desktop computer. Another example for a 
computational infrastructure is an interconnected collection of computing nodes. ML methods 
must implement their computations within the available finite computational resources such as 
time, memory or communication bandwidth.  

%A main design principle within ML is to formulate learning tasks as an optimization problem. 
%ML methods are then obtained by applying optimization methods to these learning problems. 
Therefore, engineering efficient ML methods requires a good understanding of algorithm 
design and their implementation on physical hardware. A huge algorithmic toolbox is provided 
by numerical linear algebra \cite{StrangLinAlg2016,Strang2007}. One of the key factors for the 
recent success of ML methods is that they use vectors and matrices to represent data and 
models. Using this representation allows to implement the resulting ML methods using highly 
efficient hard- and software implementations for numerical linear algebra. 


\section{Communication} 

We can interpret ML as a particular form of data processing. A ML 
algorithm is fed with observed data in order to adjust some model and, in turn, compute 
a prediction of some future event. Thus, ML involves transferring or communicating 
data to some computer which executes a ML algorithm. The design of efficient 
ML systems also involves the design of efficient communication between data source 
and ML algorithm. Loosely speaking, the learning progress of ML method will be 
slowed down if we cannot fed it with data at high speed. Given limited memory or storage capacity, 
being too slow to process data at their rate of arrival (in real-time) means that we need to 
``throw away'' data. The lost data might have carried relevant information for the ML task at hand. 



\section{Statistics}

%????? omit this ? ??????
%Many ML methods are based on statistical models for how data is generated. Once 
%such a statistical model is known or estimated, computing predictions amounts to 
%applying the calculus of probability theory. One important example for such a computation 
%is the Bayes rule which allows to compute the probability of a hidden variable (which 
%we want to predict) based on observed variables (the data we can feed into ML algorithms). 
%???????

Consider the datapoints depicted in Figure \ref{fig_scatterplot_temp_FMI}. Each datapoint 
represents some previous day. Each datapoint (day) is characterized by the minimum and 
maximum daytime temperature as measured by some weather observation station. It might 
be useful to interpret these datapoints as independent and identically distributed (i.i.d.) 
realizations of a random vector $\mathbf{z} =\big(x,y\big)^{T}$. The random vector $\vz$ 
is distributed according to some fixed but typically unknown probability distribution $p(\vz)$. 
Figure \ref{fig_scatterplot_temp_FMI_stat_model} extends the scatter plot of Figure \ref{fig_scatterplot_temp_FMI} 
with some contour line that indicates the probability distribution $p(\vz)$. 

%A simple but useful model for continuous-valued random vectors is provided by the  
%multivariate normal distribution. This distribution of parametrized by the mean vector 
%and the covariance matrix of the random vector. We can estimate the probability distribution 
%by estimating these two parameters based on the datapoints in Figure \ref{fig_scatterplot_temp_FMI}. 
Probability theory offers a great selection on methods for estimating the probability 
distribution from observed data (see Section \ref{sec_max_iikelihood}). Given (an estimate of) 
the probability distribution $p(\vz)$, we can compute predictions 
or estimates for the label $y$ of any data point for which we 
know its feature value $x'$. What is more, having a probability 
distribution $p(\vz)$ for a randomly drawn data point $\vz=(x,y)$, 
allows us to not only compute a single prediction (point estimate) 
$\hat{y}$ of the label $y$ but rather an entire probability distribution 
$q(\hat{y})$ over all possible prediction values $\hat{y}$. 

The distribution $q(\hat{y}$) represents, for each value $\hat{y}$, the probability or how likely it 
is that this is the true label value of the data point. By its very definition, this distribution $q(\hat{y})$ 
is precisely the conditional probability distribution $p(y|x)$ of the label value $y$, given 
the feature value $x$ of a randomly drawn data point $\vz=(x,y) \sim p(\vz)$. 

Having an (estimate of) probability distribution $p(\vz)$ for the observed 
data points not only allows us to compute predictions but also to generate 
new data points. Indeed, we can artificially augment the observed data points 
by random drawing new data point from the probability distribution $p(\vz)$ 
(see Section \ref{sec_data_augmentation}). A recently popularized class of 
ML methods that use a probabilistic models to generating synthetic data points 
is known as {\bf generative adversarial networks} \cite{GoodfellowGAN}. 



\begin{figure}[htbp]
	\begin{center}
		\begin{tikzpicture}
		\tikzset{x=0.35cm,y=2cm,every path/.style={>=latex},node style/.style={circle,draw}}
		%		\csvreader[ head to column names,%
		%		late after head=\xdef\iold{\maxtmp}\xdef\xold{\mintmp},,%
		%		after line=\xdef\iold{\maxtmp}\xdef\xold{\mintmp}]%
		%		{FMIData.csv}{}
		%		{\draw [line width=0.0mm] (\iold, \xold) (\i,\x) node {\large $\circ$};
		%		}
		%	
		\begin{axis}[axis x line=none,
		axis y line=none,
		ylabel near ticks,
		xlabel near ticks,
		enlarge y limits=true,
		xmin=-40, xmax=40,
		ymin=-40, ymax=40,
		width=10cm, height=10cm, ]
		\addplot[only marks] table [x=mintmp, y=maxtmp, col sep = comma] {FMIData.csv};
		\node at (axis cs:30,1) [anchor=west] {$x$};
		\node at (axis cs:0,30) [anchor=west] {$y$};
		\draw[->] (axis cs:-30,0) -- (axis cs:32,0);
		\draw[->] (axis cs:0,-30) -- (axis cs:0,30);
	    \draw [red,rotate=40,thick] \boundellipse{axis cs:10,-36}{300}{50} node[left,xshift=3mm,yshift=10mm]  {$\,\,p(\vz)$}; 
	%	\fill (axis cs:0,0) circle (2pt) ; 
	%	\node [right] at (axis cs:-9,8) {$ {\bm \Sigma}^{(3)}$} ; 
		\end{axis}
		\end{tikzpicture}
		\vspace*{-14mm}
	\end{center}
	\caption{A scatterplot where each dot represents some day that is characterized 
		by its minimum daytime temperature $x$ and its maximum daytime temperature $y$.}
	\label{fig_scatterplot_temp_FMI_stat_model}
	\vspace*{-3mm}
\end{figure}




\section{Artificial Intelligence}

ML is instrumental for the design and analysis of artificial intelligence (AI). 
AI systems (hard and software) interacts with their environment by 
taking certain actions. These actions influence the environment as 
well as the state of the AI system. The behaviour of an AI system 
is determined by how the perceptions made about the environment 
are used to form the next action. 

From an engineering point of view, AI aims at optimizing behaviour to maximize a 
long-term {\bf return}. The optimization of behaviour is based solely on the perceptions 
made by the agent. Let us consider some application domains where AI systems can be used:
\begin{itemize}
\item a {\bf forest fire management system}: perceptions given by 
satellite images and local observations using sensors or ``crowd sensing'' 
via some mobile application which allows humans to notify about 
relevant events; actions amount to issuing warnings and bans of 
open fire; return is reduction of number of forest fires. 
\item an {\bf control unit} for combustion engines: 
perceptions given by various measurements such as temperature, 
fuel consistency; actions amount to varying fuel feed and timing 
and the amount of recycled exhaust gas; return is measured in 
reduction of emissions.   
\item a {\bf severe weather warning service}: perceptions given by 
weather radar; actions are preventive measures taken by farmers or 
power grid operators; return is measured by savings in damage 
costs (see \url{https://www.munichre.com/})
\item an automated {\bf benefit application system} for a social 
insurance institute (like ``Kela'' in Finland): perceptions given by 
information about application and applicant; actions are either to 
accept or to reject the application along with a justification for the 
decision; return is measured in reduction of processing time 
(applicants tend to prefer getting decisions quickly)
\item  a {\bf personal diet assistant}: perceived environment is the 
food preferences of the app user and their health condition; actions 
amount to personalized suggestions for healthy and yummy food;  
return is the increase in well-being or the reduction in public spending 
for health-care.  
\item the {\bf cleaning robot} \rumba\, (see\ Figure \ref{fig:cleaning_robot}) 
perceives its environment using different sensors (distance sensors, 
on-board camera); actions amount to choosing different moving 
directions (``north'', ``south'', ``east'', ``west''); return might be the 
amount of cleaned floor area within a particular time period. 
\item {\bf personal health assistant:}  perceptions given by current 
health condition (blood values, weight,\ldots), lifestyle (preferred 
food, exercise plan);  actions amount to personalized suggestions 
for changing lifestyle habits (less meat, more jogging,\ldots); return 
is measured via level of well-being (or the reduction in public 
spending for health-care).  
\item {\bf government-system} for a community: perceived environment 
is constituted by current economic and demographic indicators such as 
unemployment rate, budget deficit, age distribution,\ldots; actions involve 
the design of tax and employment laws, public investment in infrastructure, 
organisation of health-care system; return might be determined by the 
gross domestic product, the budget deficit or the gross national 
happiness (cf. \url{https://en.wikipedia.org/wiki/Gross_National_Happiness}). 
\end{itemize}
	\vspace*{2mm}
\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=8cm]{AIsystemEnvML1.jpg}  
		\caption{A cleaning robot chooses actions (moving directions) to maximize 
			a long-term reward measured by the amount of cleaned floor area per day.}
		\label{fig:cleaning_robot}
	\end{center}
\end{figure}

ML methods are used on different levels within AI systems. On a low-level, ML methods 
help to extract the relevant information from raw data. As an example we could use an 
ML method to classify images into different categories. The AI system subsequently 
only needs to process the category of the image instead of its raw digital form. 

ML methods are also used for higher-level tasks of an AI system. To behave optimally 
requires an AI system or agent to learn a good hypothesis about how her behaviour 
affects its environment. We can think of optimal behaviour as the consequent choice 
of actions that are predicted as optimal according to some hypothesis which could 
be obtained by ML methods. 

%A widely used operation mode of ML is to predict some quantity of interest, or label, of 
%some data point based on some features. Such a data point could also represent some 
%situation which requires a decision. For example, a particular data point could correspond 
%to some driving situation of an autonomous car. We can use all the measurements provided 
%on-board sensors to predict the best steering angle as the quantity of interest (or label). 

What sets AI methods apart from other ML methods is that they must 
compute predictions in real-time while collecting data and choosing 
the next action. Consider an AI system that steers a toy car. In any 
given state (point of time) the resulting prediction influences immediately 
the features of the following datapoints. 

Consider data points to represent the different states of a toy car. 
For such data points we  could define their labels as the optimal 
steering angle for these states. However, it might be very challenging 
to obtain accurate label values for any of these datapoints. Instead, 
we could evaluate the usefulness of a particular steering angle only 
in an indirect fashion by using a reward signal. For the toy car example, 
we might obtain a reward from a distance sensor that indicates if the 
car reduces the distance to some goal or target location. 




\section{Organization of this Book}

We can formalize a particular ML application or problem by defining the {\bf features} and 
{\bf labels} of data points and choosing the {\bf loss function} for measuring how good a 
particular ML method performs (see Chapter \ref{ch_Elements_ML}). Besides features, labels 
and loss function we also highlight the importance of restricting the class of potential predictor 
maps from features to labels. We will refer to the restricted class of {\bf computationally feasible} 
maps used by a ML method as its {\bf hypothesis space} or {\bf model}. 

We detail in Chapter \ref{ch_some_examples} how some widely used ML methods are obtained 
as combinations of particular choices for feature and label space, loss function and hypothesis space. 

In Chapter \ref{ch_Optimization} we introduce and discuss the principle of {\bf empirical risk minimization (ERM)} 
which amounts to choosing  a predictor based on minimizing the average loss (or empirical risk) incurred over 
{\bf labeled training data}. The ERM principle amounts to a precise mathematical formulation of the 
``learning by trial and error'' paradigm. 

Chapter \ref{ch_GD} discusses a particular optimization method 
{\bf gradient descent (GD)} which is an iterative method for solving 
the ERM problem in order to find good predictors. Variations of GD 
are currently the de-facto standard method for solving ERM arising 
in large-scale ML problems \cite{Goodfellow-et-al-2016}. 

We then discuss in Chapter \ref{ch_validation_selection} the basic 
idea of validating a predictor by trying it out on labeled data (so called 
``validation data'') which is different from the training data used 
to learn a predictor. As detailed in Chapter \ref{ch_overfitting_regularization}, 
a main reason for doing validation is to detect and avoid {\bf overfitting} 
which causes poor performance of ML methods. 

The main focus of this tutorial is on {\bf supervised ML methods} which assign labels to each data 
point and require the availability of some data points (the training set) whose labels are known. 
The label of a data point is some quantity of interest such as the fact if an image shows a cat or not. 
Another example is weather prediction where a data point represents a user query containing a location 
and time. The label associated with this data point could be the local temperature at the queried location 
and time. 

Supervised ML methods aim at finding a (predictor or classifier) map that reads in features 
of a data point and outputs a prediction. The prediction should be an accurate approximation 
to the true label (see Chapter \ref{ch_Elements_ML}). To find such a map, supervised ML methods 
use historic data in order to try out different choices for the map and picking the best one. 

The basic idea of supervised ML methods, as illustrated in Figure \ref{fig_curve_fitting}, is to fit a 
curve (representing the predictor map) to data points obtained from historic data (see Chapter \ref{ch_Optimization}). 
While this sounds like a trivial task, the challenge of modern ML applications is the amount of data points. 
ML methods must process billions of data points with each single data point characterized by a potentially 
vast number of features. Consider data points representing social network users, whose features include all 
media that has been posted (videos, images, text). 

Beside the size of datasets, another computational challenge for modern ML methods is that they must be 
able to fit highly non-linear predictor maps. Deep learning methods address this challenge by using a 
computationally convenient representation of non-linear maps via artificial neural networks \cite{Goodfellow-et-al-2016}). 


\begin{figure}[htbp]
\begin{center}
\begin{tikzpicture}[auto,scale=1]
%\draw [thick] (5,2.5) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(3)}$};
%\draw [thick] (4,2) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(4)}$};
%\draw [thick] (5,1) circle (0.1cm)node[anchor=west,above] {\hspace*{0mm}$\vx^{(2)}$};
\draw [thick] (1,0.2) rectangle ++(0.1cm,0.1cm) ;%node[anchor=west,above] {\hspace*{0mm}$\vx^{(1)}$};
\draw [thick] (2,3) rectangle ++(0.1cm,0.1cm) ; %node[anchor=west,above] {\hspace*{0mm}$\vx^{(5)}$};
\draw [thick] (3,2) rectangle ++(0.1cm,0.1cm) node[anchor=west] {\hspace*{0mm}$(x^{(2)},y^{(2)})$};%node[anchor=west,above] {\hspace*{0mm}$\vx^{(6)}$};
\draw [thick] (2,1) rectangle ++(0.1cm,0.1cm) node[anchor=west] {\hspace*{0mm}$(x^{(1)},y^{(1)})$};
\draw[->] (-0.5,0) -- (3.5,0) node[right] {feature $x$};
\draw[dashed,line width=1pt] (0,0) -- (3,3) node[anchor=west] {predictor $h(x)$} ; 
\draw[->] (0,-0.5) -- (0,3.5) node[above] {label $y$};
%\foreach \y/\ytext in { 1/1,2/2,3/3,4/4,5/5} \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext/5$};  
%\foreach \x/\xtext in{ 1/1,2/2,3/3,4/4,5/5}\draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\xtext/5$};  
\end{tikzpicture}
\end{center}
\caption{Supervised ML methods fit a curve to (a huge number of) data points. 
} 
\label{fig_curve_fitting}
\end{figure}

In some ML applications there is no reason for defining labels associated with data points. 
The resulting ML methods are then called {\bf unsupervised ML methods}. In Chapter \ref{ch_Clustering}, 
we will study an important family of unsupervised ML methods which amount to {\bf clustering} data 
points into coherent groups (clusters). 

Chapter \ref{ch_FeatureLearning} discusses another important unsupervised ML problem referred 
to as {\bf dimensionality reduction}. Dimensionality reduction methods exploit intrinsic correlations 
and redundancy in the raw data to construct few relevant features for data points. These relevant 
features can be subsets of raw features (feature selection) or combinations of all raw features (feature transformation). 

Two main challenges in the widespread use of ML techniques are their privacy-preservation 
and explainability. Chapters \ref{chap_privacy_preserving_ML} and \ref{chap_explainable_ML} will discuss 
recent approaches to solve these challenges. We will see that the concepts developed in Chapter \ref{ch_FeatureLearning} 
for feature learning will be perfect tools for privacy-preserving and explainable ML. 

{\bf Prerequisites.} We assume some familiarity with basic concepts of linear algebra, real analysis 
and probability theory. For a review of those concepts, we recommend \cite[Chapter 2-4]{Goodfellow-et-al-2016} 
and the references therein. 

{\bf Notation.} We mainly follow the notational conventions used in \cite{Goodfellow-et-al-2016}. 
Boldface upper case letters (such as $\mA,\mX,\ldots$) denote matrices while boldface 
lower case letters (such as $\vy,\vx,\ldots$) denote vectors. The generalized identity 
matrix $\mathbf{I}_{\featuredim \times r} \in \{0,1\}^{\featuredim \times r}$ is a diagonal 
matrix with ones on the main diagonal. The Euclidean norm of a vector $\vx=(x_{1},\ldots,x_{\featuredim})^{T}$ 
is denoted $\| \vx \| = \sqrt{ \sum_{r=1}^{\featuredim} x^{2}_{r}}$. 

\newpage
\chapter{Three Components of ML: Data, Model and Loss} 
\label{ch_Elements_ML}

%Consider the cleaning robot ``\rumba'' in Figure \ref{fig:rumba}, which has to clean the office room B329. 
%For simplicity, we model this office room as a plain rectangular area (see Figure \ref{fig:RoomGridworld}) in what follows. 
%We discretise the office floor using small squares or ``cells''. A particular cell within room B329 is represented by the 
%coordinate pair $\gridcell{m}{y}$ with coordinates $m \in \{1,\ldots,\gridsizex\}$ and $y \in \{1,\ldots,\gridsizey\}$. 
%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width=8cm]{Rumba_Charging0.jpg}
%	\caption{The cleaning robot ``\rumba''.}
%	\label{fig:rumba}
%\end{figure}
%
%\begin{figure}[htbp]
%	\centering
%	\includegraphics[width=\textwidth]{RoomGridworld.jpg}
%	\caption{The office room which \rumba\, has to keep tidy, and a simple gridworld model of the room's floor space. 
%	We also depict the action set of Rumba, which is constituted by the four compass direction into which the cleaning robot can move.}
%	\label{fig:RoomGridworld}
%\end{figure}
%
%From time to time, \rumba\, has to visit a charging station in order to charge its battery.
%In order to reach a charging station, \rumba\, can choose from different actions each of which 
%corresponding to a particular direction into which it can move next (see Figure \ref{fig:RoomGridworld}). 
%The optimal action at a particular time $\timeidx$ depends on the current location $\gridcell{m_{\timeidx}}{y_{\timeidx}}$ of \rumba. 
%However, it turns out that determining the precise location within a typical office room is far from trivial 
%(see \url{http://www.indooratlas.com/how-it-works/}).Therefore, let us now assume that \rumba\, has to predict 
%(or infer) its location, i.e., the coordinates $m_{\timeidx}$ and $y_{\timeidx}$, using solely the 
%information contained in the snapshots generated by some on-board camera. We also depict the 
%action set of Rumba, which is constituted by the four compass direction into which the cleaning 
%robot can move. 
%
This book portrays ML as combinations of three components: 
\begin{itemize}
	\item {\bf data} as collections of data points characterized by {\bf features} (see Section \ref{sec_feature_space}) and {\bf labels} (see Section \ref{sec_labels}) 
	\item a {\bf model} or {\bf hypothesis space} (see Section \ref{sec_hypo_space}) of computationally 
	feasible maps (called ``predictors'' or ``classifiers'') from feature to label space 
	\item a {\bf loss function} (see Section \ref{sec_lossfct}) to measure the quality of a predictor (or classifier). 
\end{itemize}
We formalize a ML problem or application by identifying these three 
components for a given application. A formal ML problem is obtained 
by specific design choices for how to represent data, how to model 
the relation between features and label of a data point and in what 
loss function to measure the quality of a ML method. Once the ML 
problem is formally defined, we can apply ML methods to solve them. 

Similar to ML problems (or applications) we also think of ML 
methods as specific combinations of the three above components. 
We detail in Chapter \ref{ch_some_examples} how some of the 
most popular ML methods, such as linear regression and deep 
learning methods, are obtained by specific design choices for 
the three components. 

Linear regression is a ML method which uses linear maps for 
the hypothesis space and the squared error loss function. 
Deep learning methods are characterized by using artificial 
neural networks to represent hypothesis spaces constituted 
by highly non-linear predictor maps. The remainder of this 
chapter discusses in some depth each of the three main 
components of ML. 

\begin{figure}[htbp]
\begin{center}
    \includegraphics[width=8cm]{AMLProblem1.jpg}  
\end{center} 
\caption{A formal ML problem consists of (i) a particular choice of features and 
	label to characterize data points, (ii) a hypothesis space which consists of 
	(computationally) feasible maps from features to labels and (iii) a loss function 
	which is used to assess the quality of a particular predictor.}
\label{fig_ml_problem}
\end{figure}

\section{The Data}
\label{sec_the_data}

{\bf Data as Collections of Datapoints.} Maybe the most important component of any 
ML problem (and method) is data. We consider data as collections of 
individual datapoints which are atomic units of ``information containers''. Datapoints 
can represent text documents, signal samples of time series generated by sensors, 
entire time series generated by collections of sensors, frames within a single video, 
videos within a movie database, cows within a herd, trees within a forest, forests 
within a collection of forests. Consider the problem of predicting the duration 
of a moutain hike (see Figure \ref{fig:image}). Here, datapoints could represent 
different hiking tours.  

\begin{figure}[htbp]
	\centering
	\includegraphics[width=7cm]{BergSee1.jpg}
	\caption{Photo taken at the beginning of a mountain hike.}
	\label{fig:image}
\end{figure}


We use the concept of datapoints in a highly abstract and therefore very flexible 
manner. Datapoints can represent very different types of objects. For an image 
processing application it might be useful to define datapoints as images. Within 
a recommendation system it might make sense to use datapoints representing 
human users or consumers. The meaning or definition of datapoints is nothing 
but a design choice. 

The only conceptual requirement we have for this design choice is that datapoints 
represent objects of a similar ``type'' (e.g., every datapoint of a given dataset 
represents a mountain hike somewhere in Austria). Another, more practical, requirement 
for a good choice for data points is that we should have access to many of them 
(e.g., via hiker forums). Loosely speaking, most ML methods rely on statistical 
averages to obtain predictions and {\bf the more datapoints used for the averaging, the better.} 
A key parameter of a dataset is therefore the number $m$ of individual datapoints 
it contains. Statistically, the larger the sample size $m$ the better. However, there 
might be restrictions on computational resources that limit the maximum sample size 
$m$ that can be processed. ??? nice figure illustrating a dataset with $m$ data points????

In most cases it is impossible to have full access to every single microscopic property 
of a data point. Consider a data point that represents a particular mountain hike. A 
full characterization of such a datapoint would require to know the precise weather 
condition at every point in time during the hike. Moreover, a mountain hike also 
involves the physical conditions of the hikers themselves which, in turn, depend 
on many other properties such as body height, weight and fitness level. 

It will be useful to distinguish between two different types of properties of a 
datapoint. The first type of properties is referred to as ``features''  and the second 
type of property is referred to as ``label'', or ``target'' or ``output''. 
We emphasize that this distinction is somewhat blurry. The same 
property of a data point might be used as a feature in one application, 
while it might be used as a label in other applications. 

\subsection{Features}
\label{sec_feature_space}
%The features of a data point are properties that can be computed, measured or 
%determined easily without requiring (significant) repetitive human labour. Determining 
%features of a data point typ be automated and performed efficiently. 

Similar to the definition of data points, also the choice of what properties 
to be used as features is a design choice. We typically use as features any 
quantity that can be computed or measured easily. Note that this is a highly 
informal characterization since we do not have an objective criterion 
to measure the difficulty in measuring a specific property. 

If we develop a ML method that can use snapshots taken by a digital camera, then 
these snapshots might be a useful choice for the features. However, if we only have 
a thermometer at our disposal then we might only use the measured temperature as 
the feature. In what follows we will denote the total number of features used to describe 
a data point by the letter $\featurelen$. 

The ability of ML methods has been boosted by modern information-technology 
which allows to measure a huge number of properties about datapoints 
in many application domains. Consider a data point representing the book 
author ``Alex Jung''. Alex uses a smartphone to take snapshots. 

Let us assume that Alex takes five snapshots per day on average (sometimes 
more, e.g., during a mountain hike). This results in more than $1000$ 
snapshots per year. Each snapshot contains around $10^{6}$ pixels. 
If we only use the greyscale levels of the pixels in all those snapshots, 
we would obtain more than $10^{9}$ new features per year! Modern ML 
applications face extremely high-dimensional feature vectors which calls 
for methods from high-dimensional statistics \cite{BuhlGeerBook,Wain2019}. 

%For a data point represented by a snapshot (see Figure \ref{fig:image}), we could use the red, green and blue intensities of 
%all image pixels as features. However, we could also use only the averages (over all pixels) of red, green and blue intensity. 
At first sight it might seem that ``the more features the better'' since using more features 
might convey more relevant information to achieve the overall goal. However, as we discuss 
in Chapter \ref{ch_overfitting_regularization}, it can actually be detrimental for the performance 
of ML methods to use an excessive amount of (irrelevant) features. 

Using too many irrelevant features might overwhelm or jam ML algorithms which should invest 
their computational resources mainly in the processing of the most relevant features. It is 
difficult to give a precise characterization on the maximum number of features that should be 
used. Some guidance is offered by the checking the condition $\featurelen/\samplesize \gg 1$, i.e., the number 
of features is much larger than the number of data points available for an ML algorithm. In this 
high-dimensional regime, there is a high risk of overwhelming ML algorithms by having too many 
irrelevant features. To avoid this we could apply some feature selection or model regularization 
techniques (see Chapter \ref{ch_FeatureLearning} and Chapter \ref{ch_overfitting_regularization}). 

Choosing ``good'' features of the datapoints arising within a given ML application 
is far from trivial and might be the most difficult task within the overall ML application. 
The family of ML methods known as {\bf kernel methods} \cite{LampertNowKernel} 
is based on constructing efficient features by applying high-dimensional {\bf feature maps}. 

A recent breakthrough achieved by modern ML methods, which are known as 
{\bf deep learning methods} (see Section \ref{sec_deep_learning}), is their ability 
to automatically learn good features without requiring too much manual engineering 
(``tuning'') \cite{Goodfellow-et-al-2016}. We will discuss the very basic ideas behind 
such {\bf feature learning} methods in Chapter \ref{ch_FeatureLearning} but for now 
assume the task of selecting good features is already solved. 

A datapoint is typically characterized by many individual features  $x_{1},\ldots,x_{\featuredim}$. 
It is convenient to stack the individual features into a single feature vector 
\begin{equation}
\nonumber 
\vx=\big(x_{1},\ldots,x_{\featuredim}\big)^{T} \in \mathbb{R}^{\featuredim}. 
\end{equation}
Each datapoint is then characterized by such a feature vector $\vx$. The set of 
all possible values that the feature vector can take on is sometimes referred to 
as {\bf feature space}, which we denote as $\featurespace$. Note that we 
allow the feature space to be finite. This can be useful for {\bf network-structured datasets} 
where the data points can be compared with each other by some application-specific 
notion of similarity \cite{WhenIsNLASSO,NNSPFrontiers2018,LogisticNLasso,JungTVMin2019}. 
These approaches use as a feature space the node set of an ``empirical graph'' whose 
nodes represent individual datapoints. The edges in the empirical graph encode 
similarities between individual datapoints. 

The feature space $\featurespace$ is a design choice for the ML engineer facing a 
particular ML application and computational infrastructure. If the computational 
infrastructure allows for efficient numerical linear algebra, then using $\featurespace = \mathbb{R}^{\featurelen}$ 
might be a good choice. In general, to obtain computationally efficient ML methods 
one typically uses feature spaces $\featurespace$ with a rich mathematical structure.
 
The Euclidean space $\mathbb{R}^{\featuredim}$ is a prime example of a feature space 
with a rich geometric and algebraic structure \cite{RudinBookPrinciplesMatheAnalysis}. 
The algebraic structure of $\mathbb{R}^{\featuredim}$ is defined by linear algebra of vector 
addition and multiplication with scalars. A geometric structure is obtained by defining 
distances between two elements of $\mathbb{R}^{\featuredim}$ via the Euclidean norm. 
The interplay between these two structures allows us then to efficiently search 
over subsets of $\mathbb{R}^{\featuredim}$ to find an element that is closest to 
some other given element of $\mathbb{R}^{\featurelen}$. 

Throughout this book we will mainly use feature spaces $\featurespace \subseteq \mathbb{R}^{\featuredim}$ 
which are subsets of the Euclidean space $\mathbb{R}^{\featuredim}$ with some fixed dimension $\featuredim$. 
Using RGB intensities (modelled as real numbers) of the pixels within a (rather small) $512 \times 512$ pixel bitmap, 
we end up with a feature space $\featurespace=\mathbb{R}^{\featuredim}$ of (rather large) dimension 
$\featuredim = 3 \cdot 512^2$. Indeed, for each of the $512 \times 512$ pixels we obtain $3$ numbers which 
encode the red, green and blue colour intensity of the respective pixel (see Figure \ref{fig_snapshot_pixels}). 

Consider data points representing images. A natural construction for the feature vector of such data points 
is to stack the red, green and blue intensities for all image pixels (see Figure \ref{fig_snapshot_pixels}). For other 
types of data points it is less obvious how to represent the datapoints by a numeric feature vector in $\mathbb{R}^{\featurelen}$. 
Feature learning methods are ML methods that aim at automatically determining useful feature vectors. For 
natural language processing, some successful feature learning methods have been proposed recently \cite{Mikolov2013}. 



\begin{figure}[htbp]
	\begin{center}
		\includegraphics[width=8cm]{RumbaSnapshotFeature1.jpg}  
	\end{center}
	\caption{If the snapshot $\vz^{(\sampleidx)}$ is stored as a $512 \times 512$ RGB bitmap, we could use as 
		features $\vx^{(\sampleidx)} \in \mathbb{R}^{\featuredim}$ the red-, green- and blue component of each pixel 
		in the snapshot. The length of the feature vector would then be $\featuredim= 3 \cdot 512 \cdot 512 \approx 786000$. }
	\label{fig_snapshot_pixels}
	% \caption{A Simple Classification Example}\label{fig:A Simple Classification Example}
\end{figure}

%Our goal is to infer the current location $\state_{\timeidx}= \gridcell{x_{\timeidx}}{y_{\timeidx}}$ of \rumba\, 
%at time $\timeidx$ based only on the snapshot $\vz^{(\timeidx)}$ which is represented by the feature vector $\vx^{(\timeidx)}$. 
\subsection{Labels}
\label{sec_labels}

Besides the features of a data point, there are other properties of a data point that represent 
some higher-level information or ``quantity of interest'' 
associated with the data point. We refer to the higher level information, or quantity of 
interest, associated with a data point as its \emph{label} (or ``output'' or ``target''). 
In contrast to features, determining the value of labels is more difficult to automate. 
Many ML methods revolve around finding efficient ways to determine the label of a 
data point given its features. 

As already mentioned above, the distinction of data point properties into 
labels and those that are features is blurry. Roughly speaking, labels are 
properties of data points that might only be determined with the help of 
human experts. For data points representing humans we could define 
its label $y$ as an indicator if the person has flu ($y=1$) or not ($y=0$). 
This label value can typically only be determined by a physician. However, 
in another application we might have enough resources to determine the 
flu status of any person of interest and could use it as a feature that 
characterizes a person. 

Consider a data point that represents some hike, at the start of 
which the snapshot in Figure \ref{fig:image} has been taken. The 
features of this data point could be the red, green and blue intensities 
of each pixel in the snapshot in Figure \ref{fig:image}. We can stack 
these values into a vector $\vx \in \mathbb{R}^{\featurelen}$ whose 
length $\featurelen$ is given by three times the number of pixels in 
the image. The label $y$ associated with this data point could be 
the expected hiking time to reach the mountain in the snapshot. 
Alternatively, we could define the label $y$ as the water temperature 
of the lake visible in the snapshot. 

The label space $\labelspace$ of an ML problem contains all possible label values 
of data points. For the choice $\labelspace = \mathbb{R}$, it is customary to refer 
to such a problem as a {\bf regression problem}. It is also common to refer to ML 
problems involving a discrete (finite or countably infinite) label space as {\bf classification problems}. 

ML problems with only two different label values are referred to as 
{\bf binary classification problems}. Examples of classification 
problems are: detecting presence of a tumour in a tissue, classifying 
persons according to their age group or detecting the current floor 
conditions ( ``grass'', ``tiles'' or ``soil'') for a mower robot. 



%In contrast to features, the labels of data points are typically difficult or costly to obtain directly. 
%For the snapshot in Figure \ref{fig:image}, we can easily determine the pixel intensities as features. 
%However, if the label is the water temperature of the lake depicted on the snapshot, we need to actually 
%measure this temperature. 
A data point is called \emph{labeled} if, besides its features $\vx$, the associated label $y$ is known.
The acquisition of labeled data points typically involves human labour, such as handling a water 
thermometer at certain locations in a lake. In other applications, acquiring labels 
might require sending out a team of marine biologists to the Baltic sea \cite{MLMarineBiology}, 
running a particle physics experiment at the European organization for nuclear research (CERN) \cite{MLCERN}, 
running animal testing in pharmacology \cite{MLPharma}. There are also online marktplaces to rent human labelling workforce \cite{Mort2018}. 
In these labelling services, you can upload your data points, such as images, and then 
some human labels them, such as marking those who show a cat. 

Many applications involve data points whose features can be determined easily but 
whose labels are known for few data points only. In general, labeled data is a scarce resource. 
As observed in \cite{UnreasonableData}, some of the most successful ML methods have been 
devised in application domains where label information can be acquired easily. ML methods for  
speech recognition and machine translation can make use of massive labeled datasets that is 
freely available \cite{Koehn2005}. 

In the extreme case, we do not know the label of any single data point. Even in the absence of 
any labeled data, ML methods can be useful for extracting relevant information out of the 
features only. We refer to ML methods which do not require any labeled data points 
as {\bf unsupervised ML methods}. We discuss some of the most important unsupervised ML 
methods in Chapter \ref{ch_Clustering} and Chapter \ref{ch_FeatureLearning}).  

As discussed next, many ML methods aim at constructing (or finding) a ``good'' 
predictor $h: \featurespace \rightarrow \labelspace$ which takes the features 
$\vx \in \featurespace$ of a data point as its input and outputs a predicted label 
(or output, or target) $\hat{y} = h(\vx) \in \labelspace$. A good predictor should 
be such that $\hat{y} \approx y$, i.e., the predicted label $\hat{y}$ is close (with 
small error $\hat{y} - y$) to the true underlying label $y$. 

\subsection{Scatterplot} 
\label{equ_subsection_scatterplot}

Consider datapoints characterized by a single numeric feature $x$ 
and label $y$. Each data point represents a particular day whose 
feature and label are the minimum and maximum daytime temperature, 
respectively, at some place in Finland. 

We can access these datapoints via the archives of the Finnish Meteorological Institute (FMI). 
To get more insight into the relation between feature $x^{(\sampleidx)}$ and label 
$y^{(\sampleidx)}$, it can be helpful to generate a scatter plot as shown in Figure \ref{fig_scatterplot_temp_FMI}. 
A scatter plot depicts the data points $\vz^{(\sampleidx)}=(x^{(\sampleidx)},y^{(\sampleidx)})$ 
in a two-dimensional plane with the axes representing the values of feature $x$ and label $y$. 

A visual inspection of a scatterplot might suggest potential relationships between feature $x$ 
and label $y$. From Figure \ref{fig_scatterplot_temp_FMI}, it seems that there might be a relation 
between feature $x$ and label $y$ since datapoints with larger $x$ tend to have larger $y$. This 
makes sense since having a larger minimum daytime temperature typically implies also a larger 
maximum daytime temperature. 

%\begin{figure}[htbp]
%\begin{center}
%\begin{tikzpicture}
%    \begin{axis}[
%            axis x line=middle,
%            axis y line=middle,
%            enlarge y limits=true,
%            width=15cm, height=8cm,     % size of the image
%            grid = major,
%            grid style={dashed, gray!30},
%            ylabel=Ethereum closing price $y$,
%            xlabel=Bitcoin closing price $x$,
%         ]        
%          \addplot[only marks] table [x=CloseBTC, y=CloseETH, col sep = comma] {BTCETH-USD.csv};
%    \end{axis}
%\end{tikzpicture}
%\end{center}
%\caption{A scatter plot of some data points $(x^{(\sampleidx)},y^{(\sampleidx)})$ with Bitcoin 
%	closing price $x^{(\sampleidx)}$ and Ethereum closing price $y^{(\sampleidx)}$ at the 
%	$\sampleidx$-th day. Each data point $(x^{(\sampleidx)},y^{(\sampleidx)})$ is depicted by a 
%	dot located at the point with coordinates $x^{(\sampleidx)}$ and $y^{(\sampleidx)}$.\label{fig_scatterplot}} 
%\end{figure}

%\subsection{Missing Data} 
%The city of Turku is collecting information about pedestrian traffic by counting the number of pedestrians 
%passing by certain points in the city center. In Figure \ref{} we depict the recordings obtained from 
%few counting points over time. The recordings have some missing field which correspond to time steps 
%when for some reason the counting device did not deliver the count (estimate).  

\subsection{Probabilistic Models for Data} 
Consider data points representing previous days in Finland. Each data point is 
characterized by its (average) daytime temperature $x \in \mathbb{R}$. Assume we 
have downloaded $\samplesize$ data points $\big(x^{(1)},\ldots,x^{(\samplesize)} \big)$. 

Many successful ML methods are based on a simple but crucial idea: Interpret data points 
as realizations of random variables. The most basic and widely used form of a probabilistic 
model for the data points in ML is the {\bf ``independent and identically distributed'' (i.i.d.)} 
assumption. Here, the data points are interpreted as statistically independent realizations of 
one single random variable $x$.

Interpreting data points as realizations of a random variable $x$, allows to 
use the properties of the probability distribution $p(x)$ to characterize the statistical 
properties of the data. The probability distribution $p(x)$ is either assumed 
known or estimated from data (see Section \ref{sec_max_iikelihood}). It is often 
enough to not estimate the distribution $p(x)$ entirely but only some of its parameters. 

Some of the most basic and widely used parameters of a probability distribution $p(x)$ 
is the expected value or mean $\mu_{x} = \expect\{x\} \defeq \int_{x} x p(x) dx$ and variance 
$\sigma_{x}^{2} \defeq \expect\big\{\big(x-\expect\{x\}\big)^{2}\big\}$. These parameters 
can be estimated using the sample mean (average) and sample variance, 
\begin{equation} 
\hat{\mu}_{x} \defeq (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} x^{(\sampleidx)}\mbox{ , and } \widehat{\sigma^{2}_{x}} \defeq (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \big( x^{(\sampleidx)} - \hat{\mu}_{x} \big)^{2}.  
\end{equation} 
A widely used estimator for the square root of the variance is the 
(sample) standard deviation $\hat{s}_{x} \defeq  \sqrt{(1/(\samplesize-1)) \sum_{\sampleidx=1}^{\samplesize} \big( x^{(\sampleidx)} - \hat{\mu}_{x} \big)^{2}}$. 




\section{The Model}
\label{sec_hypo_space}

Consider the problem of classifying hand-drawings into two categories. 
The first category consists of all hand-drawings that show an apple. 
The second category consists of all hand-drawings that do not show 
an apple. We consider each hand-drawing as one particular data point. 

Each data point is associated with a numeric label $y \in \mathbb{R}$ 
which is a percentage measuring how much the underlying hand-drawing 
resembles an apple. Beside its label $y$, each data point (hand-drawing) 
is characterized by a feature vector $\vx=\big(x_{1},\ldots,x_{n}\big)^{T}$ 
obtained by stacking the greyscale intensities of a $128 \times 128$ 
pixel image of the hand-drawing.  

Given a set of $m$ labeled data points (the ``training data'')
\begin{equation} 
\label{eq_def_training_set}
\dataset  = \big\{ (\vx^{(1)},y^{(1)}), \ldots, (\vx^{(\samplesize)},y^{(\samplesize)}) \}
\end{equation}
ML methods learn a predictor $h(\vx)$ such that 
\begin{equation} 
\label{equ_approx_label_pred}
y \approx \underbrace{h(\vx)}_{\hat{y}}
\end{equation}  
for any data point with features $\vx$. The approximation \eqref{equ_approx_label_pred} 
should be accurate for any data point, also those not contained 
in the training set \eqref{eq_def_training_set}. 

%????
%Consider the cleaning robot \rumba\, which is depicted in Figure \ref{fig:cleaning_robot}. 
%A routine task of \rumba\, is to determine (predict) the coordinate $y^{(\sampleidx)} \in \mathbb{R}$ 
%of its current location at time $\sampleidx$. The prediction must be based solely on the features 
%$\vx^{(\sampleidx)} \in \mathbb{R}^{\featuredim}$ of the snapshot $\vz^{(\sampleidx)}$ delivered 
%by the on-board camera at time $\sampleidx$. ?????

To have any chance to solve this task, we must assume (or hypothesize) 
an underlying relation between the features $\vx^{(\sampleidx)}$ 
and the label $y^{(\sampleidx)}$. We represent this relation between 
features $\vx^{(\sampleidx)}$ and label $y^{(\sampleidx)}$ using a 
{\bf hypothesis map} $h: \featurespace \rightarrow \labelspace$, which 
maps the feature vector $\vx^{(\sampleidx)}$ of the snapshot to a 
predicted label $\hat{y}^{(\sampleidx)} = h(\vx^{(\sampleidx)})$. 

%A reasonable hypothesis $h$ should approximate the true label $y^{(\sampleidx)}$ 
%as accurately as possible such that $\hat{y}^{(\sampleidx)}=h(\vx^{(\sampleidx)}) \approx y^{(\sampleidx)}$ 
%at every time $\sampleidx$. Note that computing the prediction $\hat{y}=h(\vx)$ amounts to the evaluation 
%of a map $h(\cdot)$ for the particular argument $\vx$ (see Figure \ref{fig_feature_map_eval}). This seems 
%trivial, but in many ML applications the feature $\vx$ and the predictor $h(\cdot)$ can be very complicated 
%(high-dimensional) objects. Using feature vectors with millions of entries is not an exception but rather the 
%rule (see Figure \ref{fig_snapshot_pixels}). Much of ML theory revolves around the analysis and design of 
%automated methods for finding good predictors $h$ which can be evaluated efficiently. 



\begin{figure}[htbp]
%\begin{minipage}{\textwidth}
%\begin{center}
%\begin{tikzpicture}[auto,bend angle=30]
%  \filldraw[fill=red!20,snake=bumps] (0,0) rectangle (6,4);
%  \node[] (A) at (3,2) {$\featurespace$};
%  \fill (5cm,3) circle (2pt)  node[above] (C) {$\vx \in \featurespace$};
%   
%      \draw [ultra thick] (12,2) circle (2cm);
%        \node[] (B) at (12,2) {$\labelspace$};
%         \fill  (11.5,2.5)  circle (2pt) node[above] (D) {$\hat{y} \in \labelspace$};    
%         \draw [->,thick,dashed] (5,3)   --  (11.5,2.5) node[above,midway] {$h(\vx)$} ;
%\end{tikzpicture}
%\end{center}
%\end{minipage}
\begin{minipage}{\textwidth}
\begin{center}
   \includegraphics[width=0.7\textwidth]{HypoMap1.jpg}  
   \end{center}
\end{minipage}
\caption{A predictor (hypothesis) $h$ maps features $\vx\!\in\!\featurespace$, of 
	an on-board camera snapshot, to the prediction $\hat{y}\!=\!h(\vx)\!\in\!\labelspace$ 
	for the coordinate of the current location of a cleaning robot. ML methods use data 
	to learn predictors $h$ such that $\hat{y}\!\approx\!y$ (with true label $y$).}
\label{fig_feature_map_eval}
\end{figure}

In principle, we could use any possible map $h: \featurespace \rightarrow \labelspace$ 
for predicting the label $y \in \labelspace$ based on the features $\vx \in \featurespace$ 
via computing $\hat{y} = h(\vx)$. However, any practical ML methods has only {\bf limited 
	computational resources} and therefore can only make use of a subset of all possible 
predictor maps. This subset of computationally feasible (``affordable'') predictors is referred 
to as the {\bf hypothesis space} or {\bf concept class}. 

The choice of which hypothesis space to use depends crucially on the available 
computational infrastructure. Different types of computational infrastructure call 
for different choices for the hypothesis space. For ML methods implemented in 
a small embedded system, we might prefer a linear hypothesis space which can 
be processed using numerical linear algebra. Deep learning methods implemented 
using cloud computing can make use of much larger hypothesis spaces obtained 
from deep neural networks. 

If the computational infrastructure allows for efficient numerical linear algebra and the feature 
space is the Euclidean space $\mathbb{R}^{\featuredim}$, a popular choice for the hypothesis 
space is  
\begin{align}
\label{equ_def_hypo_linear_pred}
\hspace*{-3mm}\hypospace^{(\featuredim)} \!\defeq\!\{ h^{(\vw)}\!:\!\mathbb{R}^{\featuredim}\!\rightarrow\!\mathbb{R}\!:\!h^{(\vw)}(\vx)\!=\!\vx^{T} \vw \mbox{ with some weight vector } \vw\!\in\!\mathbb{R}^{\featuredim} \}.  
\end{align}
The hypothesis space $\hypospace^{(\featuredim)}$ in \eqref{equ_def_hypo_linear_pred} is constituted 
by the linear maps (functions) $h^{(\vw)}: \mathbb{R}^{\featuredim} \rightarrow \mathbb{R}$ which map 
the feature vector $\vx \in \mathbb{R}^{\featuredim}$ to the predicted label (or output) $h^{(\vw)}(\vx) = \vx^{T} \vw \in \mathbb{R}$. 
For $\featuredim\!=\!1$ the feature vector reduces to one single feature $x$ and the hypothesis space 
\eqref{equ_def_hypo_linear_pred} consists of all maps $h^{(w)}(x) = w x$ with some weight $w \in \mathbb{R}$ (see Figure \ref{scalar_lin_space}). 


\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{HypothesisMapRumba.jpg}  
    \caption{A predictor (hypothesis) $h: \featurespace \rightarrow \labelspace$ takes the feature vector $\vx^{(\timeidx)} \in \featurespace$ 
    (e.g., representing the snapshot taken by \rumba\, at time \timeidx) as input and outputs a predicted label $\hat{y}_{\timeidx}=h(\vx^{(\timeidx)} )$ (e.g., 
    the predicted $y$-coordinate of \rumba\, at time $\timeidx$). 
    A key problem studied within ML is how to automatically learn a good (accurate) predictor $h$ such that $y_{\timeidx} \approx h(\vx^{(\timeidx)} )$. }
%    \caption{Hypothesis Map}
\label{fig:Hypothesis Map}
\end{figure}

%\begin{figure}[htbp]
%    \centering
%    \includegraphics[natwidth=800,natheight=600,width=\textwidth]{HypoSpaceLinear.png}  
%    \caption{The hypothesis space $\hypospace$ which is constituted by all linear maps $h^{(w)}(x)=x w$ of the single feature $x$.}
%%    \caption{Hypothesis Map}\label{fig:Hypothesis Map}
%\label{scalar_lin_space}
%\end{figure}

\begin{figure}[htbp]
\begin{center}
    \begin{tikzpicture}
          \begin{axis}
     [ylabel=$h(x)$,scale=0.9,
              xlabel=$x$,%grid, 
              axis x line=center,
  axis y line=center,
  xtick={1},
  ytick={1},
  xlabel={feature $x$},
  ylabel={$h^{(w)}(x)$},
  xlabel style={right},
  ylabel style={above},
  xmin=-1.4,
  xmax=3.5,
  ymin=-1.1,
  ymax=1.1
  ]
  \addplot [smooth, color=red, ultra thick] table [x=a, y=b, col sep=comma] {linear.csv} node [right,color=red] {$h^{(1)}(x)\!=\!x$};
  \addplot [color=blue, ultra thick] table [x=a, y=c, col sep=comma] {linear.csv} node [right,color=blue] {$h^{(0.2)}(x)\!=\!0.2x$}; 
  \addplot [color=black, ultra thick] table [x=a, y=d, col sep=comma] {linear.csv}node [right,color=black] {$h^{(0.7)}(x)\!=\!0.7x$}; 
 \end{axis}
%\node [right,color=red] at (8.3,1.5) {$h^{(\mathbf{w})} = 7(x^2 - x^3)$};
%\node [right,color=blue] at (2.7,0.5) {$h^{(\mathbf{w})} =x^2$};
%\node [right] at (3.5,2.2) {$h^{(\mathbf{w})}\!=\!x$};
     \end{tikzpicture}
     \vspace*{-4mm}
\end{center}
\caption{Three particular members of the hypothesis space 
	$\hypospace =\{ h^{(w)}: \mathbb{R} \rightarrow \mathbb{R}, h^{(w)}(x) = w \cdot x \}$ 
which consists of all linear functions of the scalar feature $x$. We can parametrize 
this hypothesis space conveniently using the weight $w\in \mathbb{R}$ as $h^{(w)}(x) = w \cdot x$.}
\label{scalar_lin_space}
\end{figure}

The elements of the hypothesis space $\hypospace$ in \eqref{equ_def_hypo_linear_pred} are  
parametrized by the weight vector $\vw \in \mathbb{R}^{\featuredim}$. Each map $h^{(\vw)} \in \hypospace$ 
is fully specified by the weight vector $\vw$. Thus, instead of searching a good hypothesis directly 
in the function space $\hypospace$ (its elements are functions!), we can equivalently search over 
all possible weight vectors $\vw \in \mathbb{R}^{\featuredim}$. The search space $\mathbb{R}^{\featuredim}$ 
is still (unaccountably) infinite but it has lots of geometric and algebraic structure which allows to 
efficiently search over this space. 

ML methods based on the linear hypothesis space \eqref{equ_def_hypo_linear_pred} 
can be implemented by combinations of few basic operations from linear algebra such 
as vector-matrix multiplications. One reason for the popularity of methods using 
\eqref{equ_def_hypo_linear_pred} is the broad availability of computing hardware 
(such graphic processing units) and programming frameworks (such as numerical linear 
algebra libraries) allow efficient computations of the elementary operations. 

The hypothesis space \eqref{equ_def_hypo_linear_pred} can only be used for ML problems 
within which the features of data points are represented by vectors $\vx = (x_{1},\ldots,x_{\featuredim})^{T} \in \mathbb{R}^{\featuredim}$. 
Since ML methods based on the hypothesis space \eqref{equ_def_hypo_linear_pred} are 
quite well developed (using numerical linear algebra), it might be useful to re-formulate 
the ML problem at hand using vectors as features. This suggests the ``country wisdom'' 
that within ML any data has to be turned into vectors (or matrices). For text data, there has 
been significant progress recently on methods that map a human-generated text into sequences 
of vectors (see \cite[Chap. 12]{Goodfellow-et-al-2016} for more details). 

%Since the choice of what features are used to characterize data points is a design choice, we can simply require the features 
%to be represented by vectors to allow the use of  
The hypothesis space, i.e., the subset of predictor maps $h: \featurespace \rightarrow \labelspace$ 
used in a ML method, is a {\bf design choice}. Some particular choices have proven useful for 
a wide range of applications (see Chapter \ref{ch_some_examples}). As with the choice of 
what properties to use as features of a data point, finding a good choice of the hypothesis 
space typically requires a good understanding (``domain expertise'') of statistical properties 
of the data and the limitations of the available computational infrastructure. 

One important aspect guiding the choice for the hypothesis space is the available 
computational infrastructure. If we only have a spreadsheet program at our disposal 
then it might be reasonable to use a hypothesis space which is constituted by maps 
$h$ which can be represented by look-up tables (see Figure \ref{table_lookup}). 
If our computational infrastructure allows for efficient numerical linear algebra. 
which is the case for most desktop computers and scientific programming languages), 
the linear hypothesis space \eqref{equ_def_hypo_linear_pred} might be useful.

%Note that the elements of the hypothesis space $\hypospace$ are maps $h$ from the feature space 
%to the label space of a given ML problem. 
Remember that a hypothesis (or predictor) $h$ is a map which takes the 
features $\vx \in \featurespace$ as its input and outputs a predicted (or 
estimated) label $\hat{y} = h(\vx) \in \labelspace$. If the label space $\labelspace$ 
is finite (such as $\labelspace=\{-1,1\}$) we use the term {\bf classifier} instead 
of hypothesis. For regression problems involving a continuous label space (such 
as $\labelspace=\mathbb{R}$) it is common to refer to a hypothesis as a {\bf predictor}. 
%However, in the end, a classifier (or a predictor (or a hypothesis)) is nothing but a {\bf map from the feature space 
%to the label space of an ML problem.}

For a finite label space $\labelspace$ and feature space $\featurespace=\mathbb{R}^{\featuredim}$, 
we can characterize a particular classifier map $h$ using its {\bf decision boundary}. The decision 
boundary of a classifier $h$ is the set of all border points between the different regions 
$\mathcal{R}_{\hat{y}} \defeq \{ \vx: h(\vx) = \hat{y}\} \subseteq \featurespace$ of the feature 
space $\featurespace$. A particular region $\mathcal{R}_{\hat{y}}$ contains all feature 
values $\vx \in \featurespace$ which are mapped to the same prescribed label value $\hat{y} \in \labelspace$. 

ML methods known as {\bf linear classifiers}, which includes logistic regression 
(see Section \ref{sec_LogReg}), the SVM (see Section \ref{sec_SVM}) and naive 
Bayes' classifiers (see Section \ref{sec_NaiveBayes}), use classifier maps whose 
decision boundary is a hyperplane $\{ \vx: \vw^{T} \vx = b \}$ (see Figure \ref{fig_lin_dec_boundary}).
\begin{figure} 
\begin{center}
\begin{tikzpicture}[allow upside down, scale=1]
\draw[dashed,line width=3pt] (0,0) -- (10,5)  [smooth, tension=0.5]
node[sloped,inner sep=0cm,above,pos=.5,
anchor=south west,
minimum height=2cm,minimum width=1cm](N){adf};
\path [fill=yellow] (0,0) -- (0,5) -- (10,5) -- (0,0); %to %[out=-80, in=160]
\node [below] at (5,1) {$h(\vx)<0, \hat{y} = -1$};
\node [below] at (9,4) {decision boundary};
\node [below] at (1,3) {$h(\vx)\geq0, \hat{y} = 1$};
\path (N.south west)
edge[->,blue,thick] node[left] {$\mathbf{w}$} (N.north west);
\end{tikzpicture}
\end{center}
\caption{A hypothesis $h: \featurespace\!\rightarrow\!\labelspace$ for a binary 
	classification problem, with label space $\labelspace=\{ -1,1\}$ and feature 
	space $\featurespace=\mathbb{R}^{2}$, can be represented conveniently via the 
	decision boundary (dashed line) which separates all feature vectors $\vx$ with 
	$h(\vx) \geq 0 $ from the region of feature vectors with $h(\vx)<0$. If the decision 
	boundary is a hyperplane $\{ \vx: \vw^{T} \vx = b \}$ (with normal vector 
$\vw \in \mathbb{R}^{\featuredim}$), we refer to the map $h$ as a {\bf linear classifier}. }
\label{fig_lin_dec_boundary}
\end{figure}


%\begin{center}
%\framebox[0.96\textwidth]{
% \parbox{0.9\textwidth}{
%{\bf Exercise.} Given the hyperplane $\mathcal{C}=\{ \vx:  \vw^{T} \vx = 1 \}$ with vector $\vw = (1,0,0,1)^{T}$, can you find a 
%map $h: \mathbb{R}^{4} \rightarrow \mathbb{R}$ of the form $h(\vx) = \vu^{T} \vx +g$ with some vector $\vu \in \mathbb{R}^{4}$ and constant $g$, 
%such that $h(\vx)\geq 0$ for all $\vx$ above $\mathcal{C}$ and $h(\vx) < 0$ 
%for all $\vx$ below $\mathcal{C}$?
%}}
%\end{center}

One important aspect guiding the choice for the hypothesis space is the available computational 
framework. If we can only use a {\bf spreadsheet program}, we should use a hypothesis space 
constituted by maps $h: \featurespace \rightarrow \labelspace$ which can be implemented easily 
by a spreadsheet (see Table \ref{table_lookup}). If we instead use the programming language 
Python to implement a ML method, we can obtain a hypothesis class by collecting all possible 
Python subroutines with one input (scalar feature $x$), one output argument (predicted label $\hat{y}$) 
and consisting of less than $100$ lines of code. %(see Figure \ref{fig_hypospace_Python}). 

The largest possible choice for the hypothesis space $\hypospace$ of an ML problem using 
feature space $\featurespace$ and label space $\labelspace$ is the set $\labelspace^{\featurespace}$ 
constituted by all maps from the feature space $\featurespace$ to the label space $\labelspace$.
\footnote{The elements of $\labelspace^{\featurespace}$ are maps $h: \featurespace \rightarrow \labelspace$.}   
The choice $\hypospace = \labelspace^{\featurespace}$ is rarely used in practice since this space is 
simply too large to work within a reasonable amount of computational resources. Instead, the 
hypothesis space $\hypospace$ is typically a (very small) subset of $\labelspace^{\featurespace}$ 
(see Figure \ref{fig_hypo_space}). However, choosing the hypothesis space too small might result 
in poor performance of the overall ML method, as we might not find any suitable predictor (or 
classifier) map $h \in \hypospace$ which allows to capture the true underlying relationship between 
features $\vw$ and label $y$ of data points. 

%Consider the dataset depicted in Figure \ref{fig_scatterplot} consisting of historic information 
%about cryptocurrency prices. Each data point (depicted by a dot) corresponds to the cryptocurrency 
%statistics of one particular day. The $i$-th data point is characterized using the feature $x^{(\sampleidx)}$ 
%being the closing price of Bitcoin at day $\sampleidx$ and the label $y^{(\sampleidx)}$ being the closing 
%price of Ethereum at the same day. 
%
%If the relation $x \mapsto y$ cannot be well approximated by a linear function, we must 
%consider a hypothesis space which contains non-linear maps. In particular, we might 
%consider a hypothesis space which is constituted by polynomials, e.g., 
%$h(x) = w_{0} +w_{1} x + w_{2} x^2 + w_{3} x^{3}$, of a given maximum degree. 
%Figure \ref{fig_poly_hypo_space} depicts a particular predictor which belongs 
%to this larger hypothesis space. We will consider ML problems using a hypothesis 
%space of polynomial functions of a scalar feature in Section \ref{sec_polynomial_regression}. 
%
%\begin{figure}[htbp]
%\begin{center}
%    \begin{tikzpicture}
%          \begin{axis}
%     [ylabel=$h(x)$,
%     xscale=1,
%              xlabel=$x$,grid, 
%              axis x line=center,
%  axis y line=center,
%  xtick={-2,-1,...,2},
%  ytick={0,1,...,2},
%  xlabel={feature $x$},
%  ylabel={$h^{(\mathbf{w})}(x)$},
%  xlabel style={right},
%  ylabel style={above},
%  xmin=0,
%  xmax=1.2,
%  ymin=0,
%  ymax=1.1
%  ]
%
%  \addplot [smooth, color=red, ultra thick] table [x=a, y=b, col sep=comma] {polynomials.csv};    
%  \addplot [color=blue, ultra thick] table [x=a, y=c, col sep=comma] {polynomials.csv};     
%  \addplot [color=black, thick] table [x=a, y=d, col sep=comma] {polynomials.csv};      
% \end{axis}
%\node [right,color=red] at (5.5,1.5) {$h^{(\mathbf{w}_{1})}(x) = 7(x^2 - x^3)$};
%\node [right,color=blue] at (2.3,0.5) {$h^{(\mathbf{w}_{2})}(x) =x^2$};
%\node [right] at (2.5,2.8) {$h^{(\mathbf{w}_{3})}(x)\!=\!x$};
%     \end{tikzpicture}
%     \vspace*{-4mm}
%\end{center}
%\caption{Three particular members of the hypothesis space $\mathcal{H}_{\rm poly}^{(3)}$ 
%(see \eqref{equ_def_poly_hyposapce}) which consists of polynomials of degree at most $3$.}
%\label{fig_poly_hypo_space}
%\end{figure}


The design choice for the hypothesis space $\hypospace$ has to balance between two conflicting requirements.  
\begin{itemize} 
\item It has to be {\bf sufficiently large} such that it contains at least 
one accurate predictor map $\hat{h} \in \hypospace$. A hypothesis space $\hypospace$ 
that is too small might fail to include a predictor map required to reproduce the (potentially 
highly non-linear) relation between features and label. Consider the ML problem of classifying 
an images into ``cat image'' and ``no cat image'' using the pixel colour intensities as features. 
The relation between features and label ($y \in \{ \mbox{cat}, \mbox{no cat} \}$) is highly non-linear. 
Any ML method that uses a hypothesis space consisting only of linear maps will most likely 
fail to learn a good predictor (classifier). We say that a ML method {\bf underfits} the data if it 
uses a too small hypothesis space. 
\item It has to be {\bf sufficiently small} such that its processing fits the available computational 
resources (memory, bandwidth, processing time). We must be able to efficiently search over the 
hypothesis space to find good predictors (see Section \ref{sec_lossfct} and Chapter \ref{ch_Optimization}). 
This requirement implies also that the maps $h(\vx)$ contained in $\hypospace$ can be evaluated 
(computed) efficiently \cite{Austin2018}. Another important reason for using a hypothesis space $\hypospace$ 
not too large is to avoid {\bf overfitting} (see Chapter \ref{ch_overfitting_regularization}). 
If the hypothesis space $\hypospace$ is too large, then just by luck we might find a predictor which fits 
well the training dataset. Such a predictor will perform poorly on data which is different from 
the training data (it will not generalize well). 
\end{itemize}

The notion of a hypothesis space being too small or being too large can 
be made precise in different ways. For a finite hypothesis spaces $\hypospace$, 
we might simply use the cardinality $|\hypospace|$ as a measure of its size. 
Consider data points represented by $100 \times 10 = 1000$ black and white 
pixels (see Figure \ref{fig_snapshot_pixels}) and characterized by a binary label. 

This amounts to a feature space $\featurespace = \{0,1\}^{1000}$ and label space 
$\labelspace = \{0,1\}$. The largest possible hypothesis space consisting of 
all maps is finite with size $|\hypospace| = 2^{1000}$, which is larger than 
the estimated number of atoms in the visible universe (which is around $10^{80}$). 

Many ML methods use a hypothesis space which contains infinitely many 
different predictors (see, e.g., \eqref{equ_def_hypo_linear_pred}). For infinite 
hypothesis spaces, we cannot easily use its cardinality as a measure for its size. 
Different concepts have been studied for measuring the size of infinite 
hypothesis spaces with the {\bf Vapnik–Chervonenkis (VC) dimension} being 
maybe the most famous one \cite{VapnikBook}. 

We will use a simpler variant of the VC dimension concept and define the size of a hypothesis 
space $\hypospace$ as the maximum number $\sizehypospace$ of arbitrary data points that 
can be perfectly fit (with probability one). For any set of $\sizehypospace$ data points with 
different features, we can always find a hypothesis $h \in \sizehypospace$ such that $y=h(\vx)$ 
for all data points $(\vx,y) \in \dataset$. 

Let illustrate the our notion of hypothesis space size with two examples: linear regression and 
polynomial regression. First, we discuss the size of the hypothesis space $\hypospace^{(\featuredim)}$ 
used in linear regression. Consider $\samplesize$ data points characterized by feature vectors 
$\vx \in \mathbb{R}^{(\featuredim)}$ and a numeric label $y \in \mathbb{R}$. We assume that 
data points are i.i.d. realizations of some continuous random vector (probability of the vector 
belonging to a zero measure set is zero). This implies that the matrix obtained by columnise 
stacking the feature vectors is full rank with probability one. Basic linear algebra allows then 
to show that such a set of data points can be perfectly fit by a linear map $h \in \hypospace^{(\featuredim)}$ 
as long as $\samplesize \leq n$. Thus, the size of the linear hypothesis space $\hypospace^{(\featuredim)}$
is $\sizehypospace = \featuredim$. 

As a second example, consider data points characterized by a single numeric feature $x$ and 
numeric label $y$. Polynomial regression aims at learning a predictor out of the 
hypothesis space $\hypospace_{\rm poly}^{(\featuredim)}$ of polynomials with 
maximum degree $\featuredim$. The fundamental theorem of algebra tells us that 
any set of $\samplesize$ data points with different features can be perfectly fit by a 
polynomial of degree $\featuredim$ as long as $\featuredim \geq \samplesize$. Therefore, 
the size of the hypothesis space $\hypospace_{\rm poly}^{(\featuredim)}$ is $\sizehypospace=\featuredim$. 
We will discuss polynomial regression in more detail in Section \ref{sec_polynomial_regression}. 


%For the sake of simplicity let us develop a simpler concept for the size of a hypothesis space. 
%Let us use the maximum number of arbitrary data points that can be perfectly fit using some 
%element of $\hypospace$ as the measure for the size of $\hypospace$. 

%???? remove?????
%The second aspect becomes relevant already for (seemingly) simple ML problems. Indeed, consider \rumba\, 
%which has to predict if it is moving towards a charging station at time $\sampleidx$, which is encoded by the 
%label $y^{(\sampleidx)} =-1$ or not, encoded by  $y^{(\sampleidx)} =1$. In order to determine if it is moving 
%towards a charging station, \rumba\, can only use the snapshot $\vz^{(\sampleidx)}$ generated by its on-board 
%camera at time $\sampleidx$. The snapshot is available as a black-white bitmap consisting of $512 \times 512$ 
%pixels. In order to construct a classifier, we represent the snapshot using a feature vector $\vx^{(\sampleidx)} \in \{0,1\}^{\featuredim}$ 
%of length $\featuredim =  512^2$ . 
%
%If we would use as hypothesis space $\hypospace_{\rm BW}$ the set of all possible maps from 
%the feature space $\featurespace= \{0,1\}^{512^{2}}$ (representing a BW bitmap) into the label 
%space $\labelspace=\{-1,1\}$, we would end up with a hypothesis space containing $2^{| \featurespace |} = 2^{2^{512^{2}}}$ 
%elements. Thus, the naive choice of a hypothesis space (which is to consider all possible 
%maps from feature to label space) for this (quite small) ML problem would already result 
%in a hypothesis space $\hypospace_{\rm BW}$
%
%??????

\begin{figure}[htbp]
\begin{center}
\begin{tikzpicture}[allow upside down, scale=1]
%(0,0)(0.70,1.9)(1.41,3.6)
%\draw plot [smooth, tension=0.5] coordinates{(2.13,5.1)(2.86,6.4)(3.62,7.5)(4.42,8.4)(5.30,9.1)(6.34,9.6)(7.13,9.8)(8.42,9.9)}
%\draw[red,line width=1pt] (0,0) -- (10,5)  [smooth, tension=0.5]
%node[sloped,inner sep=0cm,above,pos=.5,
%anchor=south west,
%minimum height=2cm,minimum width=1cm](N){};
%\path [fill=yellow] (0,0) -- (0,5) -- (10,5) -- (0,0); %to %[out=-80, in=160]
\node [below] at (5,-3) {$\labelspace^{\featurespace}$};
\draw [ultra thick] (5,0) circle (5cm);
\draw [ultra thick,fill=black!20] (5,0) circle (1cm);
\node [] at (5,0) {$\hypospace$};
%\path (N.south west)
%edge[->,blue,thick] node[left] {$\mathbf{w}$} (N.north west);
\end{tikzpicture}
\end{center}
\caption{The hypothesis space $\hypospace$ is a (typically very small) subset of the (typically very large) 
	set $\labelspace^{\featurespace}$ of all possible maps from feature space $\featurespace$ into the label 
	space $\labelspace$.}
\label{fig_hypo_space}
\end{figure}


\vspace*{2mm}
\begin{table}
\begin{center}
\begin{tabular}{ |c|c| }
  \multicolumn{1}{c}{feature $x$}
  &  \multicolumn{1}{c}{prediction $h(x)$} \\
\cline{1-2}
 0 & 0 \\
 \cline{1-2}
 1/10 & 10 \\
 \cline{1-2}
 2/10 & 3 \\
 \cline{1-2}
 \vdots & \vdots \\
 \cline{1-2}
 1 & 22.3 \\
\cline{1-2}
\end{tabular}
\end{center}
\caption{A spreadsheet representing of a hypothesis map $h$ in the form of 
	a look-up table. The value $h(x)$ is given by the entry in the second column 
	of the row whose first column entry is $x$.}
\label{table_lookup}
\end{table}
\vspace*{2mm}

%\begin{figure}[htbp]
%%\resizebox {\textwidth} {!} {
%%\scalebox{.4}{
%\begin{tikzpicture}[scale=1,sharp corners=2pt,inner sep=7pt,node distance=.8cm,every text node part/.style={align=center}]
%\draw [draw=black] (0,-5.5) rectangle (15,4);
%\node[draw,right=1cm, minimum height = 2cm, minimum width = 3cm] (state0)
%{\begin{lstlisting}
%def predictor_map1(x):
%    return x*x
%\end{lstlisting}};
%\node[draw,right=1cm,below=2cm of state0, minimum height = 2cm, minimum width = 3cm](state2)
%{\begin{lstlisting}
%def predictor_map2(x):
%    return 3*x*x*x
%\end{lstlisting}};
%\node[draw,right=2cm of state0, minimum height = 2cm, minimum width = 3cm](state1)
%{\begin{lstlisting}
%def predictor_map3(x):
%    return 5*x
%\end{lstlisting}};
%\node[draw,below=2cm of state1, minimum height = 2cm, minimum width = 3cm](state3)
%{\begin{lstlisting}
%def predictor_map4(x):
%    return 5
%\end{lstlisting}};
%\node [below] at (3,4) {hypothesis space $\hypospace$};
%\end{tikzpicture}
%\caption{Example of a hypothesis space $\hypospace$ which consists of four predictor maps 
%%$h^{(1)}(x)=x^2$, $h^{(2)}(x)=3*x^3$, $h^{(3)}(x)=5x$, $h^{(4)}(x)=5$ 
%which are implemented using the programming language Python.}
%\label{fig_hypospace_Python}
%\end{figure}

%In some applications it is convenient to allow the hypothesis $h$ to take on 
%values also outside the label space $\labelspace$. We will consider binary 
%classification problems (see Section \ref{sec_LogReg}) with label space $\labelspace =\{-1,1\}$ where 
%it is convenient to use hypothesis maps $h: \featurespace \rightarrow \mathbb{R}$ 
%which can take on arbitrary real numbers. This is useful since we can interpret the 
%absolute value of $h(\vx)$ as the level of confidence in the predicted label $\hat{y}$ 
%which can be defined as the sign of $h(\vx)$, i.e.,
%\begin{equation} 
%\label{equ_classify_quantize}
%\hat{y}= \begin{cases} 1 \mbox{ , if } h(\vx) \geq 0 \\  -1 \mbox{ if } h(\vx) < 0.\end{cases} 
%\end{equation}
%Consider two data points $(\vx^{(1)},y^{(1)})$ and $(\vx^{(2)},y^{(2)})$ with binary 
%labels $y^{(1)} = y^{(2)}=1$ which we classify using a hypothesis map $h$ yielding 
%the values $h(\vx^{(1)})= 10$ and $h(\vx^{(2)}) = 1/100$. While the resulting labels 
%are the same, i.e., $\hat{y}^{(1)}= 1$ and $\hat{y}^{(2)}=1$ for both data points, we 
%are much more confident in the classification of $\vx^{(1)}$ while the classification 
%of $\vx^{(2)}$ seems not very reliable. 

\section{The Loss}
\label{sec_lossfct}

Every practical ML method uses some hypothesis space $\hypospace$ which consists 
of all {\bf computationally feasible} predictor maps $h$. Which predictor map $h$ out of 
all the maps in the hypothesis space $\hypospace$ is the best for the ML problem at hand?
To this end, we need to define a measure of the {\bf loss} (or error) incurred by using 
the particular predictor $h(\vx)$ when the true label is $y$. More formally, we define a 
loss function $\mathcal{L}: \featurespace \times \labelspace \times \hypospace \rightarrow \mathbb{R}$ 
which measures the loss $\loss{(\vx,y)}{h}$ incurred by predicting the label $y$ of a data point using the prediction $h(\vx)(=: \hat{y})$. 
The concept of loss functions is best understood by considering some particular examples. 

{\bf Regression Loss.} For ML problems involving numeric labels $y \in \mathbb{R}$, 
a good first choice for the loss function can be the {\bf squared error loss} (see Figure \ref{fig_squarederror_loss}) 
\begin{equation} 
\label{equ_squared_loss}
\loss{(\vx,y)}{h} \defeq \big(y - \underbrace{h(\vx)}_{=\hat{y}} \big)^{2}. 
\end{equation} 
Note that the squared error loss \eqref{equ_squared_loss} depends on the features 
$\vx$ only via the predicted label value $\hat{y}= h(\vx)$. We can evaluate the squared 
error loss solely using the prediction $h(\vx)$ and the true label value $y$. Beside the prediction $h(\vx)$, 
no other properties of the data point's features $\vx$ are required to determine the squared error loss.  
%We refer to ML problems involving a real-valued label (e.g., temperature or probabilities of an event) as {\bf regression problems}. 

\begin{figure}[htbp]
\begin{center}
    \begin{tikzpicture}
          \begin{axis}
          [ylabel=$h(x)$,
              xlabel=$x$,grid, 
              axis x line=center,
  axis y line=center,
  xtick={-2,-1,...,2},
  ytick={0,1,...,2},
  xlabel={prediction error $y-h(x)$},
  ylabel={squared error loss $\mathcal{L}$},
  xlabel style={below right},
  ylabel style={above},
  xmin=-2.5,
  xmax=2.5,
  ymin=-0.5,
  ymax=2.5
  ]
              % \addplot [smooth, color=red, ultra thick] table [x=a, y=b, col sep=comma] {logloss.csv};    
            %       \addplot [color=blue, ultra thick] table [x=a, y=b, col sep=comma] {hingeloss.csv};     
                   \addplot [color=black, ultra thick] table [x=a, y=b, col sep=comma] {squarederrorloss.csv};      
          \end{axis}
          %\node [right,color=red] at (4,2) {logistic loss (for $y\!=\!1$)};
           %\node [left,color=blue] at (3.1,1.5) {hinge loss (for $y\!=\!1)$};
          % \node [left] at (1.5,3) {$0/1$ loss (for $y\!=\!1)$};
     \end{tikzpicture}
     \vspace*{-4mm}
\end{center}
\caption{A widely used choice for the loss function in regression problems 
	(with label space $\labelspace=\mathbb{R}$) is the squared error loss $\loss{(\vx,y)}{h} \defeq (y - h(\vx))^{2}$. 
Note that in order to evaluate the loss function for a given hypothesis $h$, so that 
we can tell if $h$ is any good, we need to know the feature $\vx$ and the label $y$ of the data point.}
\label{fig_squarederror_loss}
\end{figure}

The squared error loss \eqref{equ_squared_loss} has appealing computational and statistical 
properties. For linear predictor maps $h(\vx) = \vw^{T} \vx$, the squared error loss is a 
convex and differentiable function of the weight vector $\vw$. This allows, in turn, to efficiently 
search for the optimal linear predictor using efficient iterative optimization methods (see Chapter \ref{ch_GD}). 

The squared error loss also has a useful interpretation in terms of a probabilistic 
model for the features and labels. In particular, minimizing the squared error loss 
is equivalent to maximum likelihood estimation within a linear Gaussian model \cite[Sec. 2.6.3]{hastie01statisticallearning}. 

Another loss function used in regression problems is the absolute error loss $|\hat{y} - y|$. 
Using this loss function to learn a good predictor results in methods that are robust 
against few outliers in the training set (see Section \ref{sec_lad}).  

{\bf Classification Loss.} In classification problems with a discrete label space $\labelspace$, 
such as in binary classification where $\labelspace = \{-1,1\}$, the squared error $(y - h(\vx))^2$ 
is not a useful measure for the quality of a classifier $h(\vx)$. We would like to punish wrong 
classifications, e.g., when the true label is $y=-1$ but the classifier produces a large positive 
number, e.g., $h(\vx) = 1000$. On the other hand, for a true label $y=-1$, we do not want to 
punish a classifier $h$ which yields a large negative number, e.g., $h(\vx) = -1000$. But exactly 
this unwanted result would happen for the squared error loss. 

Figure \ref{fig_squarederrornotgoodclass} depicts a dataset consisting of $5$ labeled data points 
with binary labels represented by circles (for $y=1$) and squares (for $y=-1$). The squared error loss 
incurred by the classifier $h_{1}$, which does not separate the two classes perfectly, is smaller than 
the squared error loss incurred by classifier $h_{2}$ which perfectly separates the two classes. 
The squared error loss is a bad choice for classification problems with a discrete label space $\labelspace$. 
\begin{figure}[htbp]
\begin{center}
\begin{tikzpicture}[auto,scale=0.8]
\draw [thick] (5,2) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(5)}$};
\draw [thick] (4,2) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(4)}$};
\draw [thick] (3,1.5) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(3)}$};
\draw [thick] (1,2) circle (0.1cm) node[anchor=west] {\hspace*{0mm}$\vx^{(2)}$};
\draw [thick] (-0.5,1.7) -- (6,1.7)  node[right] {$h_{1}$};
\draw [thick,dashed] (0.5,0) -- (0.5,2.5)  node[right] {$h_{2}$};
\draw [thick] (0,1.5) rectangle ++(0.1cm,0.1cm) node[anchor=west,below] {\hspace*{0mm}$\vx^{(1)}$};
 %       \node[] (B) at (12,2) {$\labelspace$};
  %       \fill  (11.5,2.5)  circle (2pt) node[above] (D) {$y \in \labelspace$};    
   %      \draw [->,thick] (5,3)   --  (11.5,2.5) node[above,midway] {$h(\vx)$} ;
\end{tikzpicture}
\caption{Minimizing the squared error loss would prefer the (poor) classifier $h_{1}$ 
	over the (reasonable) classifier $h_2$.}
\label{fig_squarederrornotgoodclass}
\end{center}
\end{figure}

We now discuss some popular choices for the loss function suitable for ML 
problems with binary labels. While the representation of the label values is completely irrelevant, 
it will be convenient to encode of the two label values by the real numbers $-1$ and $1$. As label 
space we use $\labelspace=\mathbb{R}$. The formulas for the loss functions we present only apply 
to this encoding. The modification of these formulas to a different encoding, such as label 
values $0$ and $1$, is not very difficult. 

Consider the problem of detecting forest fires as early as possible using webcam snapshots such as the one 
depicted in Figure \ref{fig:webcam_forestfire}. 
\begin{figure}[htbp]
\begin{center}
\includegraphics[width=8cm]{WebcamLapland.jpg}  
\caption{A webcam snapshot taken near a ski resort in Lapland.}
\label{fig:webcam_forestfire}
\end{center}
\end{figure}
A particular snapshot is characterized by the features $\vx$ and the label $y \in \labelspace=\{-1,1\}$ 
with $y=1$ if the snapshot shows a forest fire and $y=-1$ if there is no forest fire. We would like to 
find or learn a classifier $h(\vx)$ which takes the features $\vx$ as input and provides a classification 
according to $\hat{y} = 1$ if $h(\vx) >0$ and $\hat{y}=-1$ if $h(\vx) \leq 0$. Ideally we would like to 
have $\hat{y} = y$, which suggests to use the {\bf $0/1$ loss} (see Figure \ref{fig_class_loss})
\begin{equation}
\label{equ_def_0_1}
\loss{(\vx,y)}{h} \defeq \begin{cases} 1 &\mbox{ if } y h(\vx) < 0 \\ 0 & \mbox{ else.} \end{cases}
\end{equation} 
 
The $0/1$ loss is statistically appealing as it approximates the misclassification 
(error) probability $\prob{ y \neq \hat{y} }$ with $\hat{y} = {\rm sign } \{ h(\vx) \}$. 
If we interpret the data points $\dataset = \big\{ (\vx^{(\sampleidx)},y^{(\sampleidx)}) \big\}_{\sampleidx=1}^{\samplesize}$ 
as i.i.d.\ realizations of a random feature vector $\vx \in \featurespace$ and random label $y \in \{-1,1\}$, 
\begin{equation} 
\label{equ_0_1_approx_prob}
(1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \loss{(\vx^{(\sampleidx)},y^{(\sampleidx)})}{h} \approx \prob{ y \neq \hat{y} }
\end{equation} 
with high probability for sufficiently large sample size $\samplesize$. The approximation \eqref{equ_0_1_approx_prob} 
is based on the fact that the average of a large number of independent realizations of some random variable 
can be well approximated by its mean or expectation (``law of large numbers'') (see \cite{BillingsleyProbMeasure}).  
?? Indeed, the values $\loss{(\vx^{(\sampleidx)},y^{(\sampleidx)})}{h}$ are i.i.d.\ realizations of the random variable 
$\loss{(\vx,y)}{h}$.??

In view of \eqref{equ_0_1_approx_prob}, the $0/1$ loss seems the most natural choice for 
assessing the quality of a classifier if our goal is to enforce correct classification ($\hat{y}=y$). 
This appealing statistical property of the $0/1$ loss comes at the cost of high 
computational complexity. Indeed, for a given data point $(\vx,y)$, the $0/1$ loss 
\eqref{equ_def_0_1} is neither convex nor differentiable when viewed as a function of the 
classifier $h$. Thus, using the $0/1$ loss for binary classification problems typically involves 
advanced optimization methods for solving the resulting learning problem (see Section \ref{sec_NaiveBayes}). 

In order to ``cure'' the non-convexity of the $0/1$ loss we approximate it by a convex loss function. 
This convex approximation results in the {\bf hinge loss} (see Figure \ref{fig_class_loss}) 
\begin{equation} 
\label{equ_hinge_loss}
\loss{(\vx,y)}{h} \defeq \max \{ 0 , 1 - y\cdot h(\vx) \}. 
\end{equation}
While the hinge loss avoids the non-convexity of the $0/1$ loss it still is a non-differentiable 
function of the classifier $h$. The next example of a loss function for classification problems cures 
also the non-differentiability issue. 

As a third example for a loss function which is useful for binary classification problems 
we mention the {\bf logistic loss} (used within logistic regression, see Section \ref{sec_LogReg})
\begin{equation} 
\label{equ_log_loss}
\loss{(\vx,y)}{h} \defeq  \log ( 1 + \exp(- y h(\vx))). 
\end{equation}
For a fixed feature vector $\vx$ and label $y$, both, the hinge and the logistic 
loss function are {\bf convex functions} of the hypothesis $h$. However, while 
the logistic loss \eqref{equ_log_loss} depends {\bf smoothly} on $h$ (such that we 
could define a derivative of the loss with respect to $h$), the hinge loss \eqref{equ_hinge_loss} is 
{\bf non-smooth} which makes it more difficult to minimize. 

Thus, while ML methods based on the logistic loss function (such as logistic regression 
in Section \ref{sec_LogReg}), can make use of simple {\bf gradient descent methods} (see 
Chapter \ref{ch_GD}), ML methods based on the hinge loss (such as support vector 
machines \cite{hastie01statisticallearning}) have to make use of more advanced tools 
for solving the resulting learning problem (see Chapter \ref{ch_Optimization}).  

\begin{figure}[htbp]
\begin{center}
    \begin{tikzpicture}
          \begin{axis}
          [ylabel=$h(x)$,
              xlabel=$x$,grid, 
              axis x line=center,
  axis y line=center,
  xtick={-2,-1,...,2},
  ytick={0,1,...,2},
  xlabel={predictor $h(x)$},
  ylabel={loss $\mathcal{L}$},
  xlabel style={below right},
  ylabel style={above left},
  xmin=-3,
  xmax=3,
  ymin=-0.5,
  ymax=2.5
  ]
               \addplot [smooth, color=red, ultra thick] table [x=a, y=b, col sep=comma] {logloss.csv};    
                   \addplot [color=blue, ultra thick] table [x=a, y=b, col sep=comma] {hingeloss.csv};     
                   \addplot [color=black, ultra thick] table [x=a, y=b, col sep=comma] {zerooneloss.csv};      
          \end{axis}
          \node [right,color=red] at (5,1.5) {logistic loss (for $y\!=\!1$)};
           \node [right,color=blue] at (2.7,4) {hinge loss (for $y\!=\!1)$};
           \node [left] at (1.5,3) {$0/1$ loss (for $y\!=\!1)$};
     \end{tikzpicture}
     \vspace*{-10mm}
\end{center}
\caption{Some popular loss functions for binary classification problems with 
	label space $\labelspace=\{-1,1\}$. Note that the more correct a decision, i.e, 
	the more positive $h(x)$ is (when $y=1$), the smaller is the loss. In particular, 
    all depicted loss functions tend to $0$ monotonically with increasing $h(x)$.}
\label{fig_class_loss}
\end{figure}

Let us emphasize that, very much like the choice of features and hypothesis space, 
the question of which particular loss function to use within an ML method is a 
{\bf design choice}, which has to be tailored to the application at hand. What 
qualifies as a useful loss function depends heavily on the overall goal we wish to 
achieve using ML methods. Thus, there is no useful concept of ``optimal loss function''. 
In general, different loss functions are preferable for different ML applications. 

\begin{center}
\framebox[0.96\textwidth]{
 \parbox{0.9\textwidth}{
An important aspect guiding the choice of loss function is the computational complexity 
of the resulting ML method. Indeed, the basic idea behind ML methods is quite simple: 
learn (find) the particular hypothesis out of a given hypothesis space which yields the 
smallest loss (on average). The difficulty of the resulting optimization problem (see Chapter \ref{ch_Optimization}) 
depends crucially on the properties of the chosen loss function. Some loss functions allow to use 
very simple but efficient iterative methods for solving the optimization problem underlying 
an ML method (see Chapter \ref{ch_GD}). 
}}
\end{center}

{\bf Empirical and Generalization Risk.} 
Many ML methods are based on a simple probabilistic model for the 
observed data points (i.i.d.). Using this assumption, we can define the 
average or generalization risk as the expectation of the loss. A large class 
of ML methods is based on approximating the expected value of the loss 
by an empirical (sample) average over a finite set of labeled data points 
(referred to as training set). 

We define the empirical risk of some predictor when applied to labeled data points $\dataset=\big(\vx^{(1)},y^{(1)}\big),\ldots,\big(\vx^{(\samplesize)},y^{(\samplesize)}\big)$ 
as 
\begin{equation} 
\label{eq_def_emp_error_101}
\emperror(h|\dataset) = (1/\samplesize)\sum_{\sampleidx=1}^{\samplesize} \loss{(\vx^{(\sampleidx)},y^{(\sampleidx)})}{h}.  
\end{equation} 
To ease notational burden, if the dataset $\dataset$ is clear from the context, 
we use $\emperror(h)$. 




{\bf Regret.} In some application we might have not access to true labels 
but only to the predictions obtained from some other methods (``experts''). 
The quality of a hypothesis can then be measured by evaluating the loss 
between predictions $h(\vx)$ and the predictions of experts \cite{HazanOCO}. 
The concept of regret minimization is useful when we do not make any 
assumptions (such as i.i.d. samples) of the data points. If we do not use 
such a probabiliistic model we cannot use the Bayes risk (of Bayes optimal 
estimator) as benchmark. Regret minimization 
techniques can be designed and analyzed without any such probabilistic model 
for the data \cite{PredictionLearningGames}. This approach replaces the Bayes risk 
benchmark with few given reference predictors (experts) to serve as benchmark. 

{\bf Partial Feedback, ``Reward''.}  
In some application it is not possible to determine the true label of any data point. 
Without any labeled data we cannot use the concept of a loss function to measure 
the quality of a prediction (by comparing with the true label). Instead we must use 
some other form of indirect feedback or ``reward'' that can be attributed to a particular 
prediction \cite{PredictionLearningGames,SuttonEd2}. 

Consider the ML problem of predicting the optimal direction for 
moving next a toy care given the current state. ML methods can sense the 
state via a feature vector $\vx$ whose entries are pixel intensities of a 
snapshot. The goal is to learn a hypothesis map from the feature vector 
$\vx$ to a guess $\hat{y} = h(vx)$ for the optimal steering direction $y$ (true label). 

In some applications, we might have not access to the true label of any 
data point. This means that we cannot evaluate the quality of a particular 
map based on the average loss on training data. Instead, we might have 
only some indirect signal about the loss incurred by the prediction 
$\hat{y}=h(\vx)$. Such a feedback signal, or reward, could be obtained 
by a distance sensor who measures the the change of the distance 
between the car and its goal such as the charging station. 

\section{Putting Together the Pieces} 
\label{sec_putting_togehter_the_pieces}

To illustrate how ML methods combine particular design choices for 
data, model and loss, we consider data points characterized by a single 
numeric feature $x \in \mathbb{R}$ and a numeric label $y \in \mathbb{R}$. 
We assume to have access to $\samplesize$ labeled data points 
\begin{equation} 
\big(x^{(1)},y^{(1)}\big),\ldots, \big(x^{(\samplesize)},y^{(\samplesize)}\big)  \label{equ_labeled_data_putting_together}
\end{equation} 
for which we know the true label values $y^{(i)}$. Note that the assumption of knowing 
the exact true label values $y^{(i)}$ for any data point is an idealization. We might often 
face labelling or measurement errors such that the observed labels are noisy versions 
of the true label. However, there are efficient methods that can cope with noisy 
labels (see Chapter \ref{ch_overfitting_regularization}). 

Our goal is to learn a predictor map $h(x)$ such that $h(x) \approx y$ for any data point. 
We require the predictor map to belong to the hypothesis space $\hypospace$ of 
linear predictors 
\begin{equation} 
\label{equ_def_lin_pred_intercept}
h^{(w_{0},w_{1})}(x) = w_{1} x + w_{0}. 
\end{equation}

The predictor \eqref{equ_def_lin_pred_intercept} is parametrized by the slope 
$w_{1}$ and the intercept (bias or offset) $w_{0}$. We indicate this by the 
notation $h^{(w_{0},w_{1})}$. A particular choice for $w_{1},w_{0}$ 
defines some linear predictor $h^{(w_{0},w_{1})}(x) = w_{1}x +w_{0}$. 

Let us use some linear predictor $h^{(w_{0},w_{1})}(x)$ to predict the labels of 
training data points. In general, the predictions $\hat{y}^{(\sampleidx)} = h^{(w_{0},w_{1})}\big(x^{(\sampleidx)}\big)$ 
will not be perfect and incur a non-zero prediction error $\hat{y}^{(\sampleidx)} - y^{(\sampleidx)}$ (see Figure \ref{fig_emp_error}).  

We measure the goodness of the predictor map $h^{(w_{0},w_{1})}$ 
using the average squared error loss (see \eqref{equ_squared_loss})
\begin{align}
\label{equ_def_cost_fun_putting_togheter}
f(w_{0},w_{1}) & \defeq (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \big( y^{(\sampleidx)} - h^{(w_{0},w_{1})}(x^{(i)})  \big)^{2} \nonumber \\
& \stackrel{\eqref{equ_def_lin_pred_intercept}}{=}  (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \big( y^{(\sampleidx)} - ( w_{1} x^{(i)} + w_{0}) \big)^{2}. 
\end{align}
The training error $f(w_{0},w_{1})$ is the average of the squared prediction errors incurred 
by the predictor $h^{(w_{0},w_{1})}(x)$ to the labeled data points \eqref{equ_labeled_data_putting_together}. 

It seems natural to learn a good predictor \eqref{equ_def_lin_pred_intercept} by choosing 
the weights $w_{0},w_{1}$ to minimize the training error 
\begin{equation}
\min_{w_{1},w_{0} \in \mathbb{R}} f(w_{0},w_{1}) \stackrel{\eqref{equ_def_cost_fun_putting_togheter}}{=}  (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \big( y^{(\sampleidx)} - ( w_{1} x^{(\sampleidx)} + w_{0}) \big)^{2}.
\end{equation} 

The optimal weights $w_{0}',w_{1}'$ are characterized by the ``zero gradient'' condition \cite{}, 
\begin{equation}
\label{equ_zero_grad_condition_putting_together}
\frac{\partial f(w'_{0},w'_{1})}{\partial w_{0}} = 0 \mbox{, and }\frac{\partial f(w'_{0},w'_{1})}{\partial w_{1}} = 0. 
\end{equation} 
Inserting \eqref{equ_def_cost_fun_putting_togheter} into \eqref{equ_zero_grad_condition_putting_together}, 
using basic rules for calculating derivatives, we obtain the following optimality conditions 
\begin{equation}
\label{equ_zero_grad_condition_putting_together_special_form}
 (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \big( y^{(\sampleidx)} - ( w'_{1} x^{(\sampleidx)} + w'_{0}) \big)= 0 \mbox{, and } (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} x^{(\sampleidx)} \big( y^{(\sampleidx)} - ( w'_{1} x^{(\sampleidx)} + w'_{0}) \big) = 0. 
\end{equation} 

Any weights $w'_{0},w'_{1}$ that satisfy \eqref{equ_zero_grad_condition_putting_together_special_form} define 
a predictor $h^{(w'_{0},w'_{1})} = w'_{1}x + w'_{0}$ that is optimal in the sense of incurring minimum training error, 
$$f(w'_{0},w'_{1}) = \min_{w_{0},w_{1} \in \mathbb{R}} f(w_{0},w_{1}).$$

We find it convenient to rewrite the optimality condition \eqref{equ_zero_grad_condition_putting_together_special_form} 
using matrices and vectors. To this end, we first rewrite the predictor \eqref{equ_def_lin_pred_intercept} as 
$$ h(\vx) = \vw^{T} \vx \mbox{ with } \mathbf{w} = \big(w_{0},w_{1}\big)^{T}, \mathbf{x} = \big(1,x\big)^{T}.$$
Let us stack the feature vectors $\vx^{(i)} = \big(1,x^{(i)} \big)^{T}$ and labels $y^{(i)}$ of training data points 
\eqref{equ_labeled_data_putting_together} into the feature matrix and label vector, 
\begin{equation}
\mX  = \big(\vx^{(1)},\ldots,\vx^{(\samplesize)}\big)^{T} \in \mathbb{R}^{\samplesize \times 2}, \vy = \big(y^{(1)},\ldots,y^{(\samplesize)}\big)^{T} \in \mathbb{R}^{\samplesize}. 
\end{equation} 
We can then reformulate \eqref{equ_zero_grad_condition_putting_together_special_form} as 
\begin{equation}
\label{equ_zero_grad_condition_putting_together_special_form_mtx}
\mX^{T} \big( \vy - \mX \vw' \big) = \mathbf{0}. 
\end{equation} 
The entries of any weight vector $\vw' = \big(w_{0}',w_{1}'\big)$ that satisfies 
\eqref{equ_zero_grad_condition_putting_together_special_form_mtx} are solutions to 
\eqref{equ_zero_grad_condition_putting_together_special_form}. 




\begin{figure}[htbp]
    \centering
   \includegraphics[width=0.7\textwidth]{EmpError.jpg}  
    \caption{We can evaluate the quality of a particular predictor $h \in \hypospace$ by measuring the 
    prediction error $y- h(x)$ obtained for a labeled data point $(x,y)$. }
   % For the \rumba\ application, these labeled data points could be a bunch of snapshots $\vz^{(\sampleidx)} = (\vx^{(\sampleidx)},y^{(\sampleidx)})$, for $\sampleidx=1,\ldots,\samplesize$, 
   % with feature vectors $\vx^{(\sampleidx)}$ and known $y$-coordinate $y^{(\sampleidx)}$ of 
    %the position where the $\sampleidx$-th snapshot has been taken.}
%    \caption{Hypothesis Map}\label{fig:Hypothesis Map}
  \label{fig_emp_error}
\end{figure}

\newpage
\section{Exercises} 

\subsection{ How Many Features?} 
\label{ex_compML_shazaam}
Consider the ML problem underlying a music information retrieval smartphone 
app \cite{ShazamPaper}. Such an app aims at identifying the song-title based 
on a short audio recording of (an interpretation of) the song obtained via the 
microphone of a smartphone. Here, the feature vector $\vx$ represents the 
sampled audio signal and the label $y$ is a particular song title out of a huge 
music database. What is the length $\featuredim$ of the feature vector 
$\vx \in \mathbb{R}^{\featuredim}$ if its entries are the signal amplitudes of 
a $20$ second long recording which is sampled at a rate of $44$ kHz?


\subsection{Multilabel Prediction} 
\label{ex_ch2_multilabel}
Consider data points, each characterized by a feature vector $\vx \in \mathbb{R}^{10}$ and 
vector-valued labels $\vy \in \mathbb{R}^{30}$. Such vector-valued labels might be useful 
in multi-label classification problems. We might try to predict the label vector based on the 
features of a data point using a linear predictor map 
\begin{equation}
\label{equ_lin_predictor_multilabel}
\vh(\vx) = \mathbf{W} \mathbf{x} \mbox{ with some matrix } \mathbf{W} \in \mathbb{R}^{30 \times 10}. 
\end{equation} 
How many different linear predictors \eqref{equ_lin_predictor_multilabel} are there ? 
10, 30,40, infinite. 




\subsection{Average Squared Error Loss as Quadratic Form} 
\label{ex_2_0}
Consider linear hypothesis space consisting of linear maps parameterized by 
weights $\mathbf{w}$. We try to find the best linear map by minimizing the average 
squared error loss (empirical risk) incurred on some labeled training data points 
$(\vx^{(1)},y^{(1)}),(\vx^{(2)},y^{(2)}),\ldots,(\vx^{(\samplesize)},y^{(\samplesize)})$ 
Is it possible to write the resulting empirical risk, viewed as a function $f(\vw)$ as 
a convex quadratic form $f(\vw) = \vw^{T} \mathbf{C} \vw + \vb \vw + c$. If this 
is possible, how are the matrix $\mathbf{C}$, vector $\vb$ and constant $c$ related 
to the feature vectors and labels of the training data ? 


\subsection{Find Labeled Data for Given Empirical Risk} 
\label{ex_2_1}
Consider linear hypothesis space consisting of linear maps parameterized by 
weights $\mathbf{w}$. We try to find the best linear map by minimizing the average 
squared error loss (empirical risk) incurred on some labeled training data points. 
Assume we know the shape of the empirical risk as a function of the weight. 
Can you reconstruct the labeled training data that resulted in that empirical risk 
function? Is the resulting labeled training data unique or are there different training sets 
that could have resulted in the same empirical risk function? 
%% write down empirical risk as quadratic form f(w) = w^{T} X' X w + y^*y + 2*y'*X*w %%%% 

\subsection{Dummy Feature Instead of Intercept}
\label{sec_dummy_feature}
Show that any predictor of the form $h(x) = w_{1} x +w_{0}$ can be emulated by 
combining a feature map $x \mapsto \mathbf{z}$ with a predictor of the form $\mathbf{w}^{T} \mathbf{z}$. 

\subsection{Approximate Non-Linear Maps Using Indicator Functions for Feature Maps}
\label{ex_2_2}
Consider an ML application generating data points characterized by a scalar feature $x \in \mathbb{R}$ 
and numeric label $y \in \mathbb{R}$. We construct non-linear predictor maps by first mapping the 
feature $x$ to a new feature vector $\vz=(\phi_{1}(x),\phi_{2}(x),\phi_{3}(x),\phi_{4}(x))$. 
The components $\phi_{1}(x),\ldots,\phi_{4}(x)$ are indicator functions of intervals 
$[-10,-5), [-5,0),[0,5),[5,10]$. In particular, $\phi_{1}(x) = 1$ for $x \in [-10,-5)$ and $\phi_{1}(x)=0$ otherwise. 
We construct a hypothesis space $\hypospace_{1}$ by all maps of the form $\vw^{T}\vz$. 
Note that the map is a function of the feature $x$ since the feature vector $\vz$ is a function 
of $x$. Which of the following predictor maps belong to $\hypospace_{1}$?

\begin{minipage}{.5\textwidth} %
\includegraphics[width=4cm]{PiecewiseFun1.png}

(a)
\end{minipage} %
\begin{minipage}{.5\textwidth} %
	\includegraphics[width=4cm]{PiecewiseFun2.png}
	
(b)
\end{minipage}
 
 \subsection{Python Hypothesis Space}
 \label{ex_2_3}
 Consider the source codes below for five different Python functions that read 
 in the feature $x$ and return some prediction $\hat{y}$. How many elements 
 does the hypothesis space contain that is constituted by all maps $h(x)$ that 
 can be represented by one of those Python functions. 
 
  \subsection{A Lot of Features}
 \label{ex_2_toomanyfeatures}
In many application domains, we have access too a large number of features for 
each individual data point. Consider healthcare, where data points represent 
human patients. We could use all the measurements and diagnosis stored in 
the patient health record as features. When we use ML algorithms to 
analyse these data points, is it in general a good idea to use as much 
features as possible for data points ? 
 
  \subsection{Over parametrization}
 \label{ex_2_overparm}
Consider data points characterized by feature vectors $\vx \in \mathbb{R}^{2}$ and 
a numeric label $y \in \mathbb{R}$. We want to learn the best predictor out of the hypothesis space 
$$ \mathcal{H} = \big\{ h(\vx) = \vx^{T} \mathbf{A} \mathbf{w}: \mathbf{w} \in \mathcal{S} \}. $$
Here, we used the matrix $\mathbf{A} = \begin{pmatrix} 1 & -1 \\ -1 & 1 \end{pmatrix}$ and 
the set $\mathcal{S} = \big\{ (1,1)^{T}, (2,2)^{2}, (-1,3)^{T}, (0,4)^{T} \big\} \subseteq \mathbb{R}^{2}$. 
What is the cardinality of $\mathcal{H}$, i.e., how many different predictor maps 
does $\mathcal{H}$ contain?
 
  \subsection{Squared Error Loss}
 \label{ex_2_4}
 Consider a hypothesis space $\hypospace$ constituted by three predictors $h_{1}(\cdot), h_{2}(\cdot),h_{3}(\cdot)$. 
 Each predictor $h_{j}(x)$ is a real-valued function of a real-valued argument $x$. Moreover, for each $j \in \{1,2,4\}$, 
 $h_{j}(x) = 0$ for all $x^2 \leq 1$. Can you tell which of these predictors is optimal in the sense of incurring the 
 smallest average squared error loss on the three (training) data points $(x=1/10,y=3)$, $(0,0)$ and $(1,-1)$. 
 
 \subsection{Classification Loss} 
 \label{ex_2_classif_loss}
 \begin{center}
 	\framebox[0.96\textwidth]{
 		\parbox{0.9\textwidth}{
 			{\bf Exercise.} How would Figure \ref{fig_class_loss} change if we consider the loss 
 			functions for a data point $z=(x,y)$ with known label $y=-1$?
 	}}
 \end{center}
 
 \subsection{Intercept Term}
 \label{ex_2_5}
 Linear regression models the relation between the label $y$ and feature $x$ of 
 a data point by $y = h(x) + e$ with some small additive term $e$. The predictor 
 map $h(x)$ is assumed to be linear $h(x) =w_1 x + w_0$. The weight $w_{0}$ is 
 sometimes referred to as intercept or bias term. Assume we know for a given 
 linear predictor map its values $h(x)$ for $x=1$ and $x=3$. Can you determine 
 the weights $w_{1}$ and $w_{0}$ based on $h(1)$ and $h(3)$?
 
 \subsection{Picture Classification} 
 \label{ex_2_6}
Assume you want to sort a huge number of outdoor pictures you have taken during 
your last adventure trip to into three categories (or classes) 
\emph{dog}, \emph{bird} and \emph{fish}. How could we formalize this sorting 
problem as a ML problem? 

 \subsection{Maximum Hypothesis Space} 
\label{ex_2_7}
Consider data points characterized by a single real-valued feature $x$ and a 
single real-valued label $y$. How large is the largest possible hypothesis space 
of predictor maps $h(x)$ that read in the feature value of a data point and 
deliver a real-valued prediction $\hat{y}=h(x)$ ? 

 \subsection{A Large but Finite Hypothesis Space} 
\label{ex_2_8}
Consider data points whose features are $10 \times 10$ black and white pixel images. 
Each data point is also characterized by a binary label $y \in \{0,1\}$. Consider the 
hypothesis space constitute by all maps that take the bw image as input and deliver 
a prediction for the label. How large is this hypothesis space? 

\subsection{Size of Linear Hypothesis Space} 
\label{ex_size_lin_hypospace} 
Consider a training set of $\samplesize$ data points with feature 
vectors $\vx^{(\sampleidx)} \in \mathbb{R}^{\featuredim}$ and 
numeric labels $y^{(1)},\ldots,y^{(\samplesize)}$. The feature vectors 
and label values of the training set are arbitrary except that we assume 
the feature matrix $\mathbf{X} = \big(\vx^{(1)},\ldots \big)$ is full rank. 
What condition on $\samplesize$ and $\featurelen$ guarantee that we 
can find a linear predictor $h(\vx) = \vw^{T} \vx$ that perfectly fits the 
training set, i.e., $y^{(1)} = h\big(\vx^{(1)} \big),\ldots, y^{(\samplesize)} = h\big(\vx^{(\samplesize)} \big)$. 


\newpage
\chapter{Some Examples} 
\label{ch_some_examples}
As discussed in Chapter \ref{ch_Elements_ML}, ML methods combine 
three main components: 
\begin{itemize} 
\item the data which is characterized by {\bf features} which can be computed 
or measured easily and {\bf labels} which represent high-level facts. 
\item a {\bf model} or {\bf hypothesis space} $\hypospace$ which consists of 
computationally feasible predictor maps $h \in \hypospace$. 
\item a {\bf loss function} to measure the quality of a particular predictor map $h$. 
\end{itemize} 
Each of these three components involves design choices such as the type of 
data representation. This chapter details the design choices used by some of the 
most popular ML methods. 

%A rough guideline for which of these methods to use for a given ML problem is depicted in Figure \ref{fig_python_cheat_sheet}.
%\begin{figure}[htbp]
%    \centering
%   \includegraphics[width=0.7\textwidth]{Cheat_Sheet.png}  
%    \caption{A simple guideline for choosing the ML method for a problem at hand.}
%%    \caption{Hypothesis Map}\label{fig:Hypothesis Map}
%  \label{fig_python_cheat_sheet}
%\end{figure}
\begin{figure}
\begin{tikzpicture}[auto]
	\coordinate (OR) at (0.00, 0.0);
	\coordinate (RIGHT) at (15,0,0.0); 
	\coordinate (TOP) at (0.0,6) ; 
  %  \coordinate (MIDDLE) at (3,2) ; 
	\node[](a) at (OR) {};
    \node[](b) at (RIGHT) {};
    \node[](c) at (TOP) {};
    %\node[](c) at (TOP) {};
    \node[]  at  ($ (b) + (1,0) $)  {model};
    \node[anchor=east]  at  ($ (c) + (0,0.1) $)  {loss};
    \node[right=2cm of OR] (linpred) {} ;
    \node[]  at  ($ (linpred) - (0,1) $)  {linear maps};
  %  \draw ($ (linpred) + (0,2) $) rectangle (2,2) node[pos=.5] {linear regression};
    \node[draw,text width=3cm]  at  ($ (linpred) + (0,2) $)  {linear regression};
    \draw[black,line width=1] ($ (linpred) - (0,0.1) $) to ($ (linpred) + (0,0.1) $) ;
    \draw[->,black,line width=1] ($ (a) - (1,0) $) to (b);
    \draw[->,black,line width=1] ($ (a) - (0,0.4) $) to (c);
%\draw[blue,thin,dashed] (OR) -- (b.160);
%\draw[blue,thin,dashed] (OR) -- (b.210);
%\draw[red,thin,dashed] (OR) -- (a.-150);
%\draw[red,thin,dashed] (OR) -- (a.-30);   
%\node[punkt] (data) {observations};
%\node[red,below=0.5cm of data] (data1) {data};
%\node[above=of data] (dummy) {};
%\node[punkt,above=1cm of dummy] (hypothesis) {hypothesis};
%\node[right=1.4cm of dummy] (t) {make prediction} ; 
%\node[left=1.4cm of dummy] (g) {validate/adapt} ;
%\node[blue,below=0.5cm of g,anchor=east] (g1) {loss} ; 
%\node[black!30!blue,below=1cm of t,anchor=west] (g6) {inference} ; 
%\node[black!30!green,above=1cm of hypothesis,anchor=south] (g3) {model} ; 
%\draw [->,line width=0.5mm] (hypothesis.east) to [out=0,in=90] (t.north);
%\draw [->,line width=0.5mm] (t.south) to [out=270,in=0] (data.east);
%\draw [->,line width=0.5mm] (data.west) to [out=180,in=270] (g.south);
%\draw [->,line width=0.5mm] (g.north) to [out=90,in=180] (hypothesis.west);
%\draw [thick] (-3,-3) rectangle (4,4) node [anchor=east,above] {$\featurespace$} ;
%\draw [thick] (2,2) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(5)}$};
%\draw [thick] (-2,1.6) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(4)}$};
%\draw [thick] (-2,-1.7) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(3)}$};
%\draw [thick] (2,-1.9) circle (0.1cm) node[anchor=west] {\hspace*{0mm}$\vx^{(2)}$};
%\draw [thick] (.5,.5) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {\hspace*{0mm}$\vx^{(1)}$};
%       \node[] (B) at (12,2) {$\labelspace$};
%       \fill  (11.5,2.5)  circle (2pt) node[above] (D) {$y \in \labelspace$};    
%      \draw [->,thick] (5,3)   --  (11.5,2.5) node[above,midway] {$h(\vx)$} ;
\end{tikzpicture}
\end{figure}


\section{(Least Squares) Linear Regression} 
\label{sec_lin_regression}

Linear regression uses the feature space $\featurespace\!=\!\mathbb{R}^{\featuredim}$,  label space $\labelspace\!=\!\mathbb{R}$ and the linear hypothesis space 
\begin{align}
\label{equ_lin_hypospace}
\hypospace^{(\featuredim)} & = \{ h^{(\vw)}: \mathbb{R}^{\featuredim}\!\rightarrow\!\mathbb{R}: h^{(\vw)}(\vx)\!=\!\vw^{T} \vx \mbox{ with some} \nonumber \\ 
& \mbox{ weight vector } \vw \in \mathbb{R}^{\featuredim} \}.
\end{align}  
The quality of a particular predictor $h^{(\vw)}$ is measured by the squared error loss \eqref{equ_squared_loss}. 
Based on labeled training data $\dataset =\{ (\vx^{(\sampleidx)},y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize}$, linear 
regression aims at learning a predictor $\hat{h}$ which minimizes the average squared error loss (mean squared error) (see \eqref{equ_squared_loss})
\begin{align} 
\label{equ_opt_pred_linreg}
\hat{h} & = \argmin_{h \in \hypospace^{(\featuredim)}  }\emperror(h|\dataset)  \\ \nonumber 
& \stackrel{\eqref{eq_def_emp_error_101}}{=} \argmin_{h \in \hypospace^{(\featuredim)}  }  (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} (y^{(\sampleidx)} - h(\vx^{(\sampleidx)}))^{2}.
\end{align} 

Since the hypothesis space $\hypospace^{(\featuredim)} $ is parametrized by the weight vector $\vw$ (see \eqref{equ_lin_hypospace}), 
we can rewrite \eqref{equ_opt_pred_linreg} as an optimization problem directly over the weight vector $\vw$: 
\begin{align} 
\label{equ_opt_weight_vector_linreg_weight}
\vw_{\rm opt} &  = \argmin_{\vw \in \mathbb{R}^{\featuredim}} (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} (y^{(\sampleidx)} - h^{(\vw)}(\vx^{(\sampleidx)}))^{2} \nonumber \\
& \stackrel{h^{(\mathbf{w})}(\vx) = \vw^{T} \vx}{=} \argmin_{\vw \in \mathbb{R}^{\featuredim}} (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} (y^{(\sampleidx)} - \vw^{T} \vx^{(\sampleidx)})^{2}.
\end{align} 
The optimization problems \eqref{equ_opt_pred_linreg} and \eqref{equ_opt_weight_vector_linreg_weight} are equivalent in the following sense: 
Any optimal weight vector $\vw_{\rm opt}$ which solves \eqref{equ_opt_weight_vector_linreg_weight}, can be used to construct an optimal predictor $\hat{h}$, 
which solves \eqref{equ_opt_pred_linreg}, via $\hat{h}(\vx) = h^{(\vw_{\rm opt})}(\vx) = \vw_{\rm opt}^{T} \vx$. 


\section{Polynomial Regression} 
\label{sec_polynomial_regression}

%??????
%Implementing polynomial regression (task 3.4) really helped me understand what it is all about. 
%I did not quite grasp the concept from the course book, but in the exercise 3.4 description there was an image of the 
%feature matrix with the initial feature raised to the nth power, where n ranges from 0 to d -1, that really made it click for me.??????

Consider an ML problem involving data points which are characterized by a 
single numeric feature $x \in  \mathbb{R}$ (the feature space is $\featurespace= \mathbb{R}$) 
and a numeric label $y \in \mathbb{R}$ (the label space is $\labelspace= \mathbb{R}$). 
We observe a bunch of labeled data points which are depicted in Figure \ref{fig_scatterplot_poly}. 

\begin{figure}[htbp]
\begin{center}
\begin{tikzpicture}[scale=0.8]
    \begin{axis}[
            axis x line=middle,
            axis y line=middle,
              ylabel near ticks,
    xlabel near ticks,
            enlarge y limits=true,
            %xmin=0, xmax=2150,
            %ymin=0, ymax=600,
            width=10cm, height=8cm,     % size of the image
            grid = major,
            grid style={dashed, gray!30},
            ylabel=label $y$,
            xlabel=feature $x$,
       %     legend style={at={(0.1,-0.1)}, anchor=north}
         ]        
          \addplot[only marks] table [x=BTCNorm, y=ETHNorm, col sep = comma] {CryptoScatter.csv};
          %the code below is added via @Peter's comment.
        %  \addplot[only marks] table [col sep = comma,y={create col/linear regression={y=column 2}}]{table3.csv};
    \end{axis}
\end{tikzpicture}
\end{center}
\caption{A scatterplot of some data points $(x^{(\sampleidx)},y^{(\sampleidx)})$.} 
\label{fig_scatterplot_poly}
\end{figure}

Figure \ref{fig_scatterplot_poly} suggests that the relation $x \mapsto y$ between feature 
$x$ and label $y$ is highly non-linear. For such non-linear relations between features and 
labels it is useful to consider a hypothesis space which is constituted by
 polynomial functions
\begin{align}
\label{equ_def_poly_hyposapce}
\hypospace^{(\featuredim)}_{\rm poly}& = \{ h^{(\vw)}: \mathbb{R} \rightarrow \mathbb{R}: h^{(\vw)}(x) = \sum_{r=1}^{\featuredim+1} w_{r} x^{r-1} \mbox{, with } \nonumber \\ 
& \mbox{ some } \vw\!=\!(w_{1},\ldots,w_{\featuredim+1})^{T}\!\in\!\mathbb{R}^{\featuredim+1} \}. 
\end{align}
We can approximate any non-linear relation $y\!=\!h(x)$ with any desired level of 
accuracy using a polynomial $\sum_{r=1}^{\featuredim+1} w_{r} x^{r-1}$ of sufficiently large 
degree $\featuredim$.\footnote{The precise formulation of this statement is known as the 
``Stone-Weierstrass Theorem'' \cite[Thm. 7.26]{RudinBookPrinciplesMatheAnalysis}.}

As for linear regression (see Section \ref{sec_lin_regression}), we measure the quality 
of a predictor by the squared error loss \eqref{equ_squared_loss}. Based on labeled 
training data $\dataset =\{ (x^{(\sampleidx)},y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize}$, 
with scalar features $x^{(\sampleidx)}$ and labels $y^{(\sampleidx)}$, polynomial regression amounts to 
minimizing the average squared error loss (mean squared error) (see \eqref{equ_squared_loss}):
\begin{equation} 
\label{opt_hypo_poly}
\min_{h \in \hypospace_{\rm poly}^{(\featuredim)} } (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} (y^{(\sampleidx)} - h^{(\mathbf{w})}(x^{(\sampleidx)}))^{2}.
\end{equation} 

It is useful to interpret polynomial regression as a combination of a feature map (transformation) 
(see Section \ref{sec_feature_space}) and linear regression (see Section \ref{sec_lin_regression}). 
Indeed, any polynomial predictor $h^{(\vw)} \in \hypospace_{\rm poly}^{(\featuredim)}$ is obtained as 
a concatenation of the feature map  
\begin{equation}
\label{equ_poly_feature_map} 
\phi(x) \mapsto (1,x,\ldots,x^{\featuredim})^{T} \in \mathbb{R}^{\featuredim+1}
\end{equation}
with some linear map $g^{(\vw)}: \mathbb{R}^{\featuredim+1} \rightarrow \mathbb{R}: \vx \mapsto \vw^{T} \vx$, i.e., 
\begin{equation}
\label{equ_concact_phi_g_poly}
h^{(\vw)}(x) = g^{(\vw)}(\phi(x)). 
\end{equation}

Thus, we can implement polynomial regression by first applying the feature map $\phi$ (see \eqref{equ_poly_feature_map}) 
to the scalar features $x^{(\sampleidx)}$, resulting in the transformed feature vectors 
\begin{equation} 
\label{equ_def_poly_feature_vectors}
\vx^{(\sampleidx)} = \phi(x^{(i)}) = \big(1,x^{(i)},\ldots,\big( x^{(i)} \big)^{\featuredim} \big)^{T} \in \mathbb{R}^{\featuredim+1}, 
\end{equation} 
and then applying linear regression  (see Section \ref{sec_lin_regression}) to these new feature 
vectors. In particular, by inserting \eqref{equ_concact_phi_g_poly} into \eqref{opt_hypo_poly}, we 
end up with a linear regression problem \eqref{equ_opt_weight_vector_linreg_weight} with feature 
vectors \eqref{equ_def_poly_feature_vectors}. Thus, while a predictor $h^{(\vw)} \in \hypospace_{\rm poly}^{(\featuredim)}$ 
is a non-linear function $h^{(\vw)}(x)$ of the original feature $x$, it is a linear function, given explicitly 
by $g^{(\vw)}(\vx) = \vw^{T}\vx$ (see \eqref{equ_concact_phi_g_poly}), of the transformed features $\vx$ \eqref{equ_def_poly_feature_vectors}. 

\section{Least Absolute Deviation Regression}
\label{sec_lad}

Learning a linear predictor by minimizing the average squared error loss incurred 
on training data is not robust against outliers. This sensitivity to outliers is rooted 
in the properties of the squared error loss $(\hat{y} - y)^{2}$. Minimizing the 
average squared error forces the resulting predictor $\hat{y}$ to not be too far 
away from any data point. However, it might be useful to tolerate a large prediction 
error $\hat{y}-y$ for few data points if they can be considered as outliers. 

Replacing the squared loss with a different loss function can 
make the learning robust against few outliers. One such robust loss function is 
known as "Huber loss" \cite{HuberRobustBook}

\begin{equation}
\mathcal{L}(y,\hat{y}) = \begin{cases} (1/2) (y-\hat{y})^{2} & \mbox{ for } |y-\hat{y}| \leq   \varepsilon \\ 
\varepsilon(|y-\hat{y}| - \varepsilon/2) & \mbox{ else. }\end{cases}
\end{equation}

Note that the Huber loss contains a parameter $\epsilon$, which has to be adapted 
to the application at hand. The Huber loss is robust to outliers since data points 
with large errors $y - \hat{y}$ are not squared. Outliers have a smaller effect on 
the average Huber loss over the entire dataset. 

The Huber loss contains two important special cases. The first special case occurs when $\varepsilon$ is chosen 
to be very large (the precise value depending on the value range of the features and labels), such that the 
condition $|y-\hat{y}| \leq \varepsilon$ is always satisfied. In this case, the Huber loss is equivalent to the 
squared error loss $(y-\hat{y})^{2}$ (with an additional factor 1/2). 

The second special case occurs when $\varepsilon$ is very small (close to $0$) such that 
the condition $|y-\hat{y}| \leq \varepsilon$ is never satisfied. In this case, the Huber loss 
is equivalent to the absolute loss $|y - \hat{y}|$ scaled by a factor $\varepsilon$.

\section{Gaussian Basis Regression}
\label{sec_linbasreg}
As we have discussed in Section \ref{sec_polynomial_regression}, we can extend the basic linear 
regression problem by first transforming the features $x$ using a vector-valued feature map $\phi: \mathbb{R} \rightarrow \mathbb{R}^{\featuredim}$ 
and then applying a weight vector $\vw$ to the transformed features $\phi(x)$. For polynomial regression, 
the feature map is constructed using powers $x^{l}$ of the scalar feature $x$. However, we can use 
also other functions (different from polynomials) to construct the feature map $\phi$. 

In principle, we can extend linear regression using an arbitrary feature map 
\begin{equation} 
\phi(x) = (\phi_{1}(x),\ldots,\phi_{\featuredim}(x))^{T}  
\end{equation} 
with the scalar maps $\phi_{j}: \mathbb{R} \rightarrow \mathbb{R}$ which are referred to as {\bf basis functions}. 
The choice of basis functions depends heavily on the particular application and the underlying relation between 
features and labels of the observed data points. The basis functions underlying polynomial regression are 
$\phi_{j}(x)= x^{j}$. Another popular choice for the basis functions are ``Gaussians'' 
\begin{equation} 
\label{equ_basis_Gaussian}
\phi_{\sigma,\mu}(x) = \exp(-(1/(2\sigma^{2})) (x\!-\!\mu)^{2}) 
\end{equation}
which are parametrized by the variance $\sigma^{2}$ and the mean (shift) $\mu$. 

Thus, we obtain Gaussian basis linear regression by combining the feature map 
\begin{equation} 
\phi(x) = \big (\phi_{\sigma_{1},\mu_{1}}(x) ,\ldots,\phi_{\sigma_{\featuredim},\mu_{\featuredim}}(x) \big)^{T} 
\end{equation}
with linear regression (see Figure \ref{fig_lin_bas_expansion}). The resulting hypothesis space is then
\begin{align}
\label{equ_def_Gauss_hypospace}
\hypospace^{(\featuredim)}_{\rm Gauss} & = \{ h^{(\vw)}: \mathbb{R} \rightarrow \mathbb{R}: h^{(\vw)}(x)\!=\!\sum_{j=1}^{\featuredim}  w_{j}\phi_{\sigma_{j},\mu_{j}}(x) \nonumber \\
& \mbox{ with weights } \vw=(w_{1},\ldots,w_{\featuredim})^{T} \in \mathbb{R}^{\featuredim}\}.
\end{align}

Note that we obtain a different hypothesis space $\hypospace_{\rm Gauss}$ for different choices of the 
variance $\sigma^{2}$ and shifts $\mu_{j}$ used for the Gaussian function in \eqref{equ_basis_Gaussian}. 
These parameters have to be chosen suitably for the ML application at hand (e.g., using model selection 
techniques discussed in Section \ref{sec_modsel}). The hypothesis space \eqref{equ_def_Gauss_hypospace} 
is parameterized by the weight vector $\vw \in \mathbb{R}^{\featuredim}$ since each element 
of $\hypospace_{\rm Gauss}$ corresponds to a particular choice for the weight vector $\vw$. 

%Note that choosing the variance $\sigma^{2}$ of the Gaussian basis functions \eqref{equ_basis_Gaussian} small and using a large number of 
%regular spaced Gaussian basis functions we can approximate well any sufficiently regular function (which is not changing too rapidly). 
%Here, $\sigma^{2}$ and $\mu_{1},\ldots,\mu_{\featuredim}$ are design parameters which have to be chosen suitably. 

\begin{figure}[htbp]
\begin{center}
    \begin{tikzpicture}
       %   \begin{axis}
       %   [ylabel=$h(x)$,
         %     xlabel=$x$,grid]
             %  \addplot table [x=a, y=b, col sep=comma] {data.csv};      
                 \draw[blue,  thick, domain=-3:3] plot (\x, {2/(1+exp(-\x))});
                  \draw[red,  thick, domain=-3:3] plot (\x,  {exp(-(\x+1)^2)+exp(-(\x-1)^2)}) ;
                  \node [red,right] at (2,0.4) {$\hat{y}=h^{(\vw)}(x)$ with $h^{(\vw)} \!\in\!\hypospace^{(2)}_{\rm Gauss}$} ; 
                   \node [blue,right] at (2,1.6) {$y=h(x)$} ; 
              %   \draw[blue,  thick, domain=-1:0] plot (\x,  {1+\x}); 
                % \draw[blue,  thick, domain=0:1] plot (\x,  {1-\x});    
                 %  \draw[blue,  thick, domain=1:3] plot (\x,  {0});           
          \draw[->] (-3.5,0) -- (3.5,0) node[right] {$x$};
   \draw[->] (0,-0.25) -- (0,1.5) node[above] {$y$};
           %      \draw[dashed, thick, domain=1:3.6] plot (\x,  {\x - 1}) node[right] {$ f(\vw_{0})\!+\!(\vw\!-\!\vw_{0})^{T} \nabla f(\vw_{0})$};
                    \foreach \y/\ytext in {0/0, 1/1}
    \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext$};  
                        \foreach \x/\xtext in {-3/-3,-2/-2,-1/-1,0/0,1/1, 2/2,3/3}
    \draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\xtext$};  
       %   \end{axis}
     \end{tikzpicture}
     \vspace*{-10mm}
\end{center}\caption{The true relation $x \mapsto y=h(x)$ (blue) between feature $x$ and label $y$ is highly non-linear. We might predict 
the label using a non-linear predictor $\hat{y}=h^{(\vw)}(x)$ with some weight vector $\vw \in \mathbb{R}^{2}$ and $h^{(\vw)} \!\in\!\hypospace^{(2)}_{\rm Gauss}$.} 
\label{fig_lin_bas_expansion}
\end{figure}

\begin{center}
\framebox[0.96\textwidth]{
 \parbox{0.9\textwidth}{
{\bf Exercise.} Try to approximate the hypothesis map depicted in Figure \ref{fig_triangle} by an element of $\hypospace_{\rm Gauss}$ (see \eqref{equ_def_Gauss_hypospace}) 
using $\sigma=1/10$, $\featuredim=10$ and $\mu_{j} = -1 + (2j/10)$. 
}}
\end{center}



\section{Logistic Regression} 
\label{sec_LogReg}

Logistic regression is a method for classifying data points which are characterized by 
feature vectors $\mathbf{x} \in \mathbb{R}^{\featuredim}$ (feature space $\featurespace=\mathbb{R}^{\featuredim}$) 
according to two categories which are encoded by a label $y$. It will be convenient to 
use the label space $\labelspace = \mathbb{R}$ and encode the two label values 
as $y=1$ and $y=-1$. Logistic regression learns a predictor out of the hypothesis space 
$\hypospace^{(\featuredim)}$ (see \eqref{equ_lin_hypospace}).\footnote{It is important to 
	note that logistic regression can be used with an arbitrary label space which contains 
	two different elements. Another popular choice for the label space is $\labelspace=\{0,1\}$.} 
Note that the hypothesis space is the same as used in linear regression (see Section \ref{sec_lin_regression}). 

At first sight, using predictor maps $h \in \hypospace^{(\featuredim)}$ might seem wasteful. 
Indeed, a linear map $h(\vx) = \vw^T \vx$, with some weight vector $\vw \in \mathbb{R}^{\featuredim}$, 
can take on any real number, while the label $y \in \{-1,1\}$ takes on only one of the two 
real numbers $1$ and $-1$. It turns out to be quite useful to use classifier maps 
$h$ which can take on arbitrary real numbers. 

We can always threshold the value $h(\vx)$ to obtain a predicted label $\hat{y} \in \{-1,1\}$. 
In what follows we implicitly assume that the predicted label is obtained by thresholding the 
predictor map at $0$, $\hat{y}$ is $1$ if $h(\vx)\geq 0$ and $\hat{y}=-1$ otherwise. Thus, 
we use the sign of the predictor map $h(\vx)$ to determine the final prediction for the label. 
The absolute value $|h(\vx)|$ is then used to quantify the reliability of (or confidence in) the 
classification $\hat{y}$. 

Consider two data points with features $\vx^{(1)}, \vx^{(2)}$ and a linear classifier map $h$ yielding 
the function values $h(\vx^{(1)}) = 1/10$ and $h(\vx^{(2)}) = 100$. Whereas both yields the same 
classifications $\hat{y}^{(1)}\!=\!\hat{y}^{(2)}\!=\!1$, the classification of the data point with feature 
vector $\vx^{(2)}$ seems to be much more reliable. In general it is beneficial to complement a particular 
prediction (or classification) result by some reliability information. 

Within logistic regression, we assess the quality of a particular classifier $h^{(\vw)} \in \hypospace^{(\featuredim)}$ 
using the logistic loss \eqref{equ_log_loss} Given some labeled training data $\dataset =\{ \vx^{(\sampleidx)},y^{(\sampleidx)} \}_{\sampleidx=1}^{\samplesize}$, logistic regression amounts to minimizing the empirical risk (average logistic loss) 
\begin{align} 
\label{equ_def_emp_risk_logreg}
\emperror ( \vw | \dataset ) & = (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize}  \log( 1+ \exp ( - y^{(\sampleidx)} h^{(\vw)}(\vx^{(\sampleidx)}))) \nonumber \\
& \stackrel{h^{(\vw)}(\vx)=\vw^{T} \vx}{=}  (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \log( 1+ \exp ( - y^{(\sampleidx)} \vw^{T} \vx^{(\sampleidx)})).
\end{align} 
Once we have found the optimal weight vector $\widehat{\vw}$ which minimizes \eqref{equ_def_emp_risk_logreg}, 
we can classify a data point based on its features $\vx$ according to 
\begin{equation}
\label{equ_class_logreg}
\hat{y} =  \begin{cases} 1 & \mbox{ if } h^{(\widehat{\vw})}(\vx) \geq 0 \\ -1 & \mbox{ otherwise.} \end{cases}
\end{equation} 
Since $h^{(\widehat{\vw})}(\vx) = \big(\widehat{\vw}\big)^{T} \vx$ (see \eqref{equ_lin_hypospace}), 
the classifier \eqref{equ_class_logreg} amounts to testing whether $\big(\widehat{\vw}\big)^{T} \vx \geq 0$ 
or not. Thus, the classifier \eqref{equ_class_logreg} partitions the feature space $\featurespace\!=\!\mathbb{R}^{\featuredim}$ 
into two half-spaces $\mathcal{R}_{1}\!=\!\big\{ \vx: \big(\widehat{\vw}\big)^{T} \vx\!\geq\!0 \big\}$ and 
$\mathcal{R}_{-1}\!=\!\big\{ \vx: \big(\widehat{\vw}\big)^{T} \vx\!<\!0 \big\}$ which are separated by the 
hyperplane $\big(\widehat{\vw}\big)^{T} \vx =0$ (see Figure \ref{fig_lin_dec_boundary}). Any data point 
with features $\vx \in \mathcal{R}_{1}$ ($\vx \in \mathcal{R}_{-1}$) is classified as $\hat{y}\!=\!1$ ($\hat{y}\!=\!-1$). 

Logistic regression can be interpreted as a particular probabilistic inference method. This interpretation is 
based on modelling the labels $y \in \{-1,1\}$ as i.i.d.\ random variables with some probability $\prob{y\!=\!1}$ 
which is parameterized by a linear predictor $h^{(\vw)}(\vx)\!=\!\vw^{T} \vx$ via  
\begin{equation}
\log \prob {y=1}/(1 - \prob{y=1}) = \vw^{T} \vx, 
\end{equation} 
or, equivalently, 
\begin{equation}
\label{equ_prob_model_logistic_model}
 \prob{y=1}= 1/(1 + \exp(- \vw^{T} \vx) ). 
\end{equation} 
Since $\prob{y=1} + \prob{y=-1}=1$, 
\begin{align}
\label{equ_prob_model_logistic_model_minus_1}
\prob{y=-1} & =  1 - \prob{y=1} \nonumber \\
% & \stackrel{\eqref{equ_prob_model_logistic_model}}{=} 1 - 1/2 
& \stackrel{\eqref{equ_prob_model_logistic_model}}{=} 1 - 1/(1+ \exp(-\vw^{T} \vx)) \nonumber \\
& = 1/(1+ \exp(\vw^{T} \vx)).
\end{align}
%& \stackrel{\eqref{equ_prob_model_logistic_model}}{=} 1 -  1/(1 + \exp(- \vw^{T} \vx) )
 %& =  1/(1 + \exp(\vw^{T} \vx) ). 
 %\end{align} 
 
Given the probabilistic model \eqref{equ_prob_model_logistic_model}, a principled approach to 
choosing the weight vector $\vw$ is based on maximizing the probability (or likelihood) of the 
observed dataset $\dataset=\{ (\vx^{(\sampleidx)},y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize}$ 
under the probabilistic model \eqref{equ_prob_model_logistic_model}. This yields the maximum 
likelihood estimator 
\begin{align}
\label{equ_deriv_prog_logreg}
\hat{\vw} & = \argmax_{\vw \in \mathbb{R}^{\featuredim}} \prob{ \{y^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize}} \nonumber \\
& \stackrel{y^{(\sampleidx)}\mbox{ i.i.d.}}{=} \argmax_{\vw \in \mathbb{R}^{\featuredim}} \prod_{\sampleidx=1}^{\samplesize} \prob { y^{(\sampleidx)}}\nonumber \\
& \stackrel{\eqref{equ_prob_model_logistic_model},\eqref{equ_prob_model_logistic_model_minus_1}}{=} \argmax_{\vw \in \mathbb{R}^{\featuredim}} \prod_{\sampleidx=1}^{\samplesize} 1/(1 + \exp(- y^{(\sampleidx)} \vw^{T} \vx^{(\sampleidx)}) ). 
\end{align}
The maximizer of a positive function $f(\vw)>0$ is not affected by replacing $f(\vw)$ with $\log f(x)$, i.e., 
$\argmax\limits_{\vw \in \mathbb{R}^{\featuredim}} h(\vw)\!=\!\argmax\limits_{\vw \in \mathbb{R}^{\featuredim}}\log h(\vw)$. 
Therefore, \eqref{equ_deriv_prog_logreg} can be further developed as 
\begin{align}
\label{equ_deriv_prog_logreg_2}
\hat{\vw} & \stackrel{\eqref{equ_deriv_prog_logreg}}{=} \argmax_{\vw \in \mathbb{R}^{\featuredim}} \sum_{\sampleidx=1}^{\samplesize} - \log\big(1\!+\!\exp(- y^{(\sampleidx)} \vw^{T} \vx^{(\sampleidx)}) \big)  \nonumber \\
& = \argmin_{\vw \in \mathbb{R}^{\featuredim}} (1/\samplesize)\sum_{\sampleidx=1}^{\samplesize} \log\big(1\!+\!\exp(- y^{(\sampleidx)}\vw^{T} \vx^{(\sampleidx)}) \big). 
\end{align}
Comparing \eqref{equ_deriv_prog_logreg_2} with \eqref{equ_def_emp_risk_logreg} reveals 
that logistic regression is nothing but maximum likelihood estimation of the weight 
vector $\vw$ in the probabilistic model \eqref{equ_prob_model_logistic_model}. 



\section{Support Vector Machines} 
\label{sec_SVM} 
Support vector machines (SVM) are classification methods which use the 
hinge loss \eqref{equ_hinge_loss} to evaluate the quality of a given classifier 
$h \in \hypospace$. The most basic variant of SVM applies to ML problems 
with feature space $\featurespace=\mathbb{R}^{\featuredim}$, label space $\labelspace = \{-1,1\}$ 
and the hypothesis space $\hypospace^{(\featuredim)}$ \eqref{equ_lin_hypospace}, 
which is also underlying linear regression (see Section \ref{sec_lin_regression}) 
and logistic regression (see Section \ref{sec_LogReg}). 

The {\bf soft-margin} SVM \cite[Chapter 2]{LampertNowKernel} uses the loss  
\begin{align}
\loss{(\vx,y)}{h^{(\vw)}} & \defeq  \max \{ 0 , 1 - y\cdot h^{(\vw)}(\vx) \}  + \lambda \| \vw \|^{2} \nonumber \\
   & \hspace*{-3mm}\stackrel{h^{(\vw)}(\vx) = \vw^{T} \vx}{=}  \max \{ 0 , 1 - y\cdot \vw^{T} \vx \} + \lambda \| \vw \|^{2} \label{equ_loss_svm}
\end{align}
with a tuning parameter $\lambda >0$. According to \cite[Chapter 2]{LampertNowKernel}, a classifier $h^{(\vw_{\rm SVM})}$ 
minimizing the loss \eqref{equ_loss_svm}, averaged over some labeled data points $\dataset = \{ ( \vx^{(\sampleidx)}, y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize}$, 
is equivalent to maximizing the distance (margin) $\xi$ between the decision boundary, given by 
the set of points $\vx$ satisfying $\vw_{\rm SVM}^{T} \vx=0$, and each of the two classes 
$\mathcal{C}_{1}\!=\!\{ \vx^{(\sampleidx)} : y^{(\sampleidx)}\!=\!1\}$ and 
$\mathcal{C}_{2}\!=\!\{ \vx^{(\sampleidx)} : y^{(\sampleidx)}\!=\!-1 \}$. Maximizing this margin is 
sensible as it ensures that the resulting classifications are robust against small (relative to the margin) 
perturbations of the features (see Section \ref{sec_robustness}). 

As depicted in Figure \ref{fig_svm}, the margin between the decision boundary and 
the classes $\mathcal{C}_{1}$ and $\mathcal{C}_{2}$ is typically determined by few 
data points (such as $\vx^{(6)}$ in Figure \ref{fig_svm}) which are closest to the 
decision boundary. Such data points are referred to as {\bf support vectors} and 
entirely determine the resulting classifier $h^{(\vw_{\rm SVM})}$. In other words, once 
the support vectors are identified the remaining data points become irrelevant for 
learning the classifier $h^{(\vw_{\rm SVM})}$. 

\begin{figure}[htbp]
\begin{center}
\begin{tikzpicture}[auto,scale=0.8]
%\draw [thick] (0,-3) rectangle (4,4) node [anchor=east,above] {$\featurespace$} ;
\draw [thick] (1,2) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(5)}$};
\draw [thick] (0,1.6) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(4)}$};
\draw [thick] (0,3) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(3)}$};
\draw [thick] (2,1) circle (0.1cm)node[anchor=east,above] {\hspace*{0mm}$\vx^{(6)}$};
\node[] (B) at (-2,0) {``support vector''};
\draw[->,dashed] (B) to (1.9,1) ; 
\draw [|<->|,thick] (2.05,0.95)  -- (2.75,0.25)node[pos=0.5] {$\xi$} ; 
\draw [thick] (1,-1.5) -- (4,1.5) node [right] {$h^{(\vw)}$} ; 
\draw [thick] (3,-1.9) rectangle ++(0.1cm,0.1cm) node[anchor=west,above]  {\hspace*{0mm}$\vx^{(2)}$};
\draw [thick] (4,.-1) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {\hspace*{0mm}$\vx^{(1)}$};
\end{tikzpicture}
\caption{The SVM aims at a classifier $h^{(\vw)}$ with small hinge loss. Minimizing hinge loss of a 
 classifier is the same as maximizing the margin $\xi$ between the decision boundary (of the classifier) 
 and each class of the training set.}
\label{fig_svm}
\end{center}
\end{figure}

We highlight that both, the SVM and logistic regression amount to linear 
classifiers $h^{(\vw)} \in \hypospace^{(\featuredim)}$ (see \eqref{equ_lin_hypospace}) 
whose decision boundary is a hyperplane in the feature space $\featurespace = \mathbb{R}^{\featuredim}$ (see Figure \ref{fig_lin_dec_boundary}). The difference between SVM and logistic regression is the loss function used for 
evaluating the quality of a particular classifier $h^{(\vw)} \in \hypospace^{(\featuredim)}$. 
The SVM uses the hinge loss \eqref{equ_hinge_loss} which is the best convex approximation 
to the $0/1$ loss \eqref{equ_def_0_1}. Thus, we expect the classifier obtained by the SVM to 
yield a smaller classification error probability $\prob{ \hat{y} \neq y }$ (with $\hat{y} =1$ 
if $h(\vx) >0$ and $\hat{y}=-1$ otherwise) compared to logistic regression which uses 
the logistic loss \eqref{equ_log_loss}. 

The statistical superiority of the SVM comes at the cost of increased computational 
complexity. In particular, the hinge loss \eqref{equ_hinge_loss} is non-differentiable 
which prevents the use of  simple gradient-based methods (see Chapter \ref{ch_GD}) 
and requires more advanced optimization methods. In contrast, the logistic loss \eqref{equ_log_loss} 
is convex and differentiable which allows to apply simple iterative methods for minimization 
of the loss (see Chapter \ref{ch_GD}). 


\section{Bayes' Classifier}
\label{sec_NaiveBayes}

Consider a classification problem involving data points characterized by features 
$\vx \in \featurespace$ and associated with labels $y \in \labelspace$ out of a discrete 
label space $\labelspace$. The family of Bayes' classifier methods is based on ML 
problems using the $0/1$ loss \eqref{equ_def_0_1} for assessing the quality of 
classifiers $h$. 

The goal of ML is to find (or learn) a classifier $h: \featurespace \rightarrow \labelspace$ 
such that the predicted (or estimated) label $\hat{y} = h(\vx)$ agrees with the true label 
$y \in \labelspace$ as much as possible. Thus, it is reasonable to assess the quality of 
a classifier $h$ using the $0/1$ loss \eqref{equ_def_0_1}. If we model the data points, with 
its features $\vx$ and label $y$, as i.i.d. random variables, the $0/1$ loss approximates the 
misclassification (error) probability  $P_{\rm err} = \prob{ y \neq h(\vx) }$. 


An important subclass of Bayes' classifiers uses the hypothesis space \eqref{equ_lin_hypospace} 
which is also underlying logistic regression (see Section \ref{sec_LogReg}) and the SVM (see 
Section \ref{sec_SVM}. Thus, logistic regression, the SVM and Bayes' classifiers yield linear classifiers 
(see Figure \ref{fig_lin_dec_boundary}) which partition the feature space $\featurespace$ 
into two half-spaces: one half-space consists of all feature vectors $\vx$ which result in the 
predicted label $\hat{y}=1$ and the other half-space constituted by all feature vectors $\vx$ 
which result in the predicted label $\hat{y}=-1$. The difference between these three linear 
classifiers is how they choose these half-spaces by using different loss-functions. We will 
discuss Bayes' classifier methods in more detail in Section \ref{sec_ERM_Bayes}.  

 
\section{Kernel Methods} 
\label{sec_kernel_methods}

Consider a ML (classification or regression) problem with an 
underlying feature space $\featurespace$. In order to predict 
the label $y \in \labelspace$ of a data point based on its 
features $\vx \in \featurespace$, we apply a predictor $h$ 
selected out of some hypothesis space $\hypospace$. Let 
us assume that the available computational infrastructure only 
allows to use a linear hypothesis space $\hypospace^{(\featuredim)}$ (see \eqref{equ_lin_hypospace}).

For some applications using only linear predictor maps in 
$\hypospace^{(\featuredim)}$ is not sufficient to model the 
relation between features and labels (see Figure \ref{fig_scatterplot_poly} 
for a data set which suggests a non-linear relation between 
features and labels). In such cases it is beneficial to add a 
pre-processing step before applying a predictor $h$. 

The family of kernel methods is based on transforming the 
features $\vx$ to new features $\hat{\vx} \in \featurespace'$ 
which belong to a (typically very) high-dimensional space $\featurespace'$ \cite{LampertNowKernel}. It is 
not uncommon that, while the original feature space is a 
low-dimensional Euclidean space (e.g., $\featurespace = \mathbb{R}^{2}$), 
the transformed feature space $\featurespace'$ is an infinite-dimensional function space. 

The rationale behind transforming the original features into a 
new (higher-dimensional) feature space $\featurespace'$ is to 
reshape the intrinsic geometry of the feature vectors 
$\vx^{(\sampleidx)} \in \featurespace$ such that the transformed 
feature vectors $\hat{\vx}^{(\sampleidx)}$ have a ``simpler'' 
geometry (see Figure \ref{fig_kernelmethods}). 

Kernel methods are obtained by formulating ML problems 
(such as linear regression or logistic regression) using the 
transformed features $\hat{\vx}= \phi(\vx)$. A key challenge 
within kernel methods is the choice of the feature map $\phi: \featurespace \rightarrow \featurespace'$ which 
maps the original feature vector $\vx$ to a new feature vector $\hat{\vx}= \phi(\vx)$. 


\begin{figure}[htbp]
\begin{center}
\begin{minipage}{0.45\textwidth}
\begin{tikzpicture}[auto,scale=0.6]
\draw [thick] (-3,-3) rectangle (4,4) node [anchor=east,above] {$\featurespace$} ;
\draw [thick] (2,2) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(5)}$};
\draw [thick] (-2,1.6) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(4)}$};
\draw [thick] (-2,-1.7) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(3)}$};
\draw [thick] (2,-1.9) circle (0.1cm) node[anchor=west] {\hspace*{0mm}$\vx^{(2)}$};
\draw [thick] (.5,.5) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {\hspace*{0mm}$\vx^{(1)}$};
 %       \node[] (B) at (12,2) {$\labelspace$};
  %       \fill  (11.5,2.5)  circle (2pt) node[above] (D) {$y \in \labelspace$};    
   %      \draw [->,thick] (5,3)   --  (11.5,2.5) node[above,midway] {$h(\vx)$} ;
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\begin{tikzpicture}[auto,scale=0.6]
\draw [thick] (0,-4) rectangle (7,3) node [anchor=east,above] {$\featurespace'$} ;
\draw [thick] (3,0) circle (0.1cm)node[anchor=north] {\hspace*{0mm}$\hat{\vx}^{(5)}$};
\draw [thick] (4,0) circle (0.1cm)node[anchor=north] {\hspace*{0mm}$\hat{\vx}^{(4)}$};
\draw [thick] (5,0) circle (0.1cm)node[anchor=north] {\hspace*{0mm}$\hat{\vx}^{(3)}$};
\draw [thick] (6,0) circle (0.1cm) node[anchor=north] {\hspace*{0mm}$\hat{\vx}^{(2)}$};
\draw [thick] (1,0) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {\hspace*{0mm}$\hat{\vx}^{(1)}$};
 %       \node[] (B) at (12,2) {$\labelspace$};
  %       \fill  (11.5,2.5)  circle (2pt) node[above] (D) {$y \in \labelspace$};    
   %      \draw [->,thick] (5,3)   --  (11.5,2.5) node[above,midway] {$h(\vx)$} ;
   \end{tikzpicture}
\end{minipage}
\caption{Consider a data set $\dataset = \{ (\vx^{(\sampleidx)},y^{(\sampleidx)}) \}_{\sampleidx=1}^{5}$ constituted 
by data points with features $\vx^{(\sampleidx)}$ and binary labels $y^{(\sampleidx)}$. Left: In the original feature 
space $\featurespace$, the data points cannot be separated perfectly by any linear classifier. Right: The feature 
map $\phi: \featurespace \rightarrow \featurespace'$ transforms the features $\vx^{(\sampleidx)}$ to the new features 
$\hat{\vx}^{(\sampleidx)}=\phi\big(\vx^{(\sampleidx)}\big)$ in the new feature space $\featurespace'$. In the new 
feature space $\featurespace'$ the data points can be separated perfectly by a linear classifier. }
\label{fig_kernelmethods}
\end{center}
\end{figure}


\newpage
\section{Decision Trees} 
\label{sec_decision_trees}

A decision tree is a flowchart-like description of a map $h: \featurespace \rightarrow \labelspace$ 
which maps the features $\vx \in \featurespace$ of a data point to a predicted label $h(\vx) \in \labelspace$ \cite{hastie01statisticallearning}. 
While decision trees can be used for arbitrary feature space $\featurespace$ and label space $\labelspace$, 
we will discuss them for the particular feature space $\featurespace = \mathbb{R}^{2}$ and label 
space $\labelspace=\mathbb{R}$. %Here, a decision tree describes 
%how a feature vector $\vx=(x_{1},x_{2})^{T}$ is mapped to a predicted label $h(\vx)$. 

We have depicted an example of a decision tree in Figure \ref{fig_decision_tree}. The decision tree consists of 
nodes which are connected by directed edges. It is useful to think of a decision tree as a step-by-step instruction 
(a ``recipe'') for how to compute the predictor value $h(\vx)$ given the input feature $\vx \in \featurespace$. This 
computation starts at the {\bf root node} and ends at one of the {\bf leaf nodes}. A leaf node $m$, which does not 
have any outgoing edges, corresponds to a certain subset or ``region'' $\mathcal{R}_{m} \subseteq \featurespace$ 
of the feature space. The hypothesis $h$ associated with a decision tree is constant over the regions $\mathcal{R}_{m}$, 
such that $h(\vx) = h_{m}$ for all $\vx \in \mathcal{R}_{m}$ and some fixed number $h_{m} \in \mathbb{R}$. 
In general, there are two types of nodes in a decision tree: 
\begin{itemize} 
\item decision (or test) nodes, which represent particular ``tests'' 
about the feature vector $\vx$ (e.g., ``is the norm of $\vx$ larger than $10$?'').
\item leaf nodes, which correspond to subsets of the feature space. 
\end{itemize} 
The particular decision tree depicted in Figure \ref{fig_decision_tree} consists 
of two decision nodes (including the root node) and three leaf nodes. 

Given limited computational resources, we need to restrict ourselves to decision 
trees which are not too large. We can define a particular hypothesis space by 
collecting all decision trees which uses the tests ``$\| \vx - \vu \| \leq r$'' and  
``$\| \vx - \vu \| \leq r$''  (for fixed vectors $\vu$ and $\vv$ and fixed radius $r >0$) 
and depth not larger than $2$.\footnote{The depth of a decision tree is the maximum 
	number of hops it takes to reach a leaf node starting from the root and following 
	the arrows. The decision tree depicted in Figure \ref{fig_decision_tree} has depth $2$.} 
To assess the quality of different decision trees we need to use some loss function. 
Examples of loss functions used to measure the quality of a decision tree are the 
squared error loss (for numeric labels)or the impurity of individual decision 
regressions (for categorical labels). 

In general, we are not interested in one particular decision tree only but 
in a large set of different decision trees from which we choose the most 
suitable given some data (see Section \ref{sec_ERM_decision_tree}). We 
can define a hypothesis space by collecting predictor maps $h$ represented 
by a set of decision trees (such as depicted in Figure \ref{fig_hypospace_DT_depth_2}). 

A collection of decision trees can be constructed based on a fixed set of 
``elementary tests'' on the input feature vector, e.g., $\| \vx \| > 3$, $x_{3} < 1$ 
or a continuous ensemble of test such as $\{ x_{2} > \eta \}_ {\eta \in [0,10]}$. We then 
build a hypothesis space by considering all decision trees not exceeding a maximum 
depth and whose decision nodes implement elementary tests. 

\begin{figure}[htbp]
\begin{minipage}{.45\textwidth} %
\scalebox{0.6}{
\begin{tikzpicture}
%  [scale=0.9, 
%    grow                    = right,
%    sibling distance        = 6em,
%    level distance          = 10em,
%    edge from parent/.style = {draw, -latex},
%    every node/.style       = {font=\footnotesize},
%    sloped
 % ]
  [->,>=stealth',level/.style={sibling distance = 5cm/#1,
   level distance = 1.5cm}] 
  \node [env] {$\| \vx-\vu \| \leq r$?}
    child { node [env] {$h(\vx) = h_{1} $}
      edge from parent node [left,align=center] {no} }
    child { node [env] {$\| \vx\!-\!\vv \|\!\leq\!r$?}
         child { node [env] {$h(\vx)\!=\!h_{2}$}
              edge from parent node [left, align=center] {no} }
         child { node [env] {$h(\vx)\!=\!h_{3}$}
              edge from parent node [right, align=center]
                {yes} }
     edge from parent node [right,align=center] {yes} };
\end{tikzpicture}
}
\end{minipage}
\begin{minipage}{.45\textwidth} %
\hspace*{15mm}
\begin{tikzpicture}
      \draw (-2,2) rectangle (2,-2);
      \begin{scope}
         \clip (-0.5,0) circle (1cm);
         \clip (0.5,0) circle (1cm);
         \fill[color=gray] (-2,1.5) rectangle (2,-1.5);
      \end{scope}
      \draw (-0.5,0) circle (1cm);
      \draw (0.5,0) circle (1cm);
               \draw[fill] (-0.5,0) circle [radius=0.025];
         \node [below right, red] at (-0.5,0) {$\mathcal{R}_{3}$};
                  \node [below left, blue] at (-0.7,0) {$\mathcal{R}_{2}$};
                    \node [above left] at (-0.7,1) {$\mathcal{R}_{1}$};
                  \node [left] at (-0.4,0) {$\mathbf{u}$};
                     \draw[fill] (0.5,0) circle [radius=0.025];
                     \node [right] at (0.6,0) {$\mathbf{v}$};
   \end{tikzpicture}
\end{minipage}
\caption{A decision tree represents a hypothesis $h$ which is constant on subsets $\mathcal{R}_{m}$, i.e., 
$h(\vx)\!=\!h_{m}$ for all $\vx\!\in\!\mathcal{R}_{m}$. Each subset 
$\mathcal{R}_{m}\!\subseteq\!\featurespace$ corresponds to a leaf node in the decision tree.}
\label{fig_decision_tree}
\end{figure} 

\begin{figure}[htbp]
\begin{center}
\begin{minipage}{.4\textwidth}
\scalebox{0.8}{
\begin{tikzpicture} [->,>=stealth',level/.style={sibling distance = 3cm/#1,
   level distance = 1.5cm}] 
\node [env] {$\| \vx-\vu \| \leq r$?}
child {node [env] {$h(\vx)=1$}
  edge from parent node [left,align=center] {no} }
child {node [env] {$h(\vx)=2$}
  edge from parent node [right,align=center] {yes} };
\end{tikzpicture}}
\end{minipage}
\hspace*{3mm}
\begin{minipage}{.4\textwidth}
\scalebox{0.8}{
\begin{tikzpicture} [->,>=stealth',level/.style={sibling distance = 3.5cm,
   level distance = 1.5cm}] 
\node [env] {$\| \vx-\vu \| \leq r$?}
child {node [env] {$h(\vx)=1$}
  edge from parent node [left,align=center] {no} }
child {node  [env] {$\| \vx-\vv \| \leq r$?}    child {node  [env] {$h(\vx) =10$} edge from parent node [left,align=center] {no} }
   child {node  [env] {$h(\vx) =20$} edge from parent node [right,align=center] {yes} }
 edge from parent node [right,align=center] {yes} 
 };
\end{tikzpicture}}
      \end{minipage}
 \end{center}
 \caption{A hypothesis space $\hypospace$ consisting of two decision trees 
 	with depth at most $2$ and using the tests $\| \vx\!-\!\vu \|\!\leq\!r$ and $\|\vx\!-\!\vv \|\!\leq\!r$ 
 	with a fixed radius $r$ and points $\vu$ and $\vv$. }
 \label{fig_hypospace_DT_depth_2}
\end{figure}

A decision tree represents a map $h: \featurespace \rightarrow \labelspace$, which 
is piecewise-constant over regions of the feature space $\featurespace$. These 
non-overlapping regions form a partitioning of the feature space. Each leaf node of 
a decision tree corresponds to one particular region. Using large decision trees, which 
involve many different test nodes, we can represent very complicated partitions 
that resemble any given labeled dataset (see Figure \ref{fig_decisiontree_overfits}). 

This is quite different from ML methods using the linear hypothesis space \eqref{equ_lin_hypospace}, 
such as linear regression, logistic regression or SVM. Such linear maps have a rather 
simple geometry, since a linear map is constant along hyperplanes. In particular, linear 
classifiers partition the feature-space into two half-spaces (see Figure \ref{fig_lin_dec_boundary}). 
In contrast, the geometry of the map represented by a decision tree maps 
can be arbitrary complicated if the decision tree is sufficiently large (deep). % we can construct a sufficiently complex 
%decision tree which partitions the feature space such that each data point belongs to a separate cluster (corresponding to a particular leaf node). 

\begin{figure}[htbp]
\begin{minipage}{0.4\textwidth}
\begin{tikzpicture}[auto,scale=0.5]
%\draw [thick] (-3,-3) rectangle (1,4) node [anchor=east,above] {$\featurespace$} ;
\draw [thick] (5,5) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(3)}$};
\draw [dashed] (3,3) rectangle (6,6) ;
\draw [dashed] (3,0) rectangle (6,3) ;
\draw [dashed] (0,3) rectangle (3,6) ;
\draw [dashed] (0,0) rectangle (3,3) ;
\draw [thick] (1,1) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(4)}$};
%\draw [thick] (-2,-1.7) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(3)}$};
%\draw [thick] (2,-1.9) circle (0.1cm) node[anchor=west] {\hspace*{0mm}$\vx^{(2)}$};
\draw [thick] (5,1) rectangle ++(0.2cm,0.2cm) node[anchor=west,above] {\hspace*{0mm}$\vx^{(2)}$};
\draw [thick] (1,5) rectangle ++(0.2cm,0.2cm) node[anchor=west,above] {\hspace*{0mm}$\vx^{(1)}$};
\draw[->] (-0.5,0) -- (6.5,0) node[right] {$x_{1}$};
\draw[->] (0,-0.5) -- (0,6.5) node[above] {$x_{2}$};
\foreach \y/\ytext in {0/0, 1/1,2/2,3/3,4/4,5/5,6/6} \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext$};  
\foreach \x/\xtext in{0/0, 1/1,2/2,3/3,4/4,5/5,6/6}\draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\xtext$};  
 %       \node[] (B) at (12,2) {$\labelspace$};
  %       \fill  (11.5,2.5)  circle (2pt) node[above] (D) {$y \in \labelspace$};    
   %      \draw [->,thick] (5,3)   --  (11.5,2.5) node[above,midway] {$h(\vx)$} ;
\end{tikzpicture}
\end{minipage}
%\hspace*{-10mm}
\begin{minipage}{0.44\textwidth}
\scalebox{0.6}{
\begin{tikzpicture} [->,>=stealth',level/.style={sibling distance = 3cm/#1,
   level distance = 1.5cm},scale=0.9] 
\tikzstyle{level 1}=[sibling distance=60mm]
  \tikzstyle{level 2}=[sibling distance=30mm]
\node [env] {$x_{1}\!\leq\!3$?}
child  {node [env] {$x_{2}\!\leq\!3?$} 
    child {node [env] {$h(\vx)\!=\!y^{(3)}$}  edge from parent node [left,align=center] {no}}
    child {node [env] {$h(\vx)\!=\!y^{(2)}$}   edge from parent node [left,align=center] {yes}}
  edge from parent node [left,align=center] {no} }
child { node [env]  {$x_{2}\!\leq\!3?$}
   child {node [env] {$h(\vx)\!=\!y^{(1)}$}   edge from parent node [left,align=center] {no} }
    child {node [env] {$h(\vx)\!=\!y^{(4)}$}   edge from parent node [left,align=center] {yes} }
  edge from parent node [right,align=center] {yes}};
\end{tikzpicture}}
\end{minipage}
\caption{Using a sufficiently large (deep) decision tree, we can construct a map $h$ that perfectly fits any given 
labeled dataset $\{(\vx^{(\sampleidx)},y^{(\sampleidx)} ) \}_{\sampleidx=1}^{\samplesize}$ such that 
$h(\vx^{(\sampleidx)})\!=\!y^{(\sampleidx)}$ for $\sampleidx=1,\ldots,\samplesize$.}
\label{fig_decisiontree_overfits}
\end{figure}



\newpage
\section{Artificial Neural Networks -- Deep Learning} 
\label{sec_deep_learning}

Another example of a hypothesis space, which has proven useful 
in a wide range of applications, e.g., image captioning or automated 
translation,  is based on a {\bf network representation} of a predictor 
$h: \mathbb{R}^{\featuredim} \rightarrow \mathbb{R}$. We can define 
a predictor  $h^{(\vw)}: \mathbb{R}^{\featuredim} \rightarrow \mathbb{R}$ 
using an {\bf artificial neural network} (ANN) structure as depicted in Figure \ref{fig_ANN}. 
\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=1cm,
  row sep=-11pt
  },
>=latex, 
scale=0.6
]
\matrix[net] (mat)
{
|[plain]| \parbox{1cm}{\centering input\\layer} & |[plain]| \parbox{1cm}{\centering hidden\\layer} & |[plain]| \parbox{1cm}{\centering output\\layer} \\
|[plain]|&  |[plain]|\\
|[plain]| & \\
& |[plain]| \\
  |[plain]| & |[plain]| \\
& & \\
  |[plain]| & |[plain]| \\
|[plain]| & |[plain]| \\
  |[plain]| & \\
|[plain]| & |[plain]| \\    };
 \draw[<-] (mat-4-1) -- node[above] {$x_{1}$} +(-2cm,0);
  \draw[<-] (mat-6-1) -- node[above] {$x_{2}$} +(-2cm,0);
    \draw[->] (mat-4-1) --  node[above]{$w_{1}$}  (mat-3-2);
      \draw[->] (mat-6-1) --  node[above]{$w_{2}$}  (mat-3-2);
        \draw[->] (mat-4-1) --  node[]{\hspace*{10mm}$w_{3}$}  (mat-6-2);
      \draw[->] (mat-6-1) --  node[above]{$w_{4}$}  (mat-6-2);
              \draw[->] (mat-4-1) --  node[]{\hspace*{10mm}$w_{5}$}  (mat-9-2);
      \draw[->] (mat-6-1) --  node[above]{$w_{6}$}  (mat-9-2);
       \draw[->] (mat-3-2) --  node[]{\hspace*{10mm}$w_{7}$}  (mat-6-3);
      \draw[->] (mat-6-2) --  node[above]{$w_{8}$}  (mat-6-3);
      \draw[->] (mat-9-2) --  node[above]{$w_{9}$}  (mat-6-3);     
      \draw[->] (mat-6-3) -- node[above] {$h^{(\vw)}(\vx)$} +(2.8cm,0);
  \vspace*{-4mm}
\end{tikzpicture}
  \vspace*{-4mm}
\caption{ANN representation of a predictor $h^{(\vw)}(\vx)$ which maps 
	the input (feature) vector $\vx=(x_{1},x_{2})^{T}$ to a predicted label (output) $h^{(\vw)}(\vx)$.}
    \label{fig_ANN}
\end{figure}
A feature vector $\vx \in \mathbb{R}^{\featuredim}$ is fed into the input units, 
each of which reads in one single feature $x_{i} \in \mathbb{R}$. The features 
$x_{i}$ are then multiplied with the weights $w_{j,i}$ associated with the link 
between the $\sampleidx$-th input node (``neuron'') with the $j$-th node 
in the middle (hidden) layer. The output of the $j$-th node in the hidden layer 
is given by $s_{j} = g( \sum_{i=1}^{\featuredim} w_{j,i} x_{i} )$ with some 
(typically non-linear) {\bf activation function} $g(z)$. The input (or activation) 
$z$ for the activation (or output) $g(z)$ of a neuron is a weighted 
(linear) combination $ \sum_{i=1}^{\featuredim} w_{j,i} s_{i}$ of the outputs $s_{i}$ of the nodes in the previous layer. For 
the ANN depicted in Figure \ref{fig_ANN}, the activation of the neuron $s_{1}$ is $z=w_{1,1}x_{1} + w_{1,2}x_{2}$.

Two popular choices for the activation function used within ANNs 
are the {\bf sigmoid function} $g(z) = \frac{1}{1+\exp(-z)}$ or the 
{\bf rectified linear unit} $g(z) = \max\{0,z\}$. An ANN with many, 
say $10$, hidden layers, is often referred to as a {\bf deep neural network} 
and the obtained ML methods are known as {\bf deep learning} methods (see \cite{Goodfellow-et-al-2016} for an in-depth 
introduction to deep learning methods). 

Remarkably, using some simple non-linear activation function $g(z)$ as the building 
block for ANNs allows to represent an extremely large class of predictor maps 
$h^{(\vw)}: \mathbb{R}^{\featuredim} \rightarrow \mathbb{R}$. The hypothesis 
space generated by a given ANN structure, i.e., the set of all predictor maps which 
can be implemented by a given ANN and suitable weights $\vw$, tends to be 
much larger than the hypothesis space \eqref{equ_def_hypo_linear_pred} of 
linear predictors using weight vectors $\vw$ of the same length \cite[Ch. 6.4.1.]{Goodfellow-et-al-2016}. 
It can be shown that an ANN with only one single hidden layer can approximate any given map $h: \featurespace \rightarrow \labelspace=\mathbb{R}$ 
to any desired accuracy \cite{Cybenko1989}. However, a key insight which underlies many 
deep learning methods is that using several layers with few neurons, instead of one 
single layer containing many neurons, is computationally favourable \cite{DBLP:journals/corr/EldanS15}. 
\begin{center}
\begin{figure}[htbp]
	\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=1cm,
  row sep=-9pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| \parbox{1.3cm}{\centering input\\layer} & |[plain]| \parbox{1.3cm}{\centering hidden\\layer} & |[plain]| \parbox{1.3cm}{\centering output\\layer} \\
|[plain]|&  |[plain]|\\
|[plain]| & \\
& |[plain]| \\
  |[plain]| & |[plain]| \\
& & \\
  |[plain]| & |[plain]| \\
|[plain]| & |[plain]| \\
  |[plain]| & \\
|[plain]| & |[plain]| \\    };
 \draw[<-] (mat-4-1) -- node[above] {$x_{0}=1$} +(-2cm,0);
  \draw[<-] (mat-6-1) -- node[above] {$x$} +(-2cm,0);
    \draw[->] (mat-4-1) --  node[above]{$w_{1}$}  (mat-3-2);
      \draw[->] (mat-6-1) --  node[above]{$w_{2}$}  (mat-3-2);
        \draw[->] (mat-4-1) --  node[]{\hspace*{10mm}$w_{3}$}  (mat-6-2);
      \draw[->] (mat-6-1) --  node[above]{$w_{4}$}  (mat-6-2);
              \draw[->] (mat-4-1) --  node[]{\hspace*{10mm}$w_{5}$}  (mat-9-2);
      \draw[->] (mat-6-1) --  node[above]{$w_{6}$}  (mat-9-2);
       \draw[->] (mat-3-2) --  node[]{\hspace*{10mm}$w_{7}$}  (mat-6-3);
      \draw[->] (mat-6-2) --  node[above]{$w_{8}$}  (mat-6-3);
      \draw[->] (mat-9-2) --  node[above]{$w_{9}$}  (mat-6-3);     
      \draw[->] (mat-6-3) -- node[above] {$h^{(\vw)}(x)$} +(2cm,0);
\end{tikzpicture}
\caption{This ANN with one hidden layer defines a hypothesis space consisting of all maps $h^{(\vw)}(x)$ obtained 
by implementing the ANN with different weight vectors $\vw = (w_{1},\ldots,w_{9})^{T}$.} 
\label{fig_a_ANN}
\end{figure}
\end{center}

\begin{figure}[htbp]
\centering
\begin{tikzpicture}[
plain/.style={
  draw=none,
  fill=none,
  },
net/.style={
  matrix of nodes,
  nodes={
    draw,
    circle,
    inner sep=10pt
    },
  nodes in empty cells,
  column sep=2cm,
  row sep=-9pt
  },
>=latex
]
\matrix[net] (mat)
{
|[plain]| &|[plain]| \\
|[plain]|& |[plain]| \\
  |[plain]| & |[plain]| \\
|[plain]|& & |[plain]|\\
|[plain]|& |[plain]| \\
|[plain]| & |[plain]| \\
  |[plain]| &|[plain]| \\  };
% \draw[<-] (mat-4-1) -- node[above] {$x_{0}=1$} +(-2cm,0);
%  \draw[<-] (mat-6-1) -- node[above] {$x$} +(-2cm,0);
   \draw[->] (mat-2-1) node[]{$x_{1}$} --  node[above]{$w_{1}$}  (mat-4-2);
%      \draw[->] (mat-6-1) --  node[above]{$w_{2}$}  (mat-3-2);
        \draw[->] (mat-4-1) node[]{$x_{2}$}  --  node[above]{$w_{2}$}  (mat-4-2) ;
      \draw[->] (mat-6-1) node[]{$x_{3}$}--  node[above]{$w_{3}$}  (mat-4-2);
           %   \draw[->] (mat-4-1) --  node[]{\hspace*{10mm}$w_{5}$}  (mat-9-2);
     % \draw[->] (mat-6-1) --  node[above]{$w_{6}$}  (mat-9-2);
  %     \draw[->] (mat-3-2) --  node[]{\hspace*{10mm}$w_{7}$}  (mat-6-3);
   %   \draw[->] (mat-6-2) --  node[above]{$w_{8}$}  (mat-6-3);
    %  \draw[->] (mat-9-2) --  node[above]{$w_{9}$}  (mat-6-3);     
      \draw[->] (mat-4-2) -- node[above] {$g(z)$} +(2cm,0);
\end{tikzpicture}
\caption{Each single neuron of the ANN depicted in Figure \ref{fig_a_ANN} implements 
	a weighted summation $z=\sum_{i} w_{i} x_{i}$ of its inputs $x_{i}$ followed by applying 
	a non-linear activation function $g(z)$.}
\label{fig_activate_neuron}
\end{figure}

\begin{center}
\framebox[0.96\textwidth]{
 \parbox{0.9\textwidth}{
{\bf Exercise.} Consider the simple ANN structure in Figure \ref{fig_a_ANN} using the 
``ReLu'' activation function $g(z)= \max\{z,0\}$ (see Figure \ref{fig_activate_neuron}). Show 
that there is a particular choice for the weights $\vw =(w_{1},\ldots,w_{9})^{T}$ 
such that the resulting hypothesis map $h^{(\vw)}(x)$ is a triangle as depicted in 
Figure \ref{fig_triangle}. Can you also find a choice for the weights $\vw =(w_{1},\ldots,w_{9})^{T}$ 
that produce the same triangle shape if we replace the ReLu activation function 
with the linear function $g(z) =10 \cdot z$?
}}
\end{center}


\begin{figure}[htbp]
\begin{center}
    \begin{tikzpicture}
       %   \begin{axis}
       %   [ylabel=$h(x)$,
         %     xlabel=$x$,grid]
             %  \addplot table [x=a, y=b, col sep=comma] {data.csv};      
                 \draw[blue,  thick, domain=-3:-1] plot (\x,  {0*\x});
                 \draw[blue,  thick, domain=-1:0] plot (\x,  {1+\x}); 
                 \draw[blue,  thick, domain=0:1] plot (\x,  {1-\x});    
                   \draw[blue,  thick, domain=1:3] plot (\x,  {0});           
          \draw[->] (-3.5,0) -- (3.5,0) node[right] {$x$};
   \draw[->] (0,-0.25) -- (0,1.5) node[above] {$h^{(\vw)}(x)$};
           %      \draw[dashed, thick, domain=1:3.6] plot (\x,  {\x - 1}) node[right] {$ f(\vw_{0})\!+\!(\vw\!-\!\vw_{0})^{T} \nabla f(\vw_{0})$};
                    \foreach \y/\ytext in {0/0, 1/1}
    \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext$};  
                        \foreach \x/\xtext in {-3/-3,-2/-2,-1/-1,0/0,1/1, 2/2,3/3}
    \draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\xtext$};  
       %   \end{axis}
     \end{tikzpicture}
     \vspace*{-10mm}
\end{center}
\caption{A hypothesis map with the shape of a triangle.}
\label{fig_triangle}
\end{figure}

The recent success of ML methods based on ANN with many hidden layers 
(which makes them deep) might be attributed to the fact that the network 
representation of hypothesis maps is beneficial for the computational 
implementation of ML methods. First, we can evaluate a map $h^{(\vw)}$ represented 
by an ANN efficiently using modern parallel and distributed computing 
infrastructure via message passing over the network. Second, the ANN 
representation also allows to efficiently how the loss function changes with small 
modifications of the weights $\vw$. The gradient of the overall loss or empirical risk 
(see Chapter \ref{ch_GD}) can be obtained via a message passing procedure known 
as {\bf back-propagation} \cite{Goodfellow-et-al-2016}.

\section{Maximum Likelihood Methods}
\label{sec_max_iikelihood}

For many applications it is useful to model the observed data points $\vz^{(\sampleidx)}$ 
as realizations of a random variable $\vz$ with probability distribution $\prob{\vz; \mathbf{w}}$ 
which depends on some parameter vector $\vw \in \mathbb{R}^{\featuredim}$. A principled 
approach to estimating the vector $\vw$ based on several independent and identically 
distributed (i.i.d.) realizations $\vz^{(1)},\ldots,\vz^{(\samplesize)}\sim \prob{\vz; \mathbf{w}}$ is 
{\bf maximum likelihood estimation} \cite{LC}. 

Maximum likelihood estimation can be interpreted as an ML problem with a hypothesis space 
parameterized by the weight vector $\vw$, i.e., each element $h^{(\vw)}$ of the hypothesis 
space $\hypospace$ corresponds to one particular choice for the 
weight vector $\vw$, and loss function 
\begin{equation} 
\label{equ_loss_ML}
\loss{\vz}{h^{(\vw)}} \defeq - \log {\rm P} (\vz; \vw). 
\end{equation} 

A widely used choice for the probability distribution $\prob{\vz; \mathbf{w}}$ is a multivariate normal 
distribution with mean ${\bm \mu}$ and covariance matrix ${\bf \Sigma}$, both of which constitute the 
weight vector $\vw = ({\bm \mu}, {\bf \Sigma})$ (we have to reshape the matrix ${\bf \Sigma}$ suitably 
into a vector form). Given the i.i.d.\ realizations $\vz^{(1)},\ldots,\vz^{(\samplesize)} \sim \prob{\vz; \mathbf{w}}$,  
the maximum likelihood estimates $\hat{\bm \mu}$, $\widehat{\bf \Sigma}$ of the mean vector and 
the covariance matrix are obtained via 
\begin{equation}
\label{equ_max_likeli}
\hat{\bm \mu}, \widehat{\bf \Sigma} = \argmin_{{\bm \mu} \in \mathbb{R}^{\featurelen},{\bf \Sigma} \in \mathbb{S}^{\featurelen}_{+}} (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} - \log {\rm P} (\vz^{(\sampleidx)}; ({\bm \mu},{\bf \Sigma})). 
\end{equation} 

The optimization in \eqref{equ_max_likeli} is over all pairs given be some mean 
vector ${\bm \mu} \in \mathbb{R}^{\featurelen}$ and some covariance matrix ${\bf \Sigma} \in \mathbb{S}^{\featurelen}_{+}$. Here, $\mathbb{S}^{\featurelen}_{+}$ denotes the set of all psd Hermitian $\featurelen \times \featurelen$ matrices. 
Note that this maximum likelihood problem \eqref{equ_max_likeli} can be 
interpreted as an instance of ERM \eqref{equ_def_ERM_funs} using the 
particular loss function \eqref{equ_loss_ML}.The resulting estimates are 
given explicitly as 
\begin{equation}
\label{equ_ML_mean_cov_Gauss} 
\hat{\bm \mu} = (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \vz^{(\sampleidx)} \mbox{, and } \widehat{\bf \Sigma} = (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} (\vz^{(\sampleidx)} - \hat{\bm \mu})(\vz^{(\sampleidx)} - \hat{\bm \mu})^{T}.
\end{equation} 

Note that the expressions \eqref{equ_ML_mean_cov_Gauss} 
are valid only when the probability distribution of the data points is 
modelled as a multivariate normal distribution. 

\section{$k$-Nearest Neighbours} 
The class of $k$-nearest neighbour (k-NN) predictors (for continuous label space) or 
classifiers (for discrete label space) is defined for feature spaces $\featurespace$ equipped 
with an intrinsic notion of distance between its elements. Mathematically, such spaces are 
referred to as metric spaces \cite{RudinBookPrinciplesMatheAnalysis}). A prime example of a 
metric space is $\mathbb{R}^{\featuredim}$ with the Euclidean metric induced by the distance 
measure $\| \vx\!-\!\vy \|$ between two vectors $\vx, \vy \in \mathbb{R}^{\featuredim}$. 

The hypothesis space underlying $k$-NN problems consists of all maps $h: \featurespace \rightarrow \labelspace$ 
such that the function value $h(\vx)$ for a particular feature vector $\vx$ depends only on the (labels of the) $k$ 
nearest data points of some labeled training data $\dataset = \{ (\vx^{(\sampleidx)},y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize}$. 

In contrast to the ML problems discussed above in Section \ref{sec_lin_regression} - Section \ref{sec_deep_learning}, 
the hypothesis space of $k$-NN depends on the training data $\dataset$. 
\begin{figure}[htbp]
    \centering
 \begin{tikzpicture}
    % generate random points
    \pgfmathsetseed{1908} % init random with the year Voronoi published his paper ;)
    \def\pts{}
    \xintFor* #1 in {\xintSeq {1}{10}} \do{
  %    \pgfmathsetmacro{\ptx}{.9*\maxxy*rand} % random x in [-.9\maxxy,.9\maxxy]
    %  \pgfmathsetmacro{\pty}{.9*\maxxy*rand} % random y in [-.9\maxxy,.9\maxxy]
      \pgfmathsetmacro{\ptx}{0.8*\maxxy*sin(#1 * 36 )} % random x in [-.9\maxxy,.9\maxxy]
      \pgfmathsetmacro{\pty}{0.8*\maxxy*cos(#1 * 36 )} % random y in [-.9\maxxy,.9\maxxy]
    
      \edef\pts{\pts, (\ptx,\pty)} % stock the random point
    }
    % draw the points and their cells
    \xintForpair #1#2 in \pts \do{
      \edef\pta{#1,#2}
      \begin{scope}
        \xintForpair \#3#4 in \pts \do{
          \edef\ptb{#3,#4}
          \ifx\pta\ptb\relax % check if (#1,#2) == (#3,#4) ?
            \tikzstyle{myclip}=[];
          \else
            \tikzstyle{myclip}=[clip];
          \fi;
          \path[myclip] (#3,#4) to[half plane] (#1,#2);
        }
        \clip (-\maxxy,-\maxxy) rectangle (\maxxy,\maxxy); % last clip
        \pgfmathsetmacro{\randhue}{rnd}
        \definecolor{randcolor}{hsb}{\randhue,.5,1}
        \fill[randcolor] (#1,#2) circle (4*\biglen); % fill the cell with random color
        \fill[draw=red,very thick] (#1,#2) circle (1.4pt); % and draw the point
      \end{scope}
    }
    \node [below] at (0,0.8*\maxxy) {$\vx^{(\sampleidx)}$};
    \pgfresetboundingbox
    \draw (-\maxxy,-\maxxy) rectangle (\maxxy,\maxxy);
  \end{tikzpicture}
    \label{fig_voronoi}
        \caption{A hypothesis map $h$ for $k$-NN with $k=1$ and feature space $\featurespace = \mathbb{R}^{2}$. 
        	The hypothesis map is constant over regions (indicated by the coloured areas) located around feature 
        	vectors $\vx^{(\sampleidx)}$ (indicated by a dot) of a dataset $\dataset=\{(\vx^{(\sampleidx)},y^{(\sampleidx)})\}$.}
\end{figure}

\section{Dimensionality Reduction} 
\label{sec_dim_reduction}
data points are whole datasets (bunch of data point); label is optimal hyperplane that allows 
for optimal dimensionality reduction by projecting onto it; the notion of optimality depends 
on the application at hand; one notion of optimality is obtained from approximation errors (PCA).  

\section{Clustering Methods} 
\label{sec_clustering_methods} 
data points are whose datasets; labels are correct partitioning/clustering; loss function is some notion of purity; 


\section{Reinforcement Learning Methods}
\label{sec_reinflearning_methods}
data points are the states of some (AI) agent characterized by features (sensor readings); 
labels are optimal actions; however we typically have no access to labeled 
data as we cannot try out each and any sequence of actions and such to 
find out the best action in each situation. instead we must construct the 
loss function via a (negative) reward collected over time (e.g. over an episode); 

 
\section{Network Lasso} 
\label{sec_network_lasso}

Maybe the most widely used choice for the feature space $\featurespace$ in 
ML methods is the Euclidean space $\mathbb{R}^{\featuredim}$. If the features 
of the data points are available in numeric form it is quite natural to stack them 
into feature vectors. But even for non-numerical data such as text it is often 
preferable to transform it to numeric features (word-embedding). The feature 
space $\mathbb{R}^{\featuredim}$ is attractive since it has a rich algebraic and 
geometric structure which allows to navigate (search) it efficiently. 

A recent thread in ML is to use feature spaces whose structure better 
reflects the structure of non-Euclidean data. One example of non-Euclidean 
data is network-structured data where individual data points are related 
by some application-specific notion of similarity. For such data it might be useful 
to use as a feature space a graph whose nodes represent individual data points. 
Similar data points are connected by an edge. 


An particular class of ML problems involve partially labeled network-structured data arising in many 
important application domains including signal processing \cite{ChenVarma,Chen2015}, image 
processing \cite{LempKohli2009,ShiMalik2000}, social networks, internet and bioinformatics 
\cite{NewmannBook,SemiSupervisedBook,Fergus2009}. Such network-structured data 
(see Figure \ref{fig_graph_signals} ) can be described by an ``empirical graph'' $\graph=(\nodes,\edges,\mathbf{W})$,  
whose nodes $\nodes$ represent individual data points which are connected by edges $\edges$ 
if they are considered ``similar'' in an application-specific sense. The extend of similarity between 
connected nodes $i,j \in \nodes$ is encoded in the edge weights $W_{i,j} > 0$ which are collected 
into the weight matrix $\mathbf{W} \in \mathbb{R}_{+}^{|\nodes| \times |\nodes|}$.  

The notion of similarity between data points can be based on physical proximity (in time or space), 
communication networks or probabilistic graphical models \cite{LauritzenGM,BishopBook,koller2009probabilistic}. 
Besides the graph structure, datasets carry additional information in the form of labels associated 
with individual data points. In a social network, we might define the personal preference for some 
product as the label associated with a data point (which represents a user profile). Acquiring labels 
is often costly and requires manual labor or experiment design. Therefore, we assume to have 
access to the labels of only few data points which belong to a small ``training set''. 

The availability of accurate network models for datasets provides computational and statistical benefits. 
Computationally, network models lend naturally to highly scalable ML methods which 
can be implemented as message passing over the empirical graph \cite{DistrOptStatistLearningADMM}. 
Network models enable to borrow statistical strength between connected data points, which allows 
semi-supervised learning (SSL) methods to capitalize on massive amounts of unlabeled data \cite{SemiSupervisedBook}. 

The key idea behind many SSL methods is the assumption that labels of close-by data points are similar, 
which allows to combine partially labeled data with its network structure in order to obtain predictors which generalize 
well \cite{SemiSupervisedBook,belkin2004regularization}. While SSL methods on graphs have been 
applied to many application domains, the precise understanding of which type of data allow for 
accurate SSL is still in its infancy \cite{ijcai2017-450,NSZ09,elalaoui16}. 

%Consider a network-structured dataset which we represent by a simple undirected weighted graph $\graph = (\nodes, \edges, \mathbf{W})$. 
%Following \cite{SemiSupervisedBook}, we refer to the graph $\graph$ as the empirical graph associated with a dataset. The nodes $i\!\in\!\nodes$ 
%of the empirical graph $\graph$ represent individual data points. It will be convenient to identify (represent) the nodes by the first 
%$\samplesize$ natural numbers such that $\nodes\!=\! = \{1,\ldots,\samplesize\}$. The undirected edges $\{i,j\} \in \edges$ connect data points 
%which are considered similar (in some domain-specific sense).  Given an edge $\{i,j\} \in \edges$, the nonzero value $W_{i,j}\!>\!0$ represents the strength of the 
%connection $\{i,j\} \in \edges$. The edge set $\edges$ is encoded in the non-zero pattern of the weight matrix $\mathbf{W} \!\in\! \mathbb{R}^{\samplesize \times \samplesize}$ 
%by requiring that 
%\begin{equation}
%\label{equ_edge_set_support_weights}
%\{ i , j \} \in \edges \mbox{ if and only if } W_{i,j}  > 0.  
%\end{equation} 
%In Figure \ref{fig_graph_signals} we depict the empirical graphs of different types of data. 

\begin{figure}[htbp]
\begin{center}
\begin{minipage}[b]{.4\textwidth} 
\begin{tikzpicture}
    \tikzset{x=1cm,y=2cm,every path/.style={>=latex},node style/.style={circle,draw}}
    \node [minimum width=0.05cm]   (x1) at (-1,1)  {};     
    \node [circle,minimum width=0.05cm,draw,label={$x[-1]$}]   (x2) at (0,1)  {};
    \node [circle,minimum width=0.05cm,draw,label={$x[0]$}]   (x3) at (1,1)  {};  
    \node [circle,minimum width=0.05cm,draw,label={$x[1]$}]   (x4) at (2,1)  {};         
    \node [minimum width=0.05cm]   (x5) at (3,1)  {};          
    \draw[line width=0.4,-,dashed] (x1) edge (x2);
    \draw[line width=0.4,-] (x2) edge (x3);
    \draw[line width=0.4,-] (x3) edge (x4);
    \draw[line width=0.4,-,dashed] (x4) edge (x5);
     \coordinate [label=45:(a)] (A) at (1,-0.6);
\end{tikzpicture}
%\includegraphics[width=.3\columnwidth,angle=0]{Networks.eps}
\end{minipage} 
\begin{minipage}[b]{.4\textwidth} 
\begin{tikzpicture}
    \tikzset{x=1cm,y=2cm,every path/.style={>=latex},node style/.style={circle,draw}}
%    \node [minimum width=0.05cm]   (x1) at (-1,0)  {};     
%    \node [circle,minimum width=0.05cm,draw,label={$x[i]$}]   (x3) at (1,0)  {};  
%    \node [circle,minimum width=0.05cm,draw,label={$x[i\!+\!1]$}]   (x4) at (2,0)  {};         
%    \node [minimum width=0.05cm]   (x5) at (3,0)  {};          
%    \draw[line width=0.4,-,dashed] (x1) edge (x2);
%    \draw[line width=0.4,-] (x2) edge (x3);
%    \draw[line width=0.4,-] (x3) edge (x4);
%    \draw[line width=0.4,-,dashed] (x4) edge (x5);
      \foreach \x in {0,...,2}
       \foreach \y in {0,...,2} 
       { \node [circle,minimum width=0.05cm,draw]   (xy\x\y) at (\x,\y)  {}; 
       } 
  \foreach \x in {0,...,2}
   \foreach \y [count=\yi] in {0,...,1}  
      {\draw (xy\x\y)--(xy\x\yi) (xy\y\x)--(xy\yi\x) ;}
     
          \node [circle,minimum width=0.05cm,draw,label={$x[1,1]$}]   (x2) at (0,2)  {};
            \node [circle,minimum width=0.05cm,draw,label={$x[2,1]$}]   (x2) at (1,2)  {};
     \coordinate [label=45:(b)] (A) at (1,-0.6);\end{tikzpicture}
\end{minipage}

\vspace*{6mm}

\begin{minipage}[b]{.8\textwidth} 
\begin{center}
\begin{tikzpicture}
    \tikzset{x=0.5cm,y=3cm,every path/.style={>=latex},node style/.style={circle,draw}}
      \csvreader[ head to column names]%
                {NodesKarate.tex}{}{% 
     \coordinate[] (\nodeID) at (\a,\b);
     \draw [fill=white] (\nodeID) circle (2pt);
    }
     \draw [fill=white] (12) circle (2pt) node [above=2mm] {$x[i]$};
      \csvreader[ head to column names]%
                {EdgesKarate.tex}{}{%
    \draw[line width=0.3,-] (\a) edge (\b);
    } 
          \csvreader[ head to column names]%
                {NodesKarate.tex}{}{%
    \node [fill=white,circle,minimum width=0.05cm,draw]   (\nodeID) at (\a,\b)  {};     
    } 
       \coordinate [label=45:(c)] (A) at (3,1);
\end{tikzpicture}
\end{center}
\end{minipage}
\end{center}
\caption{\label{fig_graph_signals} Examples for the empirical graph of networked data. (a) 
Chain graph representing signal amplitudes of discrete time signals.  (b) Grid graph representing 
pixels of 2D-images. (c) Empirical graph $\graph\!=\!(\nodes,\edges,\mW)$ for a dataset obtained 
from the social relations between members of a Karate club \cite{Zachary77}. The empirical graph 
contains $\samplesize$ nodes $i \!\in\!\nodes\!=\!\{1,\ldots,\samplesize \}$ which represent $\samplesize$ 
individual club members. Two nodes $i,j\!\in\!\nodes$ are connected by an edge $\{i,j\}\!\in\!\edges$ if 
the corresponding club members have interacted outside the club.}
\end{figure}



%Some important ML applications involve datasets which can be represented efficiently by an {\bf empirical graph} $\graph\!=\!(\nodes,\edges)$ 
%whose nodes $\nodes$ represent individual data points $\vx^{(\sampleidx)}$. For example, the node $i \!\in\! \nodes$ might represent a (super-)pixel in 
%image processing, a neuron of a neural network \cite{Goodfellow-et-al-2016} or a social network user profile \cite{BigDataNetworksBook}. 
%
%These applications naturally suggest a notion of similarity between individual data points, e.g., the profiles of 
%befriended social network users or greyscale values of neighbouring image pixels. These domain-specific 
%notions of similarity are represented by the edges of the empirical graph $\graph$, i.e., the nodes $i,j\!\in\!\nodes$ 
%representing similar data points $\vx^{(i)}$ and $\vx^{(j)}$ are connected by an undirected edge $\{i,j\}\!\in\!\edges$. We denote 
%the neighbourhood of the node $i\in \nodes$ by $\mathcal{N}(i) \defeq \{ j \in \nodes: \{i,j\} \in \edges \}$. 
%
%In some applications it is possible to quantify the extent to which data points $\vx^{(i)}$ and $\vx^{(j)}$ are similar, e.g., via the Euclidean distance 
%$\| \vx^{(i)} -  \vx^{(j)} \|$. Given two similar data points $i,j\!\in\!\nodes$, which are connected by an edge $\{i,j\} \in \edges$, 
%we will quantify the strength of their connection by the edge weight $W_{i,j}\!>\!0$ which we collect 
%in the symmetric weight matrix $\mathbf{W} \in \mathbb{R}_{+}^{\samplesize \times \samplesize}$. The 
%absence of an edge between nodes $i,j \in \nodes$ is encoded by a zero weight $W_{i,j}\!=\!0$. 
%Thus the edge structure of the empirical graph $\graph$ is fully specified by the support (locations of 
%the non-zero entries) of the weight matrix $\mathbf{W}$. 

Beside the empirical graph structure $\graph$, a dataset typically conveys additional information, e.g., 
features, labels or model parameters. We can represent this additional information by a graph signal defined over $\graph$. 
A graph signal $h[\cdot]$ is a map $\nodes \rightarrow \mathbb{R}$, which associates every node 
$i\!\in\!\nodes$ with the signal value $h[i] \!\in\! \mathbb{R}$. 

Most methods for processing graph signals rely on a signal model which are inspired by a cluster 
assumption \cite{SemiSupervisedBook}. The cluster assumption requires similar signal values $h[i] \approx h[j]$ 
at nodes $i,j \in \nodes$, which belong to the same well-connected subset of nodes (``cluster'') of the empirical graph. 
The clusteredness of a graph signal $h[\cdot]$ can be measured by the total variation (TV): 
\begin{equation}
\label{equ_def_TV}
 \| h \|_{\rm TV} = \sum_{\{i,j\} \in \edges} W_{i,j} |h(i) - h(j)|. 
\end{equation} 
 
Clustered graph signals arise in digital signal processing which studies graph signals 
defined over the chain graph representing sampling time instants. Signal samples 
at adjacent time instants are strongly correlated for sufficiently high sampling rate. 
Image processing methods rely on close-by pixels tending to be coloured likely 
which amounts to a clustered graph signal over a grid graph representing pixels 
of a 2D image.  

The recently introduced network Lasso (nLasso) amounts to a formal ML problem involving 
network-structured data which can be represented by an empirical graph $\graph$. In particular, 
the hypothesis space of nLasso is constituted by graph signals on $\graph$: 
\begin{equation}
\hypospace  = \{ h: \nodes \rightarrow \labelspace \}.
\end{equation} 
The loss function of nLasso is a combination of squared error and TV (see \eqref{equ_def_TV}) 
\begin{equation}
\loss{(\vx,y)}{h} = (y - h(\vx))^{2} + \lambda \| h  \|_{\rm TV}. 
\end{equation} 
The regularization parameter $\lambda$ allows to trade-off a small prediction error $y-h(\vx)$ 
against ``clusteredness'' of the predictor $h$. 


{\bf Logistic Network Lasso.} The logistic network Lasso \cite{LogisticNLassoAsilomar2018,LogisticNLasso} 
is a modification of the network Lasso (see Section \ref{sec_network_lasso}) for classification problems 
involving partially labeled networked data represented by an empirical graph $\graph=(\nodes,\edges,\mathbf{W})$. 

Each data point $\vz$ is characterized by the features $\vx$ and is associated with a label $y \in \labelspace$, 
taking on values from a discrete label space $\labelspace$. The simplest setting is binary classification where 
each data point has a binary label $y \in \{-1,1\}$. The hypothesis space underlying logistic network Lasso 
is given by the graph signals on the empirical graph: 
\begin{equation}
\hypospace  = \{ h: \nodes \rightarrow \labelspace \}
\end{equation} 
and the loss function is a combination of logistic loss and TV (see \eqref{equ_def_TV})
\begin{equation}
\loss{(\vx,y)}{h} = -\log(1+ \exp(-yh(\vx))) + \lambda \| h  \|_{\rm TV}. 
\end{equation} 

\section{Exercises}

\subsection{How Many Neurons?} 
\label{exercise_how_many_neurons} 
Consider a predictor map $h(x)$ which is piece-wise linear and consisting 
of $1000$ pieces. Assume we want to represent this map by an ANN using 
neurons with ReLU activation functions. How many neurons must the ANN 
at least contain?

\subsection{Linear Classifiers} 
\label{exercise_linear_classifier} 
Consider data points characterized by feature vectors $\vx \in \mathbb{R}^{n}$ and 
binary labels $y \in\{-1,1\}$. We are interested in finding a good linear classifier which is such that 
the feature vectors resulting in $h(\vx) = 1$ is a half-space. Which of the methods 
discussed in this chapter aim at learning a linear classifier?


\subsection{Data Dependent Hypothesis Space} 
\label{exercise_data_dep_hypospace} 
Which of the following ML methods uses a hypothesis space that depends on  
training data point. 
\begin{itemize} 
	\item logistic regression 
	\item linear regression 
	\item k-NN 
\end{itemize}


\newpage
\chapter{Empirical Risk Minimization}
\label{ch_Optimization}

\begin{figure}[htbp]
\begin{center}
    \begin{tikzpicture}
          \begin{axis}
          [ylabel= {},
              xlabel={predictor $h\in \hypospace$},%grid, 
              axis x line=center,
  axis y line=center,
  yscale=1.2,
  xscale=1.2,  
  xtick=\empty,
  ytick=\empty,
 % ytick={0,1,...,2},
 % xlabel={prediction error $y-h(x)$},
  %ylabel={squared error loss $\mathcal{L}$},
  xlabel style={below right},
  ylabel style={above},
  xmin=-2.5,
  xmax=2.5,
  ymin=-0.1,
  ymax=1.2
  ]
              % \addplot [smooth, color=red, ultra thick] table [x=a, y=b, col sep=comma] {logloss.csv};    
            %       \addplot [color=blue, ultra thick] table [x=a, y=b, col sep=comma] {hingeloss.csv};     
\addplot [color=black, ultra thick,dashed] table [x=a, y=b, col sep=comma] {averagepredloss.csv};      
\addplot [color=black, ultra thick] table [x=a, y=b, col sep=comma] {emprisk.csv};      
\end{axis}
  \node [right] at (6.5,3.7) {average prediction error};
  \node [above] at (0,5.8) {empirical risk};
           %\node [left,color=blue] at (3.1,1.5) {hinge loss (for $y\!=\!1)$};
          % \node [left] at (1.5,3) {$0/1$ loss (for $y\!=\!1)$};
     \end{tikzpicture}
     \vspace*{-4mm}
\end{center}
\caption{ML methods aim at learning (finding) the predictor $h \in \hypospace$ with minimal 
average prediction error. However, the average prediction error (dashed curve) cannot be 
determined in general. Empirical risk minimization methods are based on replacing (or approximating) 
the average prediction error with the empirical risk (solid curve) incurred by a predictor when 
applied to a finite set of labeled data points (the training set).}
\label{fig_ERM_idea}
\end{figure}


As detailed in Chapter \ref{ch_Elements_ML}, ML problems (and methods) consist of the following 
components (see Figure \ref{fig_ml_problem}): 
\begin{itemize} 
\item the feature space $\featurespace$ and label space $\labelspace$, 
\item a hypothesis space $\hypospace$ of computationally feasible predictor maps $\featurespace \rightarrow \labelspace$, 
\item and a loss function $\loss{(\vx,y)}{h}$ which measures the error incurred by predictor $h \in \hypospace$. 
\end{itemize} 
Given such a ML problem specification, ML methods aim at finding (learning) an accurate predictor 
map $h$ out of $\hypospace$ such that $h(\vx) \approx y$ for a random data point $(\vx,y)$. We 
make this informal statement precise by using a simple probabilistic model for the data. In particular, 
we assume data points are i.i.d. realizations drawn from some fixed (unknown) probability distribution $p(\vx,y)$. 

The average prediction error or average risk of a given predictor is then defined as the expected loss 
$\expect \big \{ \loss{(\vx,y)}{h} \}$. ML methods can be considered as optimization methods that choose 
(optimize or learn) a predictor out of $\hypospace$ such that it has minimum average risk. If we would 
know the probability distribution of the data, we could in principle readily determine the best predictor 
map by solving an optimization problem. This optimal predictor is known as a Bayes predictor and depends 
on the loss function. For the squared error loss, the Bayes predictor is the posterior mean of $y$ given the 
features $\vx$. 

Note that in practive we cannot directly construct Bayes' optimal predictors using probability calculus since 
we do not know probability distribution underlying the data. However, Bayes' optimal predictors can provide 
a very useful benchmark against we can compare practical ML methods. Let us now see how the construction 
of Bayes' optimal predictors can be adapted to obtain practical ML methods. 


%Note that the average error of a given predictor depends on the probability distribution underlying 
%the data generation which is typically not known. Instead, we can determine only the error 
%(empirical risk) incurred over some labeled data points $\dataset = \{ (\vx^{(\sampleidx)},y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize}$. 

\section{The Basic Idea of ERM} 

In practice, we typically do not know exactly the probability distribution and therefore cannot determine 
precisely the average error uncured by a particular hypothesis. However, we can estimate this 
average error using an empirical average on some data points (``training data''). 
Many ML methods are optimization methods that use the empirical risk on the training data 
as an estimate for the average (expected) error. Thus, in contrast to standard optimization methods, 
these ML methods optimize an estimate (approximation) of the true objective function 
(the average error of a predictor map).  

A key idea underlying many ML methods is to find (learn) a predictor map with small empirical risk, 
which serves as a proxy for the average prediction error. The resulting ML methods are optimization 
methods which solve an {\bf empirical risk minimization (ERM) problem} 
\begin{align}
\label{equ_def_ERM_funs}
   \hat{h} & = \argmin_{h \in \hypospace} \emperror(h|\dataset) \nonumber \\ 
   & \stackrel{\eqref{eq_def_emp_error_101}}{=}  \argmin_{h \in \hypospace} (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \loss{(\vx^{(\sampleidx)},y^{(\sampleidx)})}{h}.
\end{align}
The ERM \eqref{equ_def_ERM_funs} amounts to learning (finding) a good predictor $\hat{h}$ (out of the hypothesis 
space $\hypospace$) by ``training'' it on the dataset $\dataset = \{ (\vx^{(\sampleidx)},y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize}$, 
which is referred to as the {\bf training set}. 

Solving the optimization problem \eqref{equ_def_ERM_funs} provides two things. First, the 
minimizer $\hat{h}$ is a predictor which performs optimal on the training set $\dataset$. Second, 
the corresponding objective value $\emperror(\hat{h}|\dataset)$ (the ``training error'') indicates 
how accurate the predictions of $\hat{h}$ will be. However, as we will discuss in Chapter \ref{ch_overfitting_regularization}, 
in certain ML problems the training error $\emperror(\hat{h}|\dataset)$ obtained for the 
dataset $\dataset$ can be very different from the average prediction error of $\hat{h}$ when applied 
to new data points which are not contained in $\dataset$. 

%????I would like that the "equivalent to find optimum weight vertoc w0" from slide Optimal Linear Regression would be explained again. The mathematical notation used there isn't that familiar for me.?????
%Consider, e.g., the hypothesis space $\hypospace$ of linear predictors is based on a parametrization of the hypothesis $h(\vx^{(\timeidx)})$. 
Given a parameterization $h^{(\vw)}(\vx)$ of the predictor maps in the hypothesis space $\hypospace$, such as within 
linear regression \eqref{equ_def_hypo_linear_pred} or for ANNs (see Figure \ref{fig_ANN}), % within a liner regression model, i.e., $h(\vx^{(\timeidx)}) = \vw^{T}\vx^{(\timeidx)}$, 
we can reformulate the optimization problem \eqref{equ_def_ERM_funs} (with optimization 
domain being a function space!) as an optimization directly over the weight vector:
\begin{equation}
\label{eq_def_ERM_weight}
    \vw_{\rm opt} = \argmin_{\vw \in \mathbb{R}^{\featuredim}} f(\vw) \mbox{ with } f(\vw) \defeq \emperror\big(h^{(\vw)} |\dataset\big). 
\end{equation}
The objective function $f(\vw)$ in \eqref{eq_def_ERM_weight} is the empirical risk $\emperror\big(h^{(\vw)} |\dataset\big)$ achieved by $h^{(\vw)}$ 
when applied to the data points in the dataset $\dataset$. Note that the two formulations \eqref{eq_def_ERM_weight} and \eqref{equ_def_ERM_funs} 
are fully equivalent. In particular, given the optimal weight vector $\vw_{\rm opt}$ solving 
\eqref{eq_def_ERM_weight}, the predictor $h^{(\vw_{\rm opt})}$ is an optimal predictor solving \eqref{equ_def_ERM_funs}. 

Learning a good predictor map via ERM \eqref{equ_def_ERM_funs} can interpreted as 
learning by ``trial and error'': An instructor (or supervisor) provides some snapshots 
$\vz^{(\sampleidx)}$ which are characterized by features $\vx^{(\sampleidx)}$ and associated 
with known labels $y^{(\sampleidx)}$. We (as the learner) then try out some predictor map $h$ 
in order to tell the label $y^{(\sampleidx)}$ only from the snapshot features $\vx^{(\sampleidx)}$ 
and determine the (training) error $\emperror(h|\dataset)$ incurred. If the error $\emperror(h|\dataset)$ 
is too large we try out another predictor map $h'$ instead of $h$ with the hope of achieving 
a smaller training error $\emperror(h'|\dataset)$. 


This principle of learning by supervision, i.e., using labeled data points (``training examples''), could 
also be used to model the development of language in human brains (``concept learning''). Consider a 
child, who should learn the concept ``tractor'' (see Figure \ref{fig_tractors}). We could try to show 
many different pictures to the child and for each picture say ``tractor'' or ``no tractor''. Based on this 
``labeled data'', the child tries to learn the relation between features of an image and the presence 
(or absence) of a tractor in the image. 

\begin{figure}[htbp]
    \centering
    \includegraphics[width=0.7\textwidth]{LabeledDataTractor1.jpg}  
    \label{fig_tractors}
        \caption{A bunch of labeled images. The label $y$ of an image indicates if a tractor is shown ($y=1$) or not ($y=-1$).}
\end{figure}

We highlight that the precise shape of the objective function $f(\vw)$ in \eqref{eq_def_ERM_weight} 
depends heavily on the parametrization of the predictor functions, i.e., how does the predictor $h^{(\vw)}$ 
vary with the weight vector $\vw$. Moreover, the shape of $f(\vw)$ depends also on the choice 
for the loss function $\loss{(\vx^{(\sampleidx)},y^{(\sampleidx)})}{h}$. As depicted in Figure \ref{fig_diff_types_bojec}, 
the different combinations of predictor parametrisation and loss functions can result in objective 
functions with fundamentally different properties such that their optimization is more or less difficult. 

The objective function $f(\vw)$ for the ERM obtained for linear regression (see Section \ref{sec_lin_regression}) 
is differentiable and convex and can therefore be minimized using simple iterative gradient descent methods 
(see Chapter \ref{ch_GD}). In contrast, the objective function $f(\vw)$ of ERM obtained for the SVM (see Section \ref{sec_SVM}) 
is non-differentiable but still convex. The minimization of such functions is more challenging but still tractable 
as there exist efficient convex optimization methods which do not require differentiability of the objective 
function \cite{ProximalMethods}. 

The objective function $f(\vw)$ obtained for ANN tends to be {\bf highly non-convex} 
but still differentiable for particular activation functions. The optimization of non-convex 
objective function is in general more difficult than optimizing convex objective functions. 
However, it turns out that despite the non-convexity, iterative gradient based methods can 
still be successfully applied to solve the ERM \cite{Goodfellow-et-al-2016}. Even more 
challenging is the ERM obtained for decision trees or Bayes' classifiers. These ML problems 
involve non-differentiable and non-convex objective functions. 


	
\begin{figure}[htbp]
\begin{center}
\scalebox{0.8}{
    \begin{tikzpicture}[scale=0.5]
       %   \begin{axis}
       %   [ylabel=$h(x)$,
         %     xlabel=$x$,grid]
             %  \addplot table [x=a, y=b, col sep=comma] {data.csv};      
                 \draw[thick, domain=-3:3]  node[left=2cm,below] {smooth and convex} plot ({\x-5},  {0.2*\x*\x}) node [above] {$f(\vw)$} ;
                         \draw[thick, domain=-3:3] plot ({\x+5},  {0.1*\x*\x*\x +1}) node[below=2cm] {smooth and non-convex} ;                   
                   \draw[thick, domain=-3:0] plot ({\x-5},  {-\x-5}) ;
                     \draw[thick, domain=0:3] plot ({\x-5},  {-5}) node[left=3cm,below] {non-smooth and convex};
                          \draw[thick, domain=-3:0] plot ({\x+5},  { -4})  ;         
                               \draw[thick, domain=0:3] plot ({\x+5},  { -5}) node[below] {non-smooth and non-convex} ;         
%                 \draw[blue,  thick, domain=-1:0] plot (\x,  {1+\x}); 
%                 \draw[blue,  thick, domain=0:1] plot (\x,  {1-\x});    
%                   \draw[blue,  thick, domain=1:3] plot (\x,  {0});           
%          \draw[->] (-3.5,0) -- (3.5,0) node[right] {$x$};
%   \draw[->] (0,-0.25) -- (0,1.5) node[above] {$h^{(\vw)}(x)$};
%           %      \draw[dashed, thick, domain=1:3.6] plot (\x,  {\x - 1}) node[right] {$ f(\vw_{0})\!+\!(\vw\!-\!\vw_{0})^{T} \nabla f(\vw_{0})$};
%                    \foreach \y/\ytext in {0/0, 1/1}
%    \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext$};  
%                        \foreach \x/\xtext in {-3/-3,-2/-2,-1/-1,0/0,1/1, 2/2,3/3}
%    \draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\xtext$};  
%       %   \end{axis}
     \end{tikzpicture}}
\end{center}
\caption{Different types of objective functions obtained for ERM in different settings.}
\label{fig_diff_types_bojec}
\end{figure}



In what follows, we discuss the structure of the ERM obtained for three particular ML problems: 
In Section \ref{sec_ERM_lin_reg}, we discuss the ERM obtained for linear regression (see Section \ref{sec_lin_regression}). 
The resulting ERM has appealing properties as it amounts to minimizing a differentiable (smooth) 
and convex function which can be done efficiently using efficient gradient based methods (see Chapter \ref{ch_GD}). 

We then discuss in Section \ref{sec_ERM_decision_tree} the ERM obtained for decision trees 
which yields a discrete optimization problem and is therefore fundamentally different from the 
smooth ERM obtained for linear regression. In particular, we cannot apply gradient based 
methods (see Chapter \ref{ch_GD}) to solve them but have to rely on discrete search methods. 

Finally, in Section \ref{sec_ERM_Bayes}, we discuss how Bayes' methods can be used to solve 
the non-convex and non-differentiable ERM problem obtained for classification 
problems with the $0/1$ loss \eqref{equ_def_0_1}. 
%problem we will consider decision trees. 

\section{ERM for Linear Regression}
\label{sec_ERM_lin_reg}

Let us now focus on linear regression problem (see Section \ref{sec_lin_regression}) which arises from using the 
squared error loss \eqref{equ_squared_loss} and linear predictor maps $h^{(\vw)}(\vx) = \vx^{T} \vw$. 
Here, we can rewrite the ERM problem \eqref{eq_def_ERM_weight} as 
\begin{align}
\label{equ_def_cost_MSE}
\vw_{\rm opt} & = \argmin_{\vw \in \mathbb{R}^{\featuredim}} f(\vw) \nonumber \\ 
& \mbox{ with } f(\vw)\!\defeq\!(1/|\dataset|) \sum_{(\vx,y)\!\in\!\dataset} (y \!-\! \vx^{T} \vw )^2.
\end{align} 
Here, $|\dataset|$ denotes the cardinality (number of elements) of the set $\dataset$. 
The objective function $f(\vw)$ in \eqref{equ_def_cost_MSE} has some computationally 
appealing properties, since it is convex and smooth (see Chapter \ref{ch_GD}). 

It will be useful to rewrite the ERM problem \eqref{equ_def_cost_MSE} using matrix 
and vector representations of the feature vectors $\vx^{(\sampleidx)}$ and labels 
$y^{(\sampleidx)}$ contained in the dataset $\dataset$. To this end, we stack the 
labels $y^{(\sampleidx)}$ and the feature vectors $\vx^{(\sampleidx)}$, for 
$\sampleidx=1,\ldots,\samplesize$, into a ``label vector'' $\vy$ and ``feature matrix'' $\mX$ as follows
\begin{align}
\label{equ_def_vec_matrix}
\mathbf{y} & = ( y^{(1)},\ldots, y^{(\samplesize)})^{T} \in \mathbb{R}^{\samplesize} \mbox{, and } \nonumber \\ 
\mathbf{X} & = \begin{pmatrix} \vx^{(1)},\ldots,\vx^{(\samplesize)} \end{pmatrix}^{T}\in \mathbb{R}^{\samplesize \times \featuredim}.
\end{align}
This allows to rewrite the objective function in \eqref{equ_def_cost_MSE} as 
\begin{equation}
\label{equ_min_lin_pred_vec_mat}
f(\vw) = (1/\samplesize) \| \vy - \mX\vw \|^{2}_{2}.
\end{equation} 

Inserting \eqref{equ_min_lin_pred_vec_mat} into \eqref{equ_def_cost_MSE} we obtain the quadratic problem 
\begin{align}
\label{equ_quadr_form_linreg}
& \min_{\vw \in \mathbb{R}^{\featuredim}} \underbrace{(1/2) \vw^{T} \mathbf{Q} \vw - \vq^{T}  \vw}_{= f(\vw)} \nonumber \\
& \mbox{ with } \mathbf{Q}= (1/\samplesize) \mathbf{X}^{T} \mathbf{X}, \mathbf{q} =(1/\samplesize) \mathbf{X}^{T} \mathbf{y}. 
\end{align} 
Since $f(\vw)$ is a differentiable and convex function, a necessary and sufficient condition for 
some $\vw_{0}$ to satisfy $f(\vw_{0})\!=\!\min_{\vw \in \mathbb{R}^{\featuredim}} f(\vw)$ is the 
{\bf zero-gradient condition} \cite[Sec. 4.2.3]{BoydConvexBook}
\begin{equation}
\label{equ_zero_gradient}
 \nabla f(\vw_{\rm opt}) = \mathbf{0}.
\end{equation} 
Combining \eqref{equ_quadr_form_linreg} with \eqref{equ_zero_gradient}, yields the following sufficient 
and necessary condition for a weight vector $\vw_{\rm opt}$ to solve the ERM \eqref{equ_def_cost_MSE}: 
\begin{equation}
\label{equ_zero_gradient_lin_reg}
(1/\samplesize) \mX^{T} \mX \vw_{\rm opt} = (1/\samplesize) \mX^{T} \vy.  
\end{equation} 

It can be shown that, for any given feature matrix $\mX$ and label vector $\vy$, there always 
exists at least one optimal weight vector $\vw_{\rm opt}$ which solves \eqref{equ_zero_gradient_lin_reg}. 
The optimal weight vector might not be unique, such that there are several different vectors 
which achieve the minimum in \eqref{equ_def_cost_MSE}. However, any optimal solution $\vw_{\rm opt}$, 
which solves \eqref{equ_zero_gradient_lin_reg}, achieves the same minimum empirical risk 
\begin{equation}
\label{equ_emp_risk_lin_proje}
\emperror(h^{(\vw_{\rm opt})} \mid \dataset) = \min_{\vw \in \mathbb{R}^{\featuredim}} \emperror(h^{(\vw)} \mid \dataset) = \|  (\mathbf{I}- \mathbf{P}) \vy \|^{2}.
\end{equation} 
Here, we used the orthogonal projection matrix $\mathbf{P} \in \mathbb{R}^{\samplesize \times \samplesize}$ on the linear span of the 
feature matrix $\mX = (\vx^{(1)},\ldots,\vx^{(\samplesize)})^{T} \in \mathbb{R}^{\samplesize \times \featuredim}$ (see \eqref{equ_def_vec_matrix}).\footnote{The linear span ${\rm span } \mA$ 
of a matrix $\mA=(\va^{(1)},\ldots,\va^{(\samplesize)}) \in \mathbb{R}^{\featuredim \times \samplesize}$ is the subspace of $\mathbb{R}^{\featuredim}$ consisting of 
all linear combinations of the columns $\va^{(r)} \in \mathbb{R}^{\featuredim}$ of $\mA$.} 

If the feature matrix $\mX$ (see \eqref{equ_def_vec_matrix}) has full column rank, implying invertibility of the matrix $\mX^{T} \mX$, 
the projection matrix $\mathbf{P}$ is given explicitly as 
\begin{equation} 
\nonumber
\mathbf{P} = \mX \big( \mX^{T} \mX \big)^{-1} \mX^{T}. 
\end{equation} 
Moreover, the solution of \eqref{equ_zero_gradient_lin_reg} is then unique and given by 
\begin{equation}
\label{equ_close_form_lin_reg}
\vw_{\rm opt} = \big(  \mX^{T} \mX \big)^{-1} \mX^{T} \vy. 
\end{equation}
The closed-form solution \eqref{equ_close_form_lin_reg} requires the inversion of the $\featuredim \times \featuredim$ 
matrix $\mX^{T} \mX$. Computing the inverse can be computationally challenging for large feature length $\featuredim$ 
(see Figure \ref{fig_snapshot_pixels} for a simple ML problem where the feature length is almost a million). Moreover, 
inverting a matrix which is close to singular typically introduces numerical errors. 


%????
%{\bf Example.} Consider polynomial regression with .... According to Section \ref{}, polyonmial regression is equivalent to linear 
%regression with the feature vectors $\vx^{(\sampleidx)}$ if we stack these feature vectors into the matrix 
%\begin{equation} 
%\label{equ_vandermonde}
%\mathbf{X} = \begin{pmatrix} \end{pmatrix} 
%\end{equation} 
%we could use \eqref{equ_close_form_lin_reg} to find the optimal weight vector. However, it turns out that computing the inverse of the 
%matrix $\big(  \mX^{T} \mX \big)$ with the feature matrix is often challenging numerically. Indeed, the condition number of  $\big(  \mX^{T} \mX \big)$ (see \eqref{equ_def_condition_number})
%tends to be extremely large (see \cite{Gautschi1988}) which makes it difficult to apply standard solver implementations using finite precision arithmetic. 
%????

In Section \ref{sec_lin_reg}, we will discuss an alternative method for computing (approximately) the optimal weight 
vector $\vw_{\rm opt}$ which does not require any matrix inversion. This alternative method, referred to as ``gradient descent'', 
is based on constructing a sequence $\vw^{(0)}, \vw^{(1)},\ldots$ of increasingly accurate approximations of $\vw_{\rm opt}$. 
Two particular benefits of using this iterative method, instead of using the formula \eqref{equ_close_form_lin_reg}, for 
computing $\vw_{\rm opt}$ are (i) it is computationally cheaper and (ii) it also works when the matrix $\mX$ is not full 
rank in which case the formula \eqref{equ_close_form_lin_reg} becomes invalid. 

\section{ERM for Decision Trees}
\label{sec_ERM_decision_tree}

Let us now consider the ERM problem \eqref{equ_def_ERM_funs} for a regression problem with label 
space $\labelspace=\mathbb{R}$, feature space $\featurespace= \mathbb{R}^{\featuredim}$ and using 
a hypothesis space defined by decision trees (see Section \ref{sec_decision_trees}). In stark contrast 
to the ERM problem obtained for linear or logistic regression, the ERM problem obtained for decision 
trees amounts to a {\bf discrete optimization problem}. Consider the particular hypothesis space 
$\hypospace$ depicted in Figure \ref{fig_hypospace_DT_depth_2}. This hypothesis space contains a 
finite number of predictor maps, each map corresponding to a particular decision tree. 

For the small hypothesis space $\hypospace$ in Figure \ref{fig_hypospace_DT_depth_2}, the ERM 
problem is easy since we just have to evaluate the empirical risk for each of the elements in $\hypospace$ 
and pick the one yielding the smallest. However, in ML applications we typically use significantly 
larger hypothesis spaces and then discrete optimization tends to be more complicated compared 
to smooth optimization which can solved by (variants of) gradient descent (see Chapter \ref{ch_GD}). 

A popular approach to ERM for decision trees is to use greedy algorithms which try to expand (grow) 
a given decision tree by adding new branches to leaf nodes in order to reduce the empirical risk 
(see \cite[Chapter 8]{IntroSLR} for more details). 

\begin{center}
\framebox[0.96\textwidth]{
 \parbox{0.9\textwidth}{
The idea behind many decision tree learning methods is quite simple: try out expanding a decision tree by 
replacing a leaf node with a decision node (implementing another ``test'' on the feature vector) in order to 
reduce the overall empirical risk as much as possible. 
}}
\end{center}

Consider the labeled dataset $\dataset$ depicted in Figure \ref{fig_growingatree} and a given decision tree for 
predicting the label $y$ based on the features $\vx$. We start with a very simple tree shown in the top of 
Figure \ref{fig_growingatree}. Then we try out growing the tree by replacing a leaf node with a decision node. 
According to Figure \ref{fig_growingatree}, replacing the right leaf node results in a decision tree which is 
able to perfectly represent the training dataset (it achieves zero empirical risk). 
 
 

\begin{figure}[htbp]
\begin{center}
\begin{minipage}{0.4\textwidth}
\scalebox{0.8}{
\begin{tikzpicture}[auto,scale=0.8]
%\draw [thick] (-3,-3) rectangle (1,4) node [anchor=east,above] {$\featurespace$} ;
\draw [thick] (5,5) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(3)}$};
%\draw [dashed] (3,3) rectangle (6,6) ;
%\draw [dashed] (3,0) rectangle (6,3) ;
%\draw [dashed] (0,3) rectangle (3,6) ;
\draw [dashed] (0,0) rectangle (3,6) ;
\draw [dashed] (3,0) rectangle (6,6) ;
\draw [thick] (1,1) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(4)}$};
%\draw [thick] (-2,-1.7) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(3)}$};
%\draw [thick] (2,-1.9) circle (0.1cm) node[anchor=west] {\hspace*{0mm}$\vx^{(2)}$};
\draw [thick] (5,1) circle (0.1cm)node[anchor=west,above] {\hspace*{0mm}$\vx^{(2)}$};
\draw [thick] (1,5) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {\hspace*{0mm}$\vx^{(1)}$};
\draw[->] (-0.5,0) -- (6.5,0) node[right] {$x_{1}$};
\draw[->] (0,-0.5) -- (0,6.5) node[above] {$x_{2}$};
\foreach \y/\ytext in {0/0, 1/1,2/2,3/3,4/4,5/5,6/6} \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext$};  
\foreach \x/\xtext in{0/0, 1/1,2/2,3/3,4/4,5/5,6/6}\draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\xtext$};  
\end{tikzpicture}}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\scalebox{0.9}{
\begin{tikzpicture} [->,>=stealth',level/.style={sibling distance = 2cm/#1,
   level distance = 1.5cm},scale=0.9]
   \vspace*{-30mm} 
\tikzstyle{level 1}=[sibling distance=30mm]
  \tikzstyle{level 2}=[sibling distance=30mm]
\node [env] {$x_{1}\!\leq\!3$?}
child  {node [env] {$h(\vx)\!=\!\circ$} 
   % child {node [env] {$h(\vx)\!=\!y^{(3)}$}  edge from parent node [left,align=center] {no}}
   % child {node [env] {$h(\vx)\!=\!y^{(2)}$}   edge from parent node [left,align=center] {yes}}
  edge from parent node [left,align=center] {no} }
child { node [env]  {$h(\vx)\!=\!\Box$}
%   child {node [env] {$h(\vx)\!=\!y^{(1)}$}   edge from parent node [left,align=center] {no} }
  %  child {node [env] {$h(\vx)\!=\!y^{(4)}$}   edge from parent node [left,align=center] {yes} }
  edge from parent node [right,align=center] {yes}};
\end{tikzpicture}}
\end{minipage}
\end{center}
\vspace*{5mm}
\begin{minipage}{0.2\textwidth}
\begin{tikzpicture}[auto,scale=0.3]
%\draw [thick] (-3,-3) rectangle (1,4) node [anchor=east,above] {$\featurespace$} ;
\draw [thick] (5,5) circle (0.2cm)node[anchor=west] {\hspace*{0mm}$\vx^{(3)}$};
%\draw [dashed] (3,3) rectangle (6,6) ;
%\draw [dashed] (3,0) rectangle (6,3) ;
\draw [dashed] (3,0) rectangle (3,3) ;
\draw [dashed] (0,0) rectangle (3,6) ;
\draw [dashed] (3,0) rectangle (6,3) ;
\draw [dashed] (3,3) rectangle (6,6) ;
\draw [thick] (1,1) circle (0.2cm)node[anchor=west] {\hspace*{0mm}$\vx^{(4)}$};
%\draw [thick] (-2,-1.7) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(3)}$};
%\draw [thick] (2,-1.9) circle (0.1cm) node[anchor=west] {\hspace*{0mm}$\vx^{(2)}$};
\draw [thick] (5,1) circle (0.2cm)node[anchor=west,above] {\hspace*{0mm}$\vx^{(2)}$};
\draw [thick] (1,5) rectangle ++(0.2cm,0.2cm) node[anchor=west,above] {\hspace*{0mm}$\vx^{(1)}$};
\draw[->] (-0.5,0) -- (6.5,0) node[right] {$x_{1}$};
\draw[->] (0,-0.5) -- (0,6.5) node[above] {$x_{2}$};
%\foreach \y/\ytext in {0/0, 1/1,2/2,3/3,4/4,5/5,6/6} \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext$};  
%\foreach \x/\xtext in{0/0, 1/1,2/2,3/3,4/4,5/5,6/6}\draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\xtext$};  
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.2\textwidth}
\scalebox{0.5}{
\begin{tikzpicture} [->,>=stealth',level/.style={sibling distance = 2cm/#1,
   level distance = 1.5cm},scale=0.9]
   \vspace*{-30mm} 
\tikzstyle{level 1}=[sibling distance=20mm]
  \tikzstyle{level 2}=[sibling distance=25mm]
\node [env] {$x_{1}\!\leq\!3$?}
child  {node [env] {$x_{2}\!\leq\!3?$} 
    child {node [env] {$h(\vx)\!=\!\circ$}  edge from parent node [left,align=center] {no}}
    child {node [env] {$h(\vx)\!=\!\circ$}   edge from parent node [left,align=center] {yes}}
  edge from parent node [left,align=center] {no} }
child { node [env]  {$h(\vx)\!=\!\Box$}
%   child {node [env] {$h(\vx)\!=\!y^{(1)}$}   edge from parent node [left,align=center] {no} }
  %  child {node [env] {$h(\vx)\!=\!y^{(4)}$}   edge from parent node [left,align=center] {yes} }
  edge from parent node [right,align=center] {yes}};
\end{tikzpicture}}
\end{minipage}
\hspace*{10mm}
\begin{minipage}{0.2\textwidth}
\scalebox{0.7}{
\begin{tikzpicture}[auto,scale=0.3]
%\draw [thick] (-3,-3) rectangle (1,4) node [anchor=east,above] {$\featurespace$} ;
\draw [thick] (5,5) circle (0.2cm)node[anchor=west] {\hspace*{0mm}$\vx^{(3)}$};
%\draw [dashed] (3,3) rectangle (6,6) ;
%\draw [dashed] (3,0) rectangle (6,3) ;
\draw [dashed] (3,0) rectangle (3,3) ;
\draw [dashed] (0,0) rectangle (3,6) ;
\draw [dashed] (0,0) rectangle (3,3) ;
\draw [dashed] (0,3) rectangle (3,6) ;
\draw [dashed] (3,0) rectangle (6,6) ;
\draw [thick] (1,1) circle (0.2cm)node[anchor=west] {\hspace*{0mm}$\vx^{(4)}$};
%\draw [thick] (-2,-1.7) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(3)}$};
%\draw [thick] (2,-1.9) circle (0.1cm) node[anchor=west] {\hspace*{0mm}$\vx^{(2)}$};
\draw [thick] (5,1) circle (0.2cm)node[anchor=west,above] {\hspace*{0mm}$\vx^{(2)}$};
\draw [thick] (1,5) rectangle ++(0.2cm,0.2cm) node[anchor=west,above] {\hspace*{0mm}$\vx^{(1)}$};
\draw[->] (-0.5,0) -- (6.5,0) node[right] {$x_{1}$};
\draw[->] (0,-0.5) -- (0,6.5) node[above] {$x_{2}$};
%\foreach \y/\ytext in {0/0, 1/1,2/2,3/3,4/4,5/5,6/6} \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext$};  
%\foreach \x/\xtext in{0/0, 1/1,2/2,3/3,4/4,5/5,6/6}\draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\xtext$};  
\end{tikzpicture}}
\end{minipage}
\begin{minipage}{0.25\textwidth}
\scalebox{0.5}{
\begin{tikzpicture} [->,>=stealth',level/.style={sibling distance = 2cm/#1,
   level distance = 1.5cm},scale=0.9]
   \vspace*{-30mm} 
\tikzstyle{level 1}=[sibling distance=20mm]
  \tikzstyle{level 2}=[sibling distance=25mm]
\node [env] {$x_{1}\!\leq\!3$?}
child  {node [env] {$h(\vx)\!=\!\circ$} 
  edge from parent node [left,align=center] {no} }
child { node [env]  {$x_{2}\!\leq\!3?$ }
   child {node [env] {$h(\vx)\!=\!\Box$}   edge from parent node [left,align=center] {no} }
    child {node [env] {$h(\vx)\!=\!\circ$}   edge from parent node [left,align=center] {yes} }
  edge from parent node [right,align=center] {yes}};
\end{tikzpicture}}
\end{minipage}
\caption{Given the labeled dataset and a decision tree in the top row, we would like to grow the 
decision tree by expanding it at one of its two leaf nodes. The resulting new decision trees obtained 
by expanding different leaf node is shown in the bottom row.}
\label{fig_growingatree}
\end{figure}
 
One important aspect of learning decision trees from labeled data is the question of when to stop growing. 
A natural stopping criterion might be obtained from the limitations in computational resources, i.e., we can 
only afford to use decision trees up to certain maximum depth. Beside the computational limitations, we also 
face statistical limitations for the maximum size of decision trees. Indeed, if we allow very large decision trees, 
which represent highly complicated maps, we might end up overfitting the training data (see Figure \ref{fig_decisiontree_overfits} 
and Chapter \ref{ch_overfitting_regularization}) which is detrimental for the prediction performance of decision 
trees obtained for new data (which has not been used for training or growing the decision tree). 
 
 \section{ERM for Bayes' Classifiers} 
 \label{sec_ERM_Bayes} 
 
 The family of Bayes' classifiers is based on using the $0/1$ loss \eqref{equ_def_0_1} for 
 measuring the quality of a classifier $h$. The resulting ERM is 
 %Thus, in order to minimize the error probability, it is sensible to learn a good classifier $\hat{h}$ via ERM with $0/1$ loss: 
\begin{align}
\label{equ_approx_bayes_class}
\hat{h} & = \argmin_{h \in \hypospace} (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \loss{(\vx^{(\sampleidx)},y^{(\sampleidx)})}{h}   \nonumber \\ 
                      & \stackrel{\eqref{equ_def_0_1}}{=}  \argmin_{h \in \hypospace} (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \mathcal{I} ( h(\vx^{(\sampleidx)}) \neq y^{(\sampleidx)}). %\nonumber  \\
                 %     & \stackrel{\eqref{equ_0_1_approx_prob}}{\approx}\argmin_{h \in \hypospace} \prob{ h(\vx) \neq y} . 
\end{align}
Note that the objective function of this optimization problem is non-smooth 
(non differentiable) and non-convex (see Figure \ref{fig_diff_types_bojec}). 
This prevents us from using standard gradient based optimization methods 
(see Chapter \ref{ch_GD}) to solve \eqref{equ_approx_bayes_class}.

We will now approach the ERM \eqref{equ_approx_bayes_class} via a different 
route by interpreting the data points $(\vx^{(\sampleidx)},y^{(\sampleidx)})$ as 
realizations of i.i.d.\ random variables which are distributed according to some 
probability distribution $p(\vx,y)$. As discussed in Section \ref{sec_lossfct}, the 
empirical risk obtained using $0/1$ loss approximates the error probability $\prob { \hat{y} \neq y }$ 
with the predicted label $\hat{y} = 1$ for $h(\vx) > 0$ and $\hat{y} = -1$ otherwise (see \eqref{equ_0_1_approx_prob}). 
Thus, we can approximate the ERM \eqref{equ_approx_bayes_class} as 
\begin{align}
\label{equ_approx_bayes_class_approx}
\hat{h}    & \stackrel{\eqref{equ_0_1_approx_prob}}{\approx}\argmin_{h \in \hypospace} \prob{ \hat{y} \neq y} . 
\end{align}
Note that the hypothesis $h$, which is the optimization variable 
in \eqref{equ_approx_bayes_class_approx}, enters into the objective 
function of \eqref{equ_approx_bayes_class_approx} via the definition 
of the predicted label $\hat{y}$, which is $\hat{y} = 1 $ if $h(\vx) > 0$ 
and $\hat{y} =-1$ otherwise. 

%There are two challenges here: 
%\begin{itemize}
%\item First, we typically do not know the probability distribution which is underlying the training data points $(\vx^{(\sampleidx)},y^{(\sampleidx)})$ 
%and we therefore cannot evaluate the probability $\prob{ h(\vx) \neq y}$ for a given classifier $h \in \hypospace$. 
%\item Second, even if we are lucky and somehow have perfect knowledge of the underlying probability distribution and, in turn, 
%can compute $\prob{ h(\vx) \neq y}$, we would need to compute $\prob{ h(\vx) \neq y}$ for any possible classifier 
%$h \in \hypospace$ which is typically computationally intractable since the hypothesis space is too large. 
%\end{itemize}
It turns out that if we would know the probability distribution $p(\vx,y)$, which 
is required to compute $\prob{ \hat{y} \neq y}$, the solution of \eqref{equ_approx_bayes_class_approx} 
can be found easily via elementary Bayesian decision theory \cite{PoorDetEst}. In particular, 
the optimal classifier $h(\vx)$ is such that $\hat{y}$ achieves the maximum ``a-posteriori'' 
probability $p(\hat{y}|\vx)$ of the label being $\hat{y}$, given (or conditioned on) the features 
$\vx$. However, since we do not know the probability distribution $p(\vx,y)$, we have to estimate 
(or approximate) it from the observed data points $(\vx^{(\sampleidx)},y^{(\sampleidx)})$ which 
are modelled as i.i.d.\ random variables distributed according to $p(\vx,y)$. 

The estimation of $p(\vx,y)$ can be based on a particular probabilistic model for the features and 
labels which depends on certain parameters and then determining the parameters using maximum 
likelihood (see Section \ref{sec_max_iikelihood}). A widely used probabilistic model is based on 
Gaussian random vectors. In particular, conditioned on the label $y$, we model the feature vector 
$\vx$ as a Gaussian vector with mean ${\bm \mu}_{y}$ and covariance ${\bf \Sigma}$, i.e., 
\begin{equation}
\label{equ_prob_model_Bayes}
p(\vx|y) = \mathcal{N}(\vx;{\bm \mu}_{y},{\bf \Sigma}).\footnote{We use the shorthand $\mathcal{N}(\vx;{\bm \mu},{\bf \Sigma})$ to denote the 
probability density function $$p(\vx) = \frac{1}{\sqrt{{\rm det} (2 \pi {\bf \Sigma})}} \exp\big(- (1/2) (\vx\!-\!{\bm \mu})^{T}{\bf \Sigma}^{-1}(\vx\!-\!{\bm \mu}) \big)$$ of 
a Gaussian random vector $\vx$ with mean ${\bm \mu} = \expect \{ \vx \}$ and covariance matrix ${\bf \Sigma} = \expect \big\{(\vx\!-\!{\bm \mu})  (\vx\!-\!{\bm \mu})^{T} \big\}$.}
\end{equation} 
Note that the mean vector of $\vx$ depends on the label such that for $y=1$ the mean of $\vx$ is 
${\bm \mu}_{1}$, while for data points with label $y=-1$ the mean of $\vx$ is ${\bm \mu}_{-1}$. In 
contrast, the covariance matrix ${\bf \Sigma} = \expect\{ (\vx-{\bm \mu}_{y})(\vx-{\bm \mu}_{y})^{T}|y \}$ 
of $\vx$ is the same for both values of the label $y \in \{-1,1\}$. Note that, while conditioned on $y$ 
the random vector $\vx$ is Gaussian, the marginal distribution of $\vx$ is a Gaussian mixture model 
(see Section \ref{sec_soft_clustering}).
%A ``naive'' Bayes' classifier is obtained by modelling the distribution of the features $\vx \in \mathbb{R}^{\featuredim}$, conditioned on the label $y \in \{-1,1\}$, 
%as a Gaussien random vector $\vx | y \sim \mathcal{N}({\bm \mu}_{y},{\bf \Sigma})$ with mean ${\bm \mu}_{y} \in \mathbb{R}^{\featuredim}$ and diagonal covariance 
%${\bf \Sigma} = {\rm diag} (\sigma_{1}^{2}, \ldots, \sigma_{\featuredim}^{2}) \in \mathbb{R}^{\featuredim \times \featuredim}$. 
For this probabilistic model of features and labels, the optimal classifier minimizing the 
error probability $\prob{\hat{y} \neq y}$ is $\hat{y}\!=\!1$ for $h(\vx)\!>\!0$ and $\hat{y}\!=\!-1$ 
for $h(\vx)\!\leq\!0$ using the classifier map 
\begin{equation}
\label{equ_classif_Bayes}
h(\vx) = \vw^{T} \vx \mbox{ with } \vw =  {\bf \Sigma}^{-1} ({\bm \mu}_{1} - {\bm \mu}_{-1}). 
\end{equation}
Carefully note that this expression is only valid if the matrix ${\bf \Sigma}$ is 
invertible.

We cannot implement the classifier \eqref{equ_classif_Bayes} directly, since we do not know the 
true values of the class-specific mean vectors ${\bm \mu}_{1}$, ${\bm \mu}_{-1}$ and covariance 
matrix ${\bf \Sigma}$. Therefore, we have to replace those unknown parameters with some 
estimates $\hat{\bm \mu}_{1}$, $\hat{\bm \mu}_{-1}$ and $\widehat{\bf \Sigma}$, like the maximum 
likelihood estimates which are given by (see \eqref{equ_ML_mean_cov_Gauss}) 
\begin{align}
\hat{\bm \mu}_{1}  & = (1/\samplesize_{1}) \sum_{\sampleidx=1}^{\samplesize} \mathcal{I}(y^{(\sampleidx)}=1) \vx^{(\sampleidx)}, \nonumber \\
\hat{\bm \mu}_{-1}  & = (1/\samplesize_{-1}) \sum_{\sampleidx=1}^{\samplesize} \mathcal{I}(y^{(\sampleidx)}=-1) \vx^{(\sampleidx)}, \nonumber \\
\hat{\bm \mu} & =  (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \vx^{(\sampleidx)}, \nonumber \\
 \mbox{and } \widehat{\bf \Sigma} & = (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} (\vz^{(\sampleidx)} - \hat{\bm \mu})(\vz^{(\sampleidx)} - \hat{\bm \mu})^{T}, \label{ML_est_naive_Bayes}
\end{align}
with $\samplesize_{1} = \sum_{\sampleidx=1}^{\samplesize}  \mathcal{I}(y^{(\sampleidx)}=1)$ 
denoting the number of data points with label $y=1$ ($\samplesize_{-1}$ is defined similarly). 
Inserting the estimates \eqref{ML_est_naive_Bayes} into \eqref{equ_classif_Bayes} yields the 
implementable classifier 
\begin{equation}
\label{equ_classif_Bayes_impl}
h(\vx) = \vw^{T} \vx \mbox{ with } \vw =  \widehat{\bf \Sigma}^{-1} (\hat{\bm \mu}_{1} - \hat{\bm \mu}_{-1}). 
\end{equation} 
We highlight that the classifier \eqref{equ_classif_Bayes_impl} is only well-defined if the estimated 
covariance matrix $\widehat{\bf \Sigma}$ \eqref{ML_est_naive_Bayes} is invertible. This requires 
to use a sufficiently large number of training data points such that $\samplesize \geq \featuredim$. 

Using the route via maximum likelihood estimation, we arrived at \eqref{equ_classif_Bayes_impl} 
as an approximate solution to the ERM \eqref{equ_approx_bayes_class}. The final classifier 
\eqref{equ_classif_Bayes_impl} turns out to be a linear classifier very much like logistic regression 
and SVM. In particular, the classifier \eqref{equ_classif_Bayes_impl} partitions the feature space 
$\mathbb{R}^{\featuredim}$ into two halfspaces: one for $\hat{y}=1$ and one for $\hat{y}=-1$ 
(see Figure \ref{fig_lin_dec_boundary}). Thus, the Bayes' classifier \eqref{equ_classif_Bayes_impl} 
belongs to the same family (of linear classifiers) as logistic regression and the SVM. 
These three classification methods differ only in the way of choosing the decision boundary 
(see Figure \ref{fig_lin_dec_boundary}) separating the two half-spaces in the feature space. 

For the estimator $\widehat{\bf \Sigma}$ \eqref{equ_ML_mean_cov_Gauss} 
to be accurate (close to the unknown covariance matrix) we need a number 
of data points (sample size) which is at least on the order of $\featuredim^{2}$. 
This sample size requirement might be infeasible for applications with only few 
data points available. 

The maximum likelihood estimate $\widehat{\bf \Sigma}$ \eqref{ML_est_naive_Bayes} 
is not invertible whenever $\samplesize < \featuredim$. In this case, the expression 
\eqref{equ_classif_Bayes_impl} becomes useless. To cope with small sample size 
$\samplesize < \featuredim$ we can simplify the model \eqref{equ_prob_model_Bayes} 
by requiring the covariance to be diagonal ${\bf \Sigma} = {\rm diag} (\sigma_{1}^{2}, \ldots, \sigma_{\featuredim}^{2})$. 
This is equivalent to modelling the individual features $x_{1},\ldots,x_{\featuredim}$ 
of a particular data point as conditionally independent, given the label $y$ the data 
point. The resulting special case of a Bayes' classifier is often referred to as a 
{\bf naive Bayes} classifier. 

We finally highlight that the classifier \eqref{equ_classif_Bayes_impl} 
is obtained using the generative model \eqref{equ_prob_model_Bayes} 
for the data. Therefore, Bayes' classifiers belong to the family of generative 
ML methods which involve modelling the data generation. In contrast, 
logistic regression and SVM do not require a generative model for the 
data points but aim directly at finding the relation between features $\vx$ 
and label $y$ of a data point. These methods belong therefore to the 
family of discriminative ML methods. 


Generative methods such as Bayes' classifier are preferable for 
applications with only very limited amounts of labeled data. Indeed, 
having a generative model such as \eqref{equ_prob_model_Bayes} 
allows to synthetically generate more labeled data by generating 
random features and labels according to the probability distribution 
\eqref{equ_prob_model_Bayes}. We refer to \cite{NIPS2001_2020} 
for a more detailed comparison between generative and discriminative 
methods. 

\section{Online Learning} 
\label{sec_online_learning}

So far we considered the training set to be an unordered set of data points 
whose labels are known. Many applications generate data in a sequential 
fashion, data points arrive incrementally over time. It is then desirable to update 
the current hypothesis as soon as new data arrives. 

ML methods differ in the frequency of iterating the cycle in Figure \ref{fig_AlexMLBP}. Consider a 
temperature sensor which delivers a new measurement every ten seconds. As soon as a new 
temperature measurement arrives, a ML method can use it to improve its hypothesis about 
how the temperature evolves over time. Such ML methods operate in an online fashion by 
continuously learning an improved model as new data arrives.  

To illustrate online learning, we consider the ML problem discussed in Section \ref{sec_putting_togehter_the_pieces}. 
This problem amounts to learning a linear predictor for the label $y$ of data points 
using a single numeric feature $x$. We learn the predictor based on some training 
data \eqref{}. The weight vector for the optimal linear predictor is characterized by \eqref{equ_zero_grad_condition_putting_together_special_form_mtx}. 

Let us assume that the training data is built up sequentially, we start with $m=1$ data points 
in the first time step, then in the next time step collect another data point to get 
$m=2$ data points, \ldots. We denote the feature matrix and label vector at time $m$ 
by $\mX^{(\samplesize)}$ and $\vy^{(\samplesize)}$: 
\begin{align}
m&=1: \quad &\mX^{(1)} & = \big( \vx^{(1)} \big)^{T}  \mbox{ , } & \vy^{(1)} & = \big( y^{(1)} \big)^{T} \mbox{, } \\ 
m&=2: \quad &\mX^{(2)} & = \big( \vx^{(1)},\vx^{(2)} \big)^{T}  \mbox{ , } & \vy^{(2)} & = \big( y^{(1)},y^{(2)} \big)^{T} \mbox{, } \\ 
m&=3: \quad &\mX^{(3)} & = \big( \vx^{(1)},\vx^{(2)},\vx^{(3)} \big)^{T}  \mbox{ , } & \vy^{(3} & = \big( y^{(1)},y^{(2)},y^{(3)} \big)^{T}.
\end{align} 
Note that in this online learning setting, the sample size $\samplesize$ has the meaning of a time index. 

Naively, we could try to solve the optimality condition \eqref{equ_zero_grad_condition_putting_together_special_form_mtx} 
for each time step $\samplesize$. However, this approach does not reuse computations already invested 
in solving \eqref{equ_zero_grad_condition_putting_together_special_form_mtx} at previous time steps $\samplesize' < \samplesize$. 





\section{Training and Inference Periods} 
Some ML methods repeat the cycle in Figure \ref{fig_AlexMLBP} in a 
highly irregular fashion. Consider a large image collection which we 
use to learn a hypothesis about how cat images look like. It might 
be reasonable to adjust the hypothesis by fitting a model to the image 
collection. This fitting or training amounts to repeating the cycle in Figure \ref{fig_AlexMLBP} 
during some specific time period (the ``training time'') for a large number. After the training period, 
we only apply the hypothesis to predict the labels of new images. This second phase is also known 
as inference time and might be much longer compared to the training time. Ideally, we would like 
to have only a very short training period to learn a good hypothesis and then only use the hypothesis 
for inference. 

\section{Exercise} 
\label{sec_exercise_chap_4} 

\subsection{Linear Regression}
\label{ex_chap_5_lin_reg} 
Consider linear regression with squared error loss. When is the optimal linear predictor 
unique. Does there always exist an optimal linear predictor?



\newpage
\chapter{Gradient Based Learning}
\label{ch_GD}

This chapter is devoted to a workhorse of many ML algorithms. This workhorse, 
known as gradient descent, is an optimization method that can be applied to vastly different 
ML applications. Variants of gradient descent are used to tune the weights of 
artificial neural networks within deep learning methods \cite{Goodfellow-et-al-2016}. Gradient 
descent can also be applied to reinforcement learning applications. The difference between 
these applications is merely in the details for how to compute or estimate the gradient and 
how to incorporate the information provided by the gradients. 

In the following, we will mainly focus on ML problems with hypothesis space $\hypospace$ consisting of predictor 
maps $h^{(\vw)}$ which are parametrized by a weight vector $\vw \in \mathbb{R}^{\featuredim}$. Moreover, we 
will restrict ourselves to loss functions $\loss{(\vx,y)}{h^{(\vw)}}$ which depend smoothly on the weight vector $\vw$. 
This requirement on the loss function is not too restrictive as many important ML problems, including linear regression 
(see Section \ref{sec_lin_regression}) and logistic regression (see Section \ref{sec_LogReg}), result in a smooth loss 
function.\footnote{A smooth function $f: \mathbb{R}^{\featuredim} \rightarrow \mathbb{R}$ has continuous partial 
derivatives of all orders. In particular, we can define the gradient $\nabla f(\vw)$ for a smooth function $f(\vw)$ 
at every point $\vw$.}

For a smooth loss function, the resulting ERM (see \eqref{eq_def_ERM_weight})  
\begin{align}
\label{equ_def_ERM_continous}
\vw_{\rm opt} & = \argmin_{\vw \in \mathbb{R}^{\featuredim}}  \emperror( h^{(\vw)} \mid \dataset)  \nonumber \\
& = \underbrace{(1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \loss{(\vx^{(\sampleidx)},y^{(\sampleidx)})}{h^{(\vw)}}}_{\defeq f(\vw)} 
\end{align} 
is a {\bf smooth optimization problem} of the form 
\begin{equation}
\label{equ_generic_smooth_problem}
\min_{\vw \in \mathbb{R}^{\featuredim}} f(\vw)
\end{equation} 
with a smooth function $f: \mathbb{R}^{\featuredim} \rightarrow \mathbb{R}$ of the vector argument $\vw \in \mathbb{R}^{\featuredim}$. 
It turns out that a smooth function $f(\vw$) can be approximated locally around a point $\vw_{0}$ using a hyperplane, which passes through the point 
$(\vw_{0},f(\vw_{0}))$ and having normal vector $\mathbf{n} = (\nabla f(\vw_{0}),-1)$ (see Figure \ref{fig_smooth_function}). 
Indeed, elementary analysis yields the following linear approximation (around a point $\vw_{0}$) \cite{RudinBookPrinciplesMatheAnalysis}
\begin{equation} 
\label{equ_linear_approx_diff}
f(\vw) \approx f(\vw_{0}) + (\vw-\vw_{0})^{T} \nabla f(\vw_{0}) \mbox{ for  all }\vw \mbox{ close to } \vw_{0}.
\end{equation}  
As detailed in the next section, the approximation \eqref{equ_linear_approx_diff} lends naturally to a simple but powerful iterative method for finding the 
minimum of the function $f(\vw)$. This method is known as gradient descent (GD) and (variants of it) underlies many state-of-the 
art ML methods (such as deep learning methods). 

\begin{figure}[htbp]
\begin{center}
\begin{tikzpicture}[scale=0.7]
\node [right] at (-4.1,1.7) {$f(\vw)$} ;
\draw[ultra thick, domain=-4.1:4.1] plot (\x,  {(1/4)*\x*\x});
\draw[dashed, thick, domain=1:3.6] plot (\x,  {\x - 1}) node[right] {$ f(\vw_{0})\!+\!(\vw\!-\!\vw_{0})^{T} \nabla f(\vw_{0})$};
\draw [fill] (2,1) circle [radius=0.05] node[right] {$f(\vw_{0})$};
\draw[thick,->] (3,2) -- (3.5,1.5) node[right] {$\mathbf{n}$};
\end{tikzpicture}
\end{center}
\caption{A smooth function $f(\vw)$ can be approximated locally around a point $\vw_{0}$ using a 
hyperplane whose normal vector $\mathbf{n} = (\nabla f(\vw_{0}),-1)$ is determined by the gradient $\nabla f(\vw_{0})$.}
\label{fig_smooth_function}
\end{figure}



\section{The Basic GD Step}
\label{sec_basic_GD_iteration}

We now discuss a very simple, yet quite powerful, algorithm for finding the weight vector $\vw_{\rm opt}$ which 
solves continuous optimization problems like \eqref{equ_def_ERM_continous}. Let us assume we have already some 
guess (or approximation) $\vw^{(k)}$ for the optimal weight vector $\vw_{\rm opt}$ and would like to improve it to a 
new guess $\vw^{(k+1)}$ which yields a smaller value of the objective function $f(\vw^{(k+1)}) < f(\vw^{(k)})$. 
For a differentiable objective function $f(\vw)$, we can use the approximation 
$f(\vw^{(k+1)})  \approx f(\vw^{(k)}) + (\vw^{(k+1)} - \vw^{(k)})^{T} \nabla f(\vw^{(k)})$ (cf. \eqref{equ_linear_approx_diff}) for $\vw^{(k+1)}$ 
not too far away from $\vw^{(k)}$. %(cf. \eqref{equ_linear_approx_diff}). 
Thus, we should be able to enforce $f(\vw^{(k+1)}) < f(\vw^{(k)})$ by choosing 
\begin{equation} 
\label{equ_def_GD_step}
\vw^{(k+1)} = \vw^{(k)} - \alpha \nabla f(\vw^{(k)})
\end{equation} 
with a sufficiently small {\bf step size} $\alpha>0$ (a small $\alpha$ ensures that the linear approximation \eqref{equ_linear_approx_diff} is valid). 
Then, we repeat this procedure to obtain $\vw^{(k+2)} = \vw^{(k+1)} - \alpha \nabla f(\vw^{(k+1)})$ and so on. 

The update \eqref{equ_def_GD_step} amounts to a {\bf gradient descent (GD) step}. It turns out that for a convex differentiable 
objective function $f(\vw)$ and sufficiently small step size $\alpha$, the iterates $f(\vw^{(k)})$ obtained by repeating 
the GD steps \eqref{equ_def_GD_step} converge to a minimum, i.e., $\lim_{k \rightarrow \infty} f(\vw^{(k)}) = f(\vw_{\rm opt})$ (see Figure \ref{fig_basic_GD_step}). 
When the GD step is used within an ML method (see Section \ref{sec_lin_reg} and Section \ref{sec_LogReg}), the step size $\alpha$ is also referred to 
as the {\bf learning rate}. 

\begin{figure}
\begin{center}
  \begin{tikzpicture}[scale=0.9]
    \draw[loosely dotted] (-4,0) grid (4,4);
       \draw[blue, ultra thick, domain=-4.1:4.1] plot (\x,  {(1/4)*\x*\x});
             \draw[red, thick, domain=3:4.1] plot (\x,  {2*\x - 4});
                 \draw[->] (4,4) -- node[right] {$\nabla f(\vw^{(k)})$} (4,2);
                   \draw[->] (4,4) -- node[above] {$-\alpha \nabla f(\vw^{(k)})$} (2,4);
                   \draw[->] (4,2) -- node[below] {$1$} (3,2) ;
    \draw[->] (-4.25,0) -- (4.25,0) node[right] {$\vw$};
   \draw[->] (0,-0.25) -- (0,4.25) node[above] {$f(\vw)$};
    \draw[shift={(4,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\vw^{(k)}$};
    \draw[shift={(2,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\vw^{(k+1)}$};
    \foreach \y/\ytext in {1/1, 2/2, 3/3, 4/4}
    \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext$};  
\end{tikzpicture}
\end{center}
\caption{The GD step \eqref{equ_def_GD_step} amounts to a shift by $-\alpha \nabla f(\vw^{(k)})$.}
\label{fig_basic_GD_step}
\end{figure}

In order to implement the GD step \eqref{equ_def_GD_step} we need to choose the step size 
$\alpha$ and we need to be able to compute the gradient $\nabla f(\vw^{(k)})$. Both tasks can 
be very challenging for an ML problem. The success of deep learning methods, which represent 
predictor maps using ANN (see Section \ref{sec_deep_learning}), can be partially attributed to 
the ability of computing the gradient $\nabla f(\vw^{(k)})$ efficiently via a message passing protocol 
known as back-propagation \cite{Goodfellow-et-al-2016}. 

For the particular case of linear regression (see Section \ref{sec_lin_regression}) and logistic 
regression (see Section \ref{sec_GD_logistic_regression}), we will present precise conditions 
on the step size $\alpha$ which guarantee convergence of GD in Section \ref{sec_lin_reg} and 
Section \ref{sec_GD_logistic_regression}. Moreover, the objective functions $f(\vw)$ arising 
within linear and logistic regression allow for closed-form expressions of the gradient $\nabla f(\vw)$.

\section{Choosing Step Size} 

\begin{figure}[hbtp]
\begin{center}
\begin{minipage}{0.45\columnwidth}
  \begin{tikzpicture}[xscale=0.4,yscale=0.6]
 \draw[blue, ultra thick, domain=-4.1:4.1] plot (\x,  {(1/4)*\x*\x});
  \draw[] (4,4) circle [radius=0.1];
\node [right] at (4,4) {$f(\vw^{(k)})$};
      \draw[] (3.8,3.61) circle [radius=0.1];
\node [left] at (3.8,3.61) {$f(\vw^{(k\!+\!1)})$};
    \draw[] (3.65,3.33) circle [radius=0.1];
\node [right] at (3.65,3.33) {$f(\vw^{(k\!+\!2)})$};
    \node [below] at (0,-0.2) {(a)};
\end{tikzpicture}
\end{minipage}
\begin{minipage}{0.45\columnwidth}
\begin{tikzpicture}[xscale=0.4,yscale=0.6]
\draw[blue, ultra thick, domain=-4.1:4.1] plot (\x,  {(1/4)*\x*\x});
\draw[] (1,0.25) circle [radius=0.1] node [right] (A) {$f(\vw^{(k)})$} ;
\draw[] (-2,1) circle [radius=0.1] node [left] (B) {$f(\vw^{(k\!+\!1)})$} ;
\draw[] (3,2.25) circle [radius=0.1]  node  [right] (C) {$f(\vw^{(k\!+\!2)})$} ;
\draw[->,dashed] (-2,1) -- (3,2.25) node [midway,above] {\eqref{equ_def_GD_step}};
\draw[->,dashed]  (1,0.25) -- (-2,1) node [midway,above] {\eqref{equ_def_GD_step}};
\node [below] at (0,-0.2) {(b)};
\end{tikzpicture}
\end{minipage}
\end{center}
\caption{Effect of choosing learning rate $\alpha$ in GD step \eqref{equ_def_GD_step} too small (a) or too large (b). 
If the steps size $\alpha$ in the GD step \eqref{equ_def_GD_step} is chosen too small, the iterations make 
only very little progress towards the optimum. If the learning rate $\alpha$ is chosen too large, the iterates $\vw^{(k)}$ 
might not converge at all (it might happen that $f(\vw^{(k+1)}) > f(\vw^{(k)})$!). }
\label{fig_small_large_alpha}
\end{figure}



The choice of the step size $\alpha$ in the GD step \eqref{equ_def_GD_step} has a significant influence on the performance of 
Algorithm \ref{alg:gd_linreg}. If we choose the step size $\alpha$ too large, the GD steps \eqref{equ_def_GD_step} diverge 
(see Figure \ref{fig_small_large_alpha}-(b)) and in turn, Algorithm \ref{alg:gd_linreg} to fail in delivering an approximation of 
the optimal weight vector $\vw_{\rm opt}$ (see \eqref{equ_def_cost_linreg}). If we choose the step size $\alpha$ too small 
(see Figure \ref{fig_small_large_alpha}-(a)), the updates \eqref{equ_def_GD_step} make only very little progress towards 
approximating the optimal weight vector $\vw_{\rm opt}$. In many applications it is possible to repeat the GD steps only for 
a moderate number (e.g., when processing streaming data). Thus, if the GD step size is chosen to small, Algorithm \ref{alg:gd_linreg} 
will fail to deliver a good approximation of $\vw_{\rm opt}$ within an acceptable amount of computation time. 

The optimal choice of the step size $\alpha$ of GD can be a challenging task and many sophisticated approaches 
have been proposed for its solution (see \cite[Chapter 8]{Goodfellow-et-al-2016}). We will restrict ourselves to a simple sufficient 
condition on the step size which guarantees convergence of the GD iterations $\vw^{(k)}$ for $k=1,2,\ldots$. 
If the objective function $f(\vw)$ is convex and smooth, the GD steps \eqref{equ_def_GD_step} converge 
to an optimum $\vw_{\rm opt}$ if the step size $\alpha$ satisfies \cite{nestrov04}
\begin{equation} 
\label{equ_GD_conv_guarantee}
\alpha \leq \frac{1}{\lambda_{\rm max} \big( \nabla^{2} f(\vw) \big) }\mbox{ for all } \vw \in \mathbb{R}^{\featuredim}. 
\end{equation} 
Here, we use the Hessian matrix $ \nabla^{2} f(\vw) \in \mathbb{R}^{\featuredim \times \featuredim}$ of a smooth function $f(\vw)$ 
whose entries are the second-order partial derivatives $\frac{\partial f(\vw)}{\partial w_{i} \partial w_{j}}$ of the function $f(\vw)$. 
It is important to note that \eqref{equ_GD_conv_guarantee} guarantees convergence for every possible initialization $\vw^{(0)}$ 
of the GD iterations. 

Note that while it might be computationally challenging to determine the maximum eigenvalue 
$\lambda_{\rm max} \big( \nabla^{2} f(\vw) \big)$ for arbitrary $\vw$, it might still be feasible to find an 
upper bound $U$ for the maximum eigenvalue. If we know an upper bound $U \geq \lambda_{\rm max} \big( \nabla^{2} f(\vw) \big)$ (valid for all $\vw \in \mathbb{R}^{\featuredim}$), 
the step size $\alpha =1/U$ still ensures convergence of the GD iteration.

\section{When To Stop} 
Fixed number of iteration; use gradient as indicator for distance to optimum; 

\section{GD for Linear Regression} 
\label{sec_lin_reg}

We can now formulate a full-fledged ML algorithm for solving a linear regression problem (see Section \ref{sec_lin_regression}). 
This algorithm amounts to finding the optimal weight vector $\vw_{\rm opt}$ for a linear predictor (see \eqref{equ_lin_hypospace}) 
of the form 
\begin{equation} 
\label{equ_def_lin_pred_GD}
h^{(\vw)}(\vx) = \vw^{T} \vx.
\end{equation}
The optimal weight vector $\vw_{\rm opt}$ for \eqref{equ_def_lin_pred_GD} should minimize the empirical risk (under squared error loss \eqref{equ_squared_loss}) 
\begin{equation} 
\label{equ_def_cost_linreg}
%f(\vw_{\rm opt})= \min_{\vw \in \mathbb{R}^{\featuredim}} 
\emperror(h^{(\vw)}| \dataset) \stackrel{\eqref{eq_def_ERM_weight}}{=} (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} (y^{(\sampleidx)} - \vw^{T} \vx^{(\sampleidx)})^{2}, 
\end{equation}
incurred by the predictor $h^{(\vw)}(\vx)$ when applied to the labeled dataset $\dataset=\{ (\vx^{(\sampleidx)}, y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize}$. 
Thus, $\vw_{\rm opt}$ is obtained as the solution of a particular smooth optimization problem \eqref{equ_generic_smooth_problem}, i.e., 
\begin{equation} 
\label{equ_smooth_problem_linreg}
\vw_{\rm opt} = \argmin_{\vw \in \mathbb{R}^{\featuredim}} f(\vw) \mbox{ with } f(\vw) = (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} (y^{(\sampleidx)} - \vw^{T} \vx^{(\sampleidx)})^{2}. 
\end{equation} 

In order to apply GD \eqref{equ_def_GD_step} to solve \eqref{equ_smooth_problem_linreg}, and to find the optima weight vector $\vw_{\rm opt}$, 
we need to compute the gradient $\nabla f(\vw)$. The gradient of the objective function in \eqref{equ_smooth_problem_linreg} is given by 
\begin{equation}
\label{equ_gradient_linear_regression}
\nabla f(\vw) = -(2/\samplesize) \sum_{\sampleidx=1}^{\samplesize} (y^{(\sampleidx)} - \vw^{T} \vx^{(\sampleidx)}) \vx^{(\sampleidx)}.
\end{equation} 
By inserting \eqref{equ_gradient_linear_regression} into the basic GD iteration \eqref{equ_def_GD_step}, we obtain Algorithm \ref{alg:gd_linreg}. 
\begin{algorithm}[htbp]
\caption{``Linear Regression via GD''}\label{alg:gd_linreg}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\Require   labeled dataset $\dataset=\{ (\vx^{(\sampleidx)}, y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize}$ containing feature vectors 
$\vx^{(\sampleidx)} \in \mathbb{R}^{\featuredim}$ and labels $y^{(\sampleidx)} \in \mathbb{R}$; GD step size $\alpha >0$. 
\Statex\hspace{-6mm}{\bf Initialize:} set $\vw^{(0)}\!\defeq\!\mathbf{0}$; set iteration counter $k\!\defeq\!0$   
\Repeat 
\State $k \defeq k +1$    (increase iteration counter) 
\State  $\vw^{(k)} \defeq \vw^{(k\!-\!1)} + \alpha (2/\samplesize) \sum_{\sampleidx=1}^{\samplesize} (y^{(\sampleidx)} - \big(\vw^{(k\!-\!1)})^{T} \vx^{(\sampleidx)}) \vx^{(\sampleidx)}$  (do a GD step \eqref{equ_def_GD_step})
\Until convergence 
\Ensure $\vw^{(k)}$ (which approximates $\vw_{\rm opt}$ in \eqref{equ_smooth_problem_linreg})
\end{algorithmic}
\end{algorithm}

Let us have a closer look on the update in step $3$ of Algorithm \ref{alg:gd_linreg}, which is 
\begin{equation}
\label{equ_update_gd_linreg}
\vw^{(k)} \defeq \vw^{(k\!-\!1)} + \alpha (2/\samplesize) \sum_{\sampleidx=1}^{\samplesize} (y^{(\sampleidx)} - \big(\vw^{(k\!-\!1)})^{T} \vx^{(\sampleidx)})\vx^{(\sampleidx)}. 
\end{equation}
The update \eqref{equ_update_gd_linreg} has an appealing form as it amounts to correcting the previous guess (or approximation) $\vw^{(k\!-\!1)}$ for 
the optimal weight vector $\vw_{\rm opt}$ by the correction term 
\begin{equation}
\label{equ_corr_term_linreg}
 (2\alpha/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \underbrace{(y^{(\sampleidx)} - \big(\vw^{(k\!-\!1)})^{T} \vx^{(\sampleidx)})}_{e^{(\sampleidx)}} \vx^{(\sampleidx)}. 
\end{equation}
The correction term \eqref{equ_corr_term_linreg} is a weighted average of the feature vectors $\vx^{(\sampleidx)}$ using weights $(2\alpha/\samplesize) \cdot e^{(i)}$. 
These weights consist of the global factor $(2\alpha/\samplesize)$ (that applies equally to all feature vectors $\vx^{(\sampleidx)}$) and a sample-specific factor 
$e^{(\sampleidx)} = (y^{(\sampleidx)} - \big(\vw^{(k\!-\!1)})^{T} \vx^{(\sampleidx)})$, which is the prediction (approximation) error obtained by the linear predictor 
$h^{(\vw^{(k-1)})}(\vx^{(\sampleidx)}) =   \big(\vw^{(k\!-\!1)})^{T} \vx^{(\sampleidx)}$ when predicting the label $y^{(\sampleidx)}$ from the features $\vx^{(\sampleidx)}$. 

\begin{center}
\framebox[0.96\textwidth]{
 \parbox{0.9\textwidth}{
We can interpret the GD step \eqref{equ_update_gd_linreg} as an instance of ``learning by trial and error''. Indeed, 
the GD step amounts to ``trying out'' the predictor $h(\vx^{(\sampleidx)}) = \big(\vw^{(k\!-\!1)})^{T}\vx^{(\sampleidx)}$ and then correcting the weight vector $\vw^{(k\!-\!1)}$ 
according to the error $e^{(\sampleidx)} = y^{(\sampleidx)} - \big(\vw^{(k\!-\!1)})^{T} \vx^{(\sampleidx)}$. 
}}
\end{center}

%{\bf Exercise.} Consider the dataset with feature vectors $\vx^{(1)}=(100,0)^{T} \in \mathbb{R}^{2}$ and $\vx^{(2)} = (0,1/10)^{T}$ which we 
%stack into the matrix $\mX = (\vx^{(1)},\vx^{(2)})$. What is the condition number of $\mX \mX^{T}$? What is the condition number of $\widehat{\mathbf{X}}..$ 
%obtained from the normalized feature vectors $\hat{\vx}^{(\sampleidx)}$. 


The choice of the step size $\alpha$ used for Algorithm \ref{alg:gd_linreg} can be based on the sufficient condition \eqref{equ_GD_conv_guarantee} 
with the Hessian $\nabla^{2} f(\vw)$ of the objective function $f(\vw)$ underlying linear regression (see \eqref{equ_smooth_problem_linreg}). This 
Hessian is given explicitly as  
\begin{equation}
\label{equ_hessian_linreg}
\nabla^{2} f(\vw) = (1/\samplesize) \mX^{T} \mX, 
\end{equation}
with the feature matrix $\mX=\big(\vx^{(1)},\ldots,\vx^{(\samplesize)}\big)^{T} \in \mathbb{R}^{\samplesize \times \featuredim}$ (see \eqref{equ_def_vec_matrix}). 
Note that the Hessian \eqref{equ_hessian_linreg} does not depend on the weight vector $\vw$. 


Comparing \eqref{equ_hessian_linreg} with \eqref{equ_GD_conv_guarantee}, one particular strategy for choosing 
the step size in Algorithm \ref{alg:gd_linreg} is to (i) compute the matrix product $ \mX^{T} \mX$, (ii) compute the 
maximum eigenvalue $\lambda_{\rm max} \big(  (1/\samplesize)  \mX^{T} \mX \big)$ of this product and (iii) set 
the step size to $\alpha =1/\lambda_{\rm max} \big(  (1/\samplesize)  \mX^{T} \mX \big)$. 

While it might be challenging to compute the maximum eigenvalue $\lambda_{\rm max} \big(  (1/\samplesize)  \mX^{T} \mX \big)$, 
it might be easier to find an upper bound $U$ for it.\footnote{The problem of 
computing a full eigenvalue decomposition of $\mX^{T} \mX$ has essentially the same complexity as solving the 
ERM problem directly via \eqref{equ_zero_gradient_lin_reg}, which we want to avoid by using the ``cheaper'' GD algorithm.}
Given such an upper bound $U \geq \lambda_{\rm max} \big( (1/\samplesize)  \mX^{T} \mX \big)$, the step 
size $\alpha =1/U$ still ensures convergence of the GD iteration. Consider a dataset $\{(\vx^{(\sampleidx)},y^{(\sampleidx)})\}_{\sampleidx=1}^{\samplesize}$ 
with normalized features, i.e., $\| \vx^{(\sampleidx)}\| = 1$ for all $\sampleidx =1,\ldots,\samplesize$. Then, 
by elementary linear algebra, one can verify the upper bound $U= 1$, i.e., $ 1 \geq  \lambda_{\rm max} \big(  (1/\samplesize)  \mX^{T} \mX \big)$. 
We can then ensure convergence of the GD iterations $\vw^{(k)}$ (see \eqref{equ_update_gd_linreg}) by 
choosing the step size $\alpha =1$.

%\subsection{GD for Polynomial Regression} 
%\label{sec_poly_reg}
%??? Discuss how GD for Polynomial Regression does implicit regularisation (cf ``Early Stopping''). This has been discussed in {\rm https://www.youtube.com/watch?v=MG35HdwgFoY}. ????

\section{GD for Logistic Regression}
\label{sec_GD_logistic_regression}

%We can now formulate a full-fledged ML algorithm for solving a linear regression problem (see Section \ref{sec_lin_regression}). 
As discussed in Section \ref{sec_LogReg}, the classification method logistic regression amounts to 
constructing a classifier $h^{(\vw_{\rm opt})}$ by minimizing the empirical risk \eqref{equ_def_emp_risk_logreg} 
obtained for a labeled dataset $\dataset= \{ (\vx^{(\sampleidx)}, y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize}$, 
with features $\vx^{(\sampleidx)} \in \mathbb{R}^{\featuredim}$ and binary labels $y^{(\sampleidx)} \in \{-1,1\}$. 
Thus, logistic regression amounts to an instance of the smooth optimization problem \eqref{equ_generic_smooth_problem}, i.e., 
\begin{align} 
\label{equ_smooth_problem_logeg}
\vw_{\rm opt} & = \argmin_{\vw \in \mathbb{R}^{\featuredim}} f(\vw) \nonumber \\ 
 \mbox{ with } f(\vw) & = (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \log( 1\!+\!\exp ( - y^{(\sampleidx)} \vw^{T} \vx^{(\sampleidx)})). 
\end{align} 

In order to apply GD \eqref{equ_def_GD_step} to solve \eqref{equ_smooth_problem_logeg}, we need to compute the gradient $\nabla f(\vw)$. 
The gradient of the objective function in \eqref{equ_smooth_problem_logeg} is given by 
\begin{equation}
\label{equ_gradient_logistic_regression}
\nabla f(\vw) = (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \frac{-y^{(\sampleidx)}}{1 + \exp ( y^{(\sampleidx)} \vw^{T} \vx^{(\sampleidx)})} \vx^{(\sampleidx)}.
\end{equation} 
By inserting \eqref{equ_gradient_logistic_regression} into the basic GD iteration \eqref{equ_def_GD_step}, we obtain Algorithm \ref{alg:gd_logreg}. 
\begin{algorithm}[htbp]
\caption{``Logistic Regression via GD''}\label{alg:gd_logreg}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\Require   labeled dataset $\dataset=\{ (\vx^{(\sampleidx)}, y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize}$ containing feature vectors 
$\vx^{(\sampleidx)} \in \mathbb{R}^{\featuredim}$ and labels $y^{(\sampleidx)} \in \mathbb{R}$; GD step size $\alpha >0$. 
\Statex\hspace{-6mm}{\bf Initialize:}set $\vw^{(0)}\!\defeq\!\mathbf{0}$; set iteration counter $k\!\defeq\!0$   
\Repeat 
\State $k \defeq k +1$    (increase iteration counter) 
\State  $\vw^{(k)} \defeq \vw^{(k\!-\!1)}  \alpha (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \frac{y^{(\sampleidx)}}{1 + \exp \big( y^{(\sampleidx)} \big(\vw^{(k\!-\!1)}\big)^{T} \vx^{(\sampleidx)}\big)} \vx^{(\sampleidx)}$  (do a GD step \eqref{equ_def_GD_step})
\Until convergence 
\Ensure $\vw^{(k)}$ (which approximates the optimal weight vector $\vw_{\rm opt}$ defined in \eqref{equ_smooth_problem_logeg})
\end{algorithmic}
\end{algorithm}

Let us have a closer look on the update in step $3$ of Algorithm \ref{alg:gd_logreg}, which is 
\begin{equation}
\label{equ_update_logreg_GD}
\vw^{(k)} \defeq \vw^{(k\!-\!1)} + \alpha (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \frac{y^{(\sampleidx)}}{1 + \exp \big( y^{(\sampleidx)} \big( \vw^{(k\!-\!1)}  \big)^{T} \vx^{(\sampleidx)}\big)} \vx^{(\sampleidx)}. 
\end{equation}
The update \eqref{equ_update_logreg_GD} has an appealing form as it amounts to correcting the previous 
guess (or approximation) $\vw^{(k\!-\!1)}$ for the optimal weight vector $\vw_{\rm opt}$ by the correction term 
\begin{equation}
\label{equ_correction_logreg_GD}
(\alpha/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \underbrace{ \frac{y^{(\sampleidx)}}{1 + \exp ( y^{(\sampleidx)} \vw^{T} \vx^{(\sampleidx)})}}_{e^{(\sampleidx)}} \vx^{(\sampleidx)}. 
\end{equation}
The correction term \eqref{equ_correction_logreg_GD} is a weighted average of the feature vectors $\vx^{(\sampleidx)}$, 
each of those vectors is weighted by the factor $(\alpha/\samplesize) \cdot e^{(i)}$. These weighting factors consist of 
the global factor $(\alpha/\samplesize)$ (that applies equally to all feature vectors $\vx^{(\sampleidx)}$) and a sample-specific 
factor $e^{(\sampleidx)} =  \frac{y^{(\sampleidx)}}{1 + \exp ( y^{(\sampleidx)} \vw^{T} \vx^{(\sampleidx)})}$, which quantifies 
the error of the classifier  $h^{(\vw^{(k-1)})}(\vx^{(\sampleidx)}) =   \big(\vw^{(k\!-\!1)})^{T} \vx^{(\sampleidx)}$ for a data point 
having true label $y^{(\sampleidx)} \in \{-1,1\}$ and the features $\vx^{(\sampleidx)} \in \mathbb{R}^{\featuredim}$. 

We can use the sufficient condition \eqref{equ_GD_conv_guarantee} (which guarantees convergence of GD) 
to guide the choice of the step size $\alpha$ in Algorithm \ref{alg:gd_logreg}. In order to apply condition 
\eqref{equ_GD_conv_guarantee}, we need to determine the Hessian $\nabla^{2} f(\vw)$ matrix of the 
objective function $f(\vw)$ underlying logistic regression (see \eqref{equ_smooth_problem_logeg}). 
Some basic calculus reveals (see \cite[Ch. 4.4.]{hastie01statisticallearning})
\begin{equation}
\label{equ_hessian_logreg}
\nabla^{2} f(\vw) =  (1/\samplesize) \mX^{T} \mD \mX. 
\end{equation}
Here, we used the feature matrix $\mX=\big(\vx^{(1)},\ldots,\vx^{(\samplesize)}\big)^{T} \in \mathbb{R}^{\samplesize \times \featuredim}$ (see \eqref{equ_def_vec_matrix}) 
and the diagonal matrix $\mD = {\rm diag} \{d_{1},\ldots,d_{\samplesize}\} \in \mathbb{R}^{\samplesize \times \samplesize}$ with diagonal elements 
\begin{equation}
d_{\sampleidx} = \frac{1}{1+\exp(-\vw^{T} \vx^{(\sampleidx)})} \bigg(1- \frac{1}{1+\exp(-\vw^{T} \vx^{(\sampleidx)})}  \bigg). \label{equ_diag_entries_log_reg}   % \frac{1}{1+\exp(-\vw^{T} \vx)} %\big( 1- \frac{1}{1+\exp(-\vw^{T} \vx)}  \big). 
\end{equation} 
We highlight that, in contrast to the Hessian \eqref{equ_hessian_linreg} obtained for the objective function 
arising in linear regression, the Hessian \eqref{equ_hessian_logreg} varies with the weight vector $\vw$. 
This makes the analysis of Algorithm \ref{alg:gd_logreg} and the optimal choice of step size somewhat 
more difficult compared to Algorithm \ref{alg:gd_linreg}. However, since the diagonal entries \eqref{equ_diag_entries_log_reg} 
take values in the interval $[0,1]$, for normalized features (with $\| \vx^{(\sampleidx)} \|=1$) the step size $\alpha=1$ 
ensures convergence of the GD updates \eqref{equ_update_logreg_GD} to the optimal weight vector 
$\vw_{\rm opt}$ solving \eqref{equ_smooth_problem_logeg}. 

\section{Data Normalization} 

The convergence speed of the GD steps \eqref{equ_def_GD_step}, i.e., the 
number of steps required to reach the minimum of the objective function 
\eqref{equ_def_cost_MSE} within a prescribed accuracy, depends crucially 
on the condition number $\kappa( \mX^{T} \mX)$. This condition number is 
defined as the ratio 
\begin{equation}
\label{equ_def_condition_number} 
\kappa( \mX^{T} \mX) \defeq \lambda_{\rm max}/\lambda_{\rm min}
\end{equation} 
between the largest and smallest eigenvalue of the matrix $\mX^{T} \mX$. 

The condition number is only well defined if the columns of the feature matrix $\mX$ 
(see \eqref{equ_def_vec_matrix}), which are precisely the feature vectors $\vx^{(\sampleidx)}$, 
are linearly independent. In this case the condition number is lower bounded 
as $\kappa( \mX^{T} \mX) \geq 1$. 

It can be shown that the GD steps \eqref{equ_def_GD_step} converge faster for 
smaller condition number $\kappa( \mX^{T} \mX)$ \cite{JungFixedPoint}. Thus, GD 
will be faster for datasets with a feature matrix $\mX$ such that  $\kappa( \mX^{T} \mX) \approx 1$. 
It is therefore often beneficial to pre-process the feature vectors using a {\bf normalization} 
(or {\bf standardization}) procedure as detailed in Algorithm \ref{alg_reshaping}. 
 
 \begin{algorithm}[htbp]
\caption{``Data Normalization''}\label{alg_reshaping}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\Require   labeled dataset $\dataset = \{(\vx^{(\sampleidx)},y^{(\sampleidx)})\}_{\sampleidx=1}^{\samplesize}$
%\Statex\hspace{-6mm}{\bf Initialize:}set $\vw^{(0)}\!\defeq\!\mathbf{0}$; set iteration counter $k\!\defeq\!0$   
%\Repeat 
\vspace*{2mm}
\State remove sample means $\bar{\vx}=(1/\samplesize) \sum_{\sampleidx=1}^{\samplesize}  \vx^{(\timeidx)}$ from features, i.e., 
 \begin{equation}
 \nonumber
 \vx^{(\sampleidx)} \defeq \vx^{(\sampleidx)} - \bar{\vx} \mbox{ for  }  \sampleidx=1,\ldots,\samplesize
 \end{equation} 
\State normalise features to have unit variance, i.e.,  
 \begin{equation} 
 \nonumber
 \hat{x}^{(\sampleidx)}_{j} \defeq x^{(\sampleidx)}_{j}/ \hat{\sigma}   \mbox{ for  } j=1,\ldots,\featuredim \mbox{ and } \sampleidx=1,\ldots,\samplesize
 \end{equation}
 with the empirical variance $\hat{\sigma}_{j}^{2}  =(1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \big( x^{(\sampleidx)}_{j} \big)^{2}$ 
\Ensure normalized feature vectors $\{\hat{\vx}^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize}$
\end{algorithmic}
\end{algorithm}
The preprocessing implemented in Algorithm \ref{alg_reshaping} reshapes (transforms) the 
original feature vectors $\vx^{(\sampleidx)}$ into new feature vectors $\hat{\vx}^{(\sampleidx)}$ 
such that the new feature matrix $\widehat{\mX} = (\hat{\vx}^{(1)},\ldots,\hat{\vx}^{(\samplesize)})^{T}$ 
tends to be well-conditioned, i.e., $\kappa( \widehat{\mX}^{T} \widehat{\mX}) \approx 1$. 

\begin{center}
\framebox[0.96\textwidth]{
 \parbox{0.9\textwidth}{
{\bf Exercise.} Consider the dataset with feature vectors $\vx^{(1)}=(100,0)^{T} \in \mathbb{R}^{2}$ 
and $\vx^{(2)} = (0,1/10)^{T}$ which we stack into the matrix $\mX = (\vx^{(1)},\vx^{(2)})^{T}$. 
What is the condition number of $\mX^{T} \mX$? What is the condition number of 
$\big(\widehat{\mX}\big)^{T} \widehat{\mX}$ with the matrix $\widehat{\mX}=(\hat{\vx}^{(1)},\hat{\vx}^{(2)})^{T}$ 
constructed from the normalized feature vectors $\hat{\vx}^{(\sampleidx)}$ delivered by Algorithm \ref{alg_reshaping}. 
}}
\end{center}
 
 
\section{Stochastic GD}

Consider an ML problem with a hypothesis space $\hypospace$ which is parametreized by a 
weight vector $\vw \in \mathbb{R}^{\featuredim}$ (such that each element $h^{(\vw)}$ of $\hypospace$ 
corresponds to a particular choice of $\vw$) and a loss function $\loss{(\vx,y)}{h^{(\vw)}}$ which 
depends smoothly on the weight vector $\vw$. The resulting ERM \eqref{equ_def_ERM_continous} 
amounts to a smooth optimization problem which can be solved using GD \eqref{equ_def_GD_step}. 

Note that the gradient $\nabla f(\vw)$ obtained for the optimization problem \eqref{equ_def_ERM_continous} has a particular structure. 
Indeed, the gradient is a sum 
\vspace*{-2mm}
\begin{equation}
\label{eq_gradient_sum}
\nabla f(\vw) = (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \nabla f_{\sampleidx}(\vw)   \mbox{ with } f_{\sampleidx}(\vw) \defeq \loss{(\vx^{(\sampleidx)},y^{(\sampleidx)})}{h^{(\vw)}}. 
\end{equation} 
Evaluating the gradient $\nabla f(\vw)$ (e.g., within a GD step \eqref{equ_def_GD_step}) by 
computing the sum in \eqref{eq_gradient_sum} can be computationally challenging for at least 
two reasons. First, computing the sum exactly is challenging for extremely large datasets with 
$\samplesize$ in the order of billions. Second, for datasets which are stored in different data 
centres located all over the world, the summation would require huge amount of network resources 
and also put limits on the rate by which the GD steps \eqref{equ_def_GD_step} can be executed. 
\begin{center}
\framebox[0.96\textwidth]{
 \parbox{0.9\textwidth}{
{\bf ImageNet.} The ``ImageNet'' database contains more than $10^{6}$ images \cite{ImageNet}. These 
images are labeled according to their content (e.g., does the image show a dog?). Let us assume that 
each image is represented by a (rather small) feature vector $\vx \in \mathbb{R}^{\featuredim}$ 
of length $\featuredim=1000$. Then, if we represent each feature by a  floating point number, 
performing only one single GD update \eqref{equ_def_GD_step} per second would require at 
least $10^{9}$ FLOPS. 
}}
\end{center}


The idea of {\bf stochastic GD (SGD)} is quite simple: Replace the exact gradient $\nabla f(\vw)$ 
by some approximation which can be computed easier than \eqref{eq_gradient_sum}. The word ``stochastic'' 
in the name SGD hints already at the use of randomness (stochastic approximations). 

One basic variant of SGD approximates the gradient $\nabla f(\vw)$ (see \eqref{eq_gradient_sum}) 
a randomly selected component $\nabla f_{\hat{\sampleidx}}(\vw)$ in \eqref{eq_gradient_sum}, 
with the index $\hat{\sampleidx}$ being chosen randomly out of $\{1,\ldots,\samplesize\}$. SGD 
amounts to iterating the update 
\begin{equation}
\label{equ_SGD_update}
\mathbf{w}^{(\itercntr+1)} = \mathbf{w}^{(\itercntr)} - \alpha \nabla f_{\hat{\sampleidx}}(\vw^{(\itercntr)}).
\end{equation} 
It is important to use a fresh randomly chosen index $\hat{i}$ during each new iteration. 
The indices used in different iterations are statistically independent. 

Note that SGD replaces the summation over all training data points in the GD step 
\eqref{equ_def_GD_step} just by the random selection of a single component of the sum. 
The resulting savings in computational complexity can be significant in applications 
where a large number of  data points is stored in a distributed fashion. However, this 
saving in computational complexity comes at the cost of introducing a non-zero 
gradient noise 
\begin{equation}
\label{equ_gradient_noise}
\varepsilon = \nabla f(\vw)  - \nabla f_{\hat{\sampleidx}}(\vw), 
\end{equation} 
into the SGD updates. In order avoid the accumulation of the gradient noise \eqref{equ_gradient_noise} 
while running SGD updates \eqref{equ_SGD_update} the step size $\alpha$ needs to be gradually 
decreased, e.g., using $\alpha = 1/\itercntr$ with $\itercntr$ being the iteration counter (see \cite{Murata98astatistical}). 

The SGD iteration \eqref{equ_SGD_update} assumes that the training data is already 
collected but so large that the sum in \eqref{eq_gradient_sum} is computationally intractable. 
Another variant of SGD is obtained by assuming a different data generation 
mechanism. If the data points are collected sequentially, one new data point $\vx^{(\timeidx)},y^{(\timeidx)}$ 
at each new time step $\timeidx$, we could a SGD variant for online learning 
(see Section \ref{sec_online_learning}). This online SGD algorithm amounts to 
computing, for each time step $\timeidx$, the iteration 
\begin{equation}
\label{equ_SGD_update_timeiedx}
\mathbf{w}^{(\timeidx+1)} = \mathbf{w}^{(\timeidx)} - \alpha_{\timeidx} \nabla f_{\timeidx+1}(\vw^{(\timeidx)}).
\end{equation} 



\section{Exercises}

\subsection{Use Knowledge About Problem Class} 
Consider the space $\mathcal{P}$ of sequences $f = (f[0],f[1],\ldots)$ that have the following properties 
\begin{itemize} 
	\item they are monotone increasing, $f[n] \geq f[m]$ for any $n \geq m$ and $f \in \mathcal{P}$
	\item a change point $n$, where $f[n] \neq f[n+1]$ can only be at integer multiples of $100$, 
	e.g., $n=100$ or $n=300$. 
\end{itemize} 
Given some unknown function $f \in \mathcal{P}$ and starting point $n_{0}$ the problem is to find 
the minimum value of $f$ as quickly as possible. We consider iterative algorithms that can query
the function at some point $n$ to obtain the values $f[n], f[n-1]$ and $f[n+1]$. 


\newpage
\chapter{Model Validation and Selection} 
\label{ch_validation_selection}
% \begin{quote} 
 %``There are known knowns ... But there are also unknown unknowns'' (D. Rumsfeld)
% \end{quote}
The idea of ERM is to learn a good predictor by searching the hypothesis space $\hypospace$ 
for a predictor map $h$ with minimum average loss (empirical error) on a training set. For ML 
methods using high-dimensional hypothesis spaces, such as linear maps with a large number of 
features or deep neural networks, this approach bears the risk of overfitting. A 
method overfits if it learns a predictor $h \in \hypospace$ that, merely by luck, fits 
well the training data but does a poor job on other data. Such a predictor will fail 
to {\bf generalize} well to new data for which we do not know the label $y$ but 
only the features $\vx$ (if we would know the label, then there is no point in learning 
predictors which estimate the label). 

This chapter discusses few basic techniques to detect and avoid overfitting. 
%Consider some ML algorithm (such as Algorithm \ref{alg:gd_linreg}), which learns a predictor $\hat{h}$ 
%via ERM \eqref{equ_def_ERM_funs} based on some labeled dataset $\dataset = \{(\vx^{(\sampleidx)},y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize}$.
To detect overfitting we need to monitor or {\bf to validate} the performance of 
the predictor $h$ on new data point which are not contained in the training set. 
We call the set of data points used for validation, as the validation set. The empirical 
risk incurred by the predictor $h$ on the validation set is referred to as validation error. 
If a method overfits, it will learn a predictor whose training error is much smaller than 
the validation error. 
%Indeed, the ultimate goal of ML methods is to find a predictor $\hat{h}$ which is accurate 
%when applied to a new unlabeled data point for which we do not know the label $y$ but 
%only the features $\vx$ (if we would know the label, then there is no point in learning 
%predictors which estimate the label). 
%
%
%A good indicator for the accuracy of a predictor is 
%the empirical risk (training error) $\emperror(\hat{h}|\dataset)$ incurred over the labeled 
%training dataset $\dataset$. However, there are situations where the training error $\emperror(\hat{h}|\dataset)$ 
%is quite small but the prediction error $y-\hat{h}(\vx)$ obtained for new data $(\vx,y)$ 
%is unacceptably large. This situation is referred to as {\bf overfitting} and the reasons 
%for this to happen will be discussed in Section \ref{sec_overfitting_sec_6}. 

Validation is useful not only for verifying if the predictor generalises well to 
new data (in particular detecting overfitting) but also for guiding model selection. 
In what follows, we mean by model selection the problem of selecting a 
particular hypothesis space out of a whole ensemble of potential hypothesis spaces $\hypospace_{1},\hypospace_{2},\ldots$.

We first study the phenomenon of overfitting within a simple probabilistic model for 
the data points in Section \ref{sec_overfitting_sec_6}. Then, in Section \ref{sec_validate_predictor}, 
we analyze a simple validation technique that allows to detect overfitting. 


\section{Overfitting}
\label{sec_overfitting_sec_6}

Let us illustrate the phenomenon of overfitting using a simplified model 
for how a human child learns the concept ``tractor''. In particular, this 
learning task amounts to finding an association (or predictor) between 
an image and the fact if the image shows a tractor or not. To teach this 
association to a child, we show it many pictures and tell for each picture 
if there is a ``tractor'' or if there is ``no tractor'' depicted. 

Consider that we have taught the child using the image collection $\trainset$ 
depicted in Figure \ref{fig_misleading_data_set}. For some reason, one of 
the images is labeled erroneously as ``tractor'' but actually shows an ocean 
wave. As a consequence, if the child is good in memorizing images, it might 
predict the presence of tractors whenever looking at a wave (Figure \ref{equ_wave_as_tractor}). 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{LabeledDataTractorOverfitting1.jpg}  
	\caption{A (misleading) training dataset $\trainset= \{ (\vx^{(\sampleidx)},y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize_{\rm t}}$ 
		consisting of $\samplesize_{\rm t}=9$ images. The $\sampleidx$-th image is characterized by the feature vector $\vx^{(\sampleidx)} \in \mathbb{R}^{\featuredim}$ 
		and labeled with $y^{(\sampleidx)}=1$(if image depicts a tractor) or with $y^{(\sampleidx)}=-1$ (if image does not depict a tractor).}
	%    \caption{Hypothesis Map}\label{fig:Hypothesis Map}
	\label{fig_misleading_data_set}
\end{figure}


\begin{figure}[htbp]
	\centering
	\includegraphics[width=0.7\textwidth]{OverfittingSample.jpg}   
	\caption{The child, who has been taught the concept ``tractor'' using the image collection 
		$\trainset$ in Figure \ref{fig_misleading_data_set}, might ``see'' a lot of tractors during the next beach holiday.}
	\label{equ_wave_as_tractor}   
\end{figure}


For the sake of the argument, we assume that the child uses a linear predictor 
$h^{(\vw)}(\vx) = \vx^{T} \vw$, using the features $\vx$ of the image, and encodes 
the fact of showing a tractor by $y=1$ and if it is not showing a tractor by $y=-1$. 
In order to learn the weight vector, we use ERM with squared error loss over the 
training dataset, i.e., its learning process amounts to solving the ERM problem 
\eqref{equ_def_cost_MSE} using the labeled training dataset $ \dataset^{(\rm train)}$. 

If we stack the feature vectors $\vx^{(\sampleidx)}$ and labels $y^{(\sampleidx)}$ into 
the feature matrix $\mX=(\vx^{(1)},\ldots,\vx^{(\samplesize_{\rm t})})^{T}$ and label 
vector $\vy=(y^{(1)},\ldots,y^{(\samplesize_{\rm t})})^{T}$, the optimal linear predictor 
is obtained for the weight vector solving \eqref{equ_zero_gradient_lin_reg} 
and the associated training error is given by \eqref{equ_emp_risk_lin_proje}, which 
we repeat here for convenience: 
\begin{equation}
\emperror(h^{(\vw_{\rm opt})} \mid \trainset) = \min_{\vw \in \mathbb{R}^{\featuredim}} \emperror(h^{(\vw)} \mid \trainset) = \|  (\mathbf{I}- \mathbf{P}) \vy \|^{2}.
\end{equation} 
Here, we used the orthogonal projection matrix $\mathbf{P}$ on the linear span 
\begin{equation} 
\nonumber
{\rm span}\{ \mX \} = \big\{  \mX \va : \va \in \mathbb{R}^{\featuredim} \big\} \subseteq \mathbb{R}^{\samplesize_{\rm t}} , 
\end{equation}
of the feature matrix 
\begin{equation} 
\label{equ_feature_matrix_overfit}
\mX = (\vx^{(1)},\ldots,\vx^{(\samplesize_{\rm t})})^{T} \in \mathbb{R}^{  \samplesize_{\rm t} \times \featuredim}. 
\end{equation} 

Let us now study how overfitting arises whenever the sample size $\samplesize$ is 
smaller or equal to the length $\featuredim$ of the feature vectors $\vx$, i.e., whenever 
\begin{equation} 
\label{equ_condition_overfitting}
\samplesize_{\rm t} \leq \featuredim. 
\end{equation} 
In practice, a set of $\samplesize_{\rm t}$ feature vectors $\vx^{(\sampleidx)} \in \mathbb{R}^{\featuredim}$ is 
typically linearly independent whenever \eqref{equ_condition_overfitting} is satisfied. In this case, the span of 
the transposed feature matrix \eqref{equ_feature_matrix_overfit} coincides with $\mathbb{R}^{\samplesize}$ 
which implies, in turn, $\mathbf{P} = \mathbf{I}$. Inserting $\mathbf{P} = \mathbf{I}$ into \eqref{equ_emp_risk_lin_proje} 
yields 
\begin{equation}
\label{eq_zero_trianing_error}
\emperror(h^{(\vw_{\rm opt})} \mid \dataset^{(\rm train)}) = 0.
\end{equation} 
To sum up: as soon as the number of training examples $\samplesize_{\rm t} = | \dataset_{\rm train}|$ is smaller 
than the size $\featuredim$ of the feature vector $\vx$, there is a linear predictor $h^{(\vw_{\rm opt})}$ achieving 
{\bf zero empirical risk} (see \eqref{eq_zero_trianing_error})on the training data. 

While this ``optimal'' predictor $h^{(\vw_{\rm opt})}$ is perfectly accurate on the training 
data (the training error is zero!), it will typically incur a non-zero average prediction error 
$y - h^{(\vw_{\rm opt})}(\vx)$ on new data points $(\vx,y)$ (which are different from the 
training data). Indeed, using a simple toy model for the data generation, we obtained the 
expression \eqref{equ_decomp_E_pred_toy_model} for the average prediction error. This 
average prediction error is lower bounded by the noise variance $\sigma^{2}$ which might 
be very large even the training error is zero. Thus, in case of overfitting, a small training 
error can be highly misleading regarding the average prediction error of a predictor. 

A simple, yet quite useful, strategy to detect if a predictor $\hat{h}$ overfits the training 
dataset $\dataset^{(\rm train)}$, is to compare the resulting training error 
$\emperror(\hat{h}| \dataset^{(\rm train)})$ (see \eqref{equ_def_training_error_val}) with the 
validation error $\emperror(\hat{h}| \dataset^{(\rm val)})$ (see \eqref{equ_def_training_val_val}). 
The validation error $\emperror(\hat{h}| \dataset^{(\rm val)})$ is the empirical risk of the predictor 
$\hat{h}$ on the validation dataset $\dataset^{(\rm val)}$. If overfitting occurs, the validation 
error $\emperror(\hat{h}| \dataset^{(\rm val)})$ is significantly larger than the training error 
$\emperror(\hat{h}| \dataset^{(\rm train)})$. The occurrence of overfitting for polynomial regression with 
degree $\featuredim$ (see Section \ref{sec_polynomial_regression}) chosen too large is depicted in 
Figure \ref{fig_polyn_training}. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{OverfittedPolyn.png}  
	\caption{The training dataset consists of the blue crosses and can be almost perfectly 
		fit by a high-degree polynomial. This high-degree polynomial gives only poor results 
		for a different (validation) dataset indicated by the orange dots.}
	\label{fig_polyn_training}
\end{figure}


\section{Validation}
\label{sec_validate_predictor}

Consider an ML problem using a hypothesis space $\hypospace$. We have chosen a particular predictor 
$\hat{h} \in \hypospace$ by ERM \eqref{equ_def_ERM_funs} using a labeled dataset (the training set). The 
basic idea of validating the predictor $\hat{h}$ is simple: compute the empirical risk of $\hat{h}$ over another 
set of data points $(\vx,y)$ which have not been already used for training. It is very important to validate the 
predictor $\hat{h}$ using labeled data points which do not belong to the dataset which has been used to 
learn $\hat{h}$ (e.g., via ERM \eqref{equ_def_ERM_funs}). This is reasonable since the predictor $\hat{h}$ 
tends to ``look better'' on the training set than for other data points, since it is optimized precisely for the data 
points in the training set. 
\vspace*{2mm}
\begin{center}
\framebox[0.96\textwidth]{
 \parbox{0.9\textwidth}{
\begin{quote}
A golden rule of ML practice: try always to use different data points for the training (see \eqref{equ_def_ERM_funs}) 
and the validation of a predictor $\hat{h}$!
\end{quote}
}}
\end{center}
 
A very simple recipe for implementing learning and validation of a predictor based on one single labeled dataset 
$\dataset = \{(\vx^{(\sampleidx)},y^{(\sampleidx)})\}_{\sampleidx=1}^{\samplesize}$ is as follows (see Figure \ref{fig_split_train_val}): 
\begin{enumerate} 
\item randomly divide (``split'') the entire dataset $\dataset$ of labeled snapshots into two disjoint subsets 
$\trainset$ (the ``training set'') and $\valset$ (the ``validation set''): 
$\dataset = \trainset \cup \valset$ (see Figure \ref{fig_split_train_val}). 
\item learn a predictor $\hat{h}$ via ERM using the training data $\trainset$, i.e., compute (cf.\ \eqref{equ_def_ERM_funs}) 
\begin{align} 
\label{equ_def_hat_h_fitting}
\hat{h} & = \argmin_{h \in \hypospace} \emperror(h| \trainset) \nonumber \\
& = \argmin_{h \in \hypospace}  (1/\samplesize_{\rm t}) \sum_{(\vx,y) \in \trainset} \loss{(\vx,y)}{h} %( y- h(\vx))^2
\end{align} 
with corresponding {\bf training error} 
\begin{equation} 
\label{equ_def_training_error_val}
\emperror(\hat{h}|\trainset)=  (1/\samplesize_{\rm t}) \sum_{\sampleidx=1}^{\samplesize_{\rm t}} \loss{(\vx^{(\sampleidx)},y^{(\sampleidx)})} {\hat{h}}  %( y- \hat{h}(\vx))^2.
\end{equation}  
\item validate the predictor $\hat{h}$ obtained from \eqref{equ_def_hat_h_fitting} by computing the empirical risk 
\begin{equation} 
\label{equ_def_training_val_val}
\emperror(\hat{h}|\valset) = (1/\samplesize_{\rm v})  \sum_{(\vx,y) \in \valset} \loss{(\vx,y)} {\hat{h}}  % ( y- \hat{h}(\vx))^2, 
\end{equation}  
obtained when applying the predictor $\hat{h}$ to the {\bf validation dataset} $\dataset^{(\rm val)}$. 
We might refer to $\emperror(\hat{h}| \dataset^{(\rm val)})$ as the {\bf validation error}. 
\end{enumerate} 
The choice of the split ratio $|\dataset^{(\rm val)}| / |\dataset^{(\rm train)}|$, i.e., how 
large should the training set be relative to the validation set is often based on experimental 
tuning. It seems difficult to make a precise statement on how to choose the split ratio 
which applies broadly to different ML problems \cite{Larsen1999}.
 
 \begin{figure}[htbp]
    \centering
   \includegraphics[width=0.7\textwidth]{TrainValSmall.jpg}  
    \caption{If we have only one single labeled dataset $\dataset$, we split it into a {\bf training set} $\dataset^{(\rm train)}$ 
    and a {\bf validation set} $\dataset^{(\rm val)}$. We use the training set in order to learn (find) a good predictor $\hat{h}(\vx)$ 
    by minimizing the empirical risk $\emperror(h| \dataset^{(\rm train)})$ (see \eqref{equ_def_ERM_funs}). 
    In order to validate the performance of the predictor $\hat{h}$ on new data, we compute the empirical risk 
    $\emperror(h| \dataset^{(\rm val)})$ incurred by $\hat{h}(\vx)$ for the validation set $\dataset^{(\rm val)}$. 
    We refer to the empirical risk $\emperror(h| \dataset^{(\rm val)})$ obtained for the validation set as the {\bf validation error}.}
    \label{fig_split_train_val}
\end{figure}

The basic idea of randomly splitting the available labeled data into training and 
validation sets is underlying many validation techniques. A popular extension 
of the above approach, which is known as $k$-fold cross-validation, is based 
on repeating the splitting into training and validation sets $k$ times. During 
each repetition, this method uses different subsets for training and validation. 
We refer to \cite[Sec. 7.10]{hastie01statisticallearning} for a detailed discussion of $k$-fold 
cross-validation. 

\section{Model Selection}
\label{sec_modsel}

We will now discuss how to use the validation principle of Section \ref{sec_validate_predictor} to perform model selection. 
As discussed in Chapter \ref{ch_Elements_ML}, the choice of the hypothesis space from which 
we select a predictor map (e.g., via solving the ERM \eqref{equ_def_ERM_funs}) is a design choice. 
However, it is often not obvious what a good first choice for the hypothesis space is. Rather, we often try out 
different choices $\hypospace_{1},\hypospace_{2},\ldots,\hypospace_{M}$ for the hypothesis space. 

Consider a prediction problem where the relation between feature $x$ and $y$ is non-linear (see, e.g., Figure \ref{fig_scatterplot}). 
We might then use polynomial regression (see Section \ref{sec_polynomial_regression}) using the hypothesis 
space $\hypospace_{\rm poly}^{(\featuredim)}$ with some maximum degree $\featuredim$. For each value of the 
maximum degree $\featuredim$ we get a different hypothesis space: $\hypospace_{1} = \mathcal{H}_{\rm poly}^{(0)},\hypospace_{2} = \mathcal{H}_{\rm poly}^{(1)},\ldots,\hypospace_{M} = \hypospace_{\rm poly}^{(M-1)}$. 
Or, instead of polynomial regression, we might use Gaussian basis regression (see Section \ref{sec_linbasreg}), 
with different choices for the variance $\sigma$ and shifts $\mu$ of the Gaussian basis function \eqref{equ_basis_Gaussian}, 
e.g., $\hypospace_{1} = \mathcal{H}^{(2)}_{\rm Gauss}$ with $\sigma=1$ and $\mu_{1}=1$ and $\mu_{2}=2$, 
$\hypospace_{2} = \hypospace^{(2)}_{\rm Gauss}$ with $\sigma = 1/10$, $\mu_{1}=10$, $\mu_{2}= 20$.

A principled approach for choosing a hypothesis space out of a given a set of candidates $\hypospace_{1},\hypospace_{2},\ldots,\hypospace_{M}$ is as follows: 
\begin{itemize} 
\item randomly divide (split) the entire dataset $\dataset$ of labeled snapshots into two disjoint subsets 
$\trainset$ (the ``training set'') and $\valset$ (the ''validation set''): 
$\dataset = \trainset \cup \valset$ (see Figure \ref{fig_split_train_val}). 
\item for each hypothesis space $\hypospace_{l}$ learn predictor $\hat{h}_{l} \in \hypospace_{l}$ via ERM \eqref{equ_def_ERM_funs} using training data $\trainset$:
\begin{align} 
\label{equ_def_hat_h_fitting_sel}
\hat{h}_{l} & = \argmin_{h \in \hypospace_{l}} \emperror(h| \trainset) \nonumber \\
& = \argmin_{h \in \hypospace_{l}}  (1/\samplesize_{\rm t}) \sum_{\sampleidx=1}^{\samplesize_{\rm t}} \loss{(\vx^{(\sampleidx)},y^{(\sampleidx)})}{h} %( y- h(\vx))^2
\end{align} 
\item compute the validation error of $\hat{h}_{l}$ 
\begin{equation} 
\label{equ_def_training_val_val_modsel}
\emperror(\hat{h}_{l}|\valset) = (1/\samplesize_{\rm v}) \sum_{\sampleidx=1}^{\samplesize_{\rm v}} \loss{(\vx^{(\sampleidx)},y^{(\sampleidx)})} {\hat{h}_{l}}  % ( y- \hat{h}(\vx))^2, 
\end{equation}  
obtained when applying the predictor $\hat{h}_{l}$ to the {\bf validation dataset} $\valset$. 
\item pick the hypothesis space $\hypospace_{l}$ resulting in the smallest validation error $\emperror(\hat{h}_{l}|\valset)$
\end{itemize} 



\section{Bias, Variance and Generalization within Linear Regression} 
\label{sec_gen_linreg}
\emph{More Data Beats Clever Algorithms ?; More Data Beats Clever Feature Selection?}

A core problem or challenge within ML is the verification (or validation) if a 
predictor or classifier which works well on a labeled training dataset will also 
work well (generalize) to new data points. In practice we can only validate 
by using different data points than for training an ML method via ERM. However, 
if we can find some generative probabilistic model which well explains the 
observed data points $\vz^{(\sampleidx)}$ we can study the generalization 
ability via probability theory. 

To study generalization within a linear regression problem 
(see Section \ref{sec_lin_regression}), we will invoke a {\bf probabilistic toy model} 
for the data arising in an ML application. We assume that 
any observed data point $\vz=(\vx,y)$ with features $\vx \in \mathbb{R}^{\featuredim}$ 
and label $y \in \mathbb{R}$ is an i.i.d.\ realization of a 
Gaussian random vector. 

The feature vector $\vx$ is assumed to have zero mean and covariance 
being the identity matrix, i.e., $\vx \sim \mathcal{N}(\mathbf{0}, \mathbf{I})$. 
The label $y$ of a data point is related to its features $\vx$ via a 
{\bf linear Gaussian model} 
\begin{equation} 
\label{equ_linear_obs_model}
y = \vw_{\rm true}^{T}  \vx + \varepsilon \mbox{, with noise } \varepsilon \sim \mathcal{N}(0,\sigma^{2}).
\end{equation} 
The noise variance $\sigma^{2}$ is assumed fixed (non-random) and known. 
Note that the error component $\varepsilon$ in \eqref{equ_linear_obs_model} 
is intrinsic to the data (within our toy model) and cannot be overcome by 
any ML method. We highlight that this model for the observed data 
points might not be accurate for a particular ML application. However, 
this toy model will allow us to study some fundamental behaviour of ML methods. 

In order to predict the label $y$ from the features $\vx$ we will use 
predictors $h$ that are linear maps of the first $r$ features $x_{1},\ldots,x_{r}$. 
This results in the hypothesis space 
\vspace*{-2mm}
\begin{equation}
\label{equ_generalization_hypospace_r}
\hypospace^{(r)} = \{ h^{(\vw)}(\vx)= (\vw^{T},\mathbf{0}) \vx \mbox{ with } \mathbf{w} \in \mathbb{R}^{r} \}.   
\vspace*{-1mm}
\end{equation}
The design parameter $r$ determines the size of the hypothesis space 
$\hypospace^{(r)}$ and allows to control the computational complexity 
of the resulting ML method which is based on the hypothesis space 
$\hypospace^{(r)}$. For $r < \featuredim$, the hypothesis space $\hypospace^{(r)}$ 
is a proper subset of the space of linear predictors \eqref{equ_def_hypo_linear_pred} 
used within linear regression (see Section \ref{sec_lin_regression}). Note that 
each element $h^{(\vw)} \in \hypospace^{(r)}$ corresponds to a particular 
choice of the weight vector $\vw \in \mathbb{R}^{r}$. 

The quality of a particular predictor $h^{(\vw)} \in \hypospace^{(r)}$ is measured 
via the mean squared error $\emperror (h^{(\vw)} \mid \trainset)$ incurred over a 
labeled training set $\trainset= \{ \vx^{(\sampleidx)}, y^{(\sampleidx)} \}_{\sampleidx=1}^{\samplesize_{\rm t}}$. 
Within our toy model (see \eqref{equ_linear_obs_model}, \eqref{equ_toy_model_iid} 
and \eqref{equ_labels_training_data}), the training data points $(\vx^{(\sampleidx)},y^{(\sampleidx)})$ 
are i.i.d.\ copies of the data point $\vz = (\vx,y)$. 

Each of the data points in the training dataset is statistically independent from 
any other data point $(\vx,y)$ (which has not been used for training). However, 
the training data points $(\vx^{(\sampleidx)},y^{(\sampleidx)})$ and any other (new) 
data point $(\vx,y)$ share the same probability distribution (a multivariate normal distribution): 
\begin{equation} 
\label{equ_toy_model_iid}
\vx, \vx^{(\sampleidx)} \mbox{ i.i.d. with } \vx, \vx^{(\sampleidx)} \sim \mathcal{N}(\mathbf{0}, \mathbf{I}) 
\end{equation} 
and the labels $y^{(\sampleidx)},y$ are obtained as 
\begin{equation} 
\label{equ_labels_training_data}
y^{(\sampleidx)} = \vw_{\rm true}^{T}  \vx^{(\sampleidx)} + \varepsilon^{(\sampleidx)}\mbox{, and } y = \vw_{\rm true}^{T}  \vx + \varepsilon
\end{equation}  
with i.i.d.\ noise $\varepsilon, \varepsilon^{(\sampleidx)} \sim \mathcal{N}(0,\sigma^{2})$. 

As discussed in Chapter \ref{ch_Optimization}, the training error $\emperror (h^{(\vw)} \mid \trainset)$ is minimized by the predictor 
$h^{(\hat{\vw})}(\vx) =  \hat{\vw}^{T} \mathbf{I}_{r \times \featuredim} \vx$, 
with weight vector 
\begin{equation}
\label{equ_optimal_weight_closed_form}
\widehat{\vw} =  (\mX^{T}_{r} \mX_{r})^{-1} \mX_{r}^{T} \vy
\end{equation} 
with feature matrix $\mX_{r}$ and label vector $\vy$ defined as 
\begin{align}
\label{equ_def_feature_matrix_r}
\mX_{r}& \!=\!(\vx^{(1)},\ldots,\vx^{(\samplesize_{\rm t})})^{T} \mathbf{I}_{\featuredim \times r}\!\in\!\mathbb{R}^{\samplesize_{\rm t} \times r} \mbox{, and }  \nonumber \\
\vy& \!=\!\big(y^{(1)},\ldots,y^{(\samplesize_{\rm t})}\big)^{T}\!\in\!\mathbb{R}^{\samplesize_{\rm t}}.
\end{align} 
It will be convenient to tolerate a slight abuse of notation and denote by $\widehat{\vw}$ both, the length-$r$ vector 
\eqref{equ_optimal_weight_closed_form} as well as the zero padded length-$\featuredim$ vector 
$(\widehat{\vw}^{T},\mathbf{0})^{T}$. This allows us to write % the predictor $h^{(\hat{\vw})}(\vx)$ as 
\begin{equation} 
h^{(\widehat{\vw})}(\vx) = \widehat{\vw}^{T} \vx. 
\end{equation}

We highlight that the formula \eqref{equ_optimal_weight_closed_form} for the optimal weight vector $\widehat{\vw}$ 
is only valid if the matrix $\mX^{T}_{r} \mX_{r}$ is invertible. However, it can be shown that within our toy model 
(see \eqref{equ_toy_model_iid}), this is true with probability one whenever $\samplesize_{\rm t} \geq r$. In what 
follows, we will consider the case of having more training samples than the dimension of the hypothesis space, i.e., 
$\samplesize_{\rm t} > r$ such that the formula \eqref{equ_optimal_weight_closed_form} is valid (with probability one). 
The case $\samplesize_{\rm t} \leq r$ will be studied in Chapter \ref{ch_overfitting_regularization}.

The optimal weight vector $\widehat{\vw}$ (see \eqref{equ_optimal_weight_closed_form}) depends on the training 
data $\trainset$ via the feature matrix $\mX_{r}$ and label vector $\vy$ (see \eqref{equ_def_feature_matrix_r}). 
Therefore, since we model the training data as random, the weight vector $\widehat{\vw}$ \eqref{equ_optimal_weight_closed_form} 
is a random quantity. For each different realization of the training dataset, we obtain a different realization of the 
optimal weight $\widehat{\vw}$. 

Within our toy model, which relates the features $\vx$ of a data point to its label $y$ via \eqref{equ_linear_obs_model}, 
the best case would be if $\widehat{\vw} = \vw_{\rm true}$. However, in general this will not happen since we have to 
compute $\widehat{\vw}$ based on the features $\vx^{(\sampleidx)}$ and noisy labels $y^{(\sampleidx)}$ of the data 
points in the training dataset $\dataset$. Thus, we typically have to face a non-zero {\bf estimation error}
\begin{equation}
\label{equ_def_est_error}
\Delta \vw \defeq \widehat{\vw} - \vw_{\rm true}. 
\end{equation} 
Note that this estimation error is a random quantity since the learnt weight vector $\widehat{\vw}$ 
(see \eqref{equ_optimal_weight_closed_form}) is random.

{\bf Bias and Variance.} 
As we will see below, the prediction quality achieved by $h^{(\widehat{\vw})}$ depends crucially on the 
{\bf mean squared estimation error (MSE)}
\begin{equation}
\label{equ_def_est_err}
\mathcal{E}_{\rm est} \defeq \expect \{  \| \Delta \vw \|^{2}_{2} \} =  \expect \big\{ \big \| \widehat{\vw} - \vw_{\rm true} \big\|^{2}_{2} \big \}. 
\end{equation}
It is useful to characterize the MSE $\mathcal{E}_{\rm est}$ by decomposing it into two components, one component 
(the ``bias'') which depends on the choice $r$ for the hypothesis space and another component (the ``variance'') which 
only depends on the distribution of the observed feature vectors $\vx^{(\sampleidx)}$ and labels $y^{(\sampleidx)}$. 
It is then not too hard to show that 
\vspace*{-1mm}
\begin{align}
\label{equ_bias_var_decomp}
\mathcal{E}_{\rm est} & = \underbrace{ \| \vw_{\rm true} - \expect \{ \widehat{\vw} \} \|^{2}_{2} }_{\mbox{``bias''} B^2}+ \underbrace{\expect \| \widehat{\vw} - \expect \{ \widehat{\vw} \} \|^{2}_{2}}_{\mbox{``variance''} V} 
\end{align} 

The bias term in \eqref{equ_bias_var_decomp}, which can be computed as 
\vspace*{-3mm}
\begin{equation}
\label{equ_def_bias_term}
B^{2} = \| \vw_{\rm true} - \expect \{ \widehat{\vw} \} \|^{2}_{2} = \sum_{l=r+1}^{\featuredim} w_{{\rm true},l}^2, 
\end{equation} 
measures the distance between the ``true predictor'' $h^{(\vw_{\rm true})}(\vx)=\vw_{\rm true}^{T} \vx$ and the 
hypothesis space $\hypospace^{(r)}$ (see \eqref{equ_generalization_hypospace_r}) of the linear regression problem. 
The bias is zero if $w_{{\rm true},l}=0$ for any index $l=r+1,\ldots,\featuredim$, or equivalently if $h^{(\vw_{\rm true})} \in \hypospace^{(r)}$. 
We can guarantee $h^{(\vw_{\rm true})} \in \hypospace^{(r)}$ only if we use the largest possible hypothesis 
space $\hypospace^{(r)}$ with $r=\featuredim$. For $r < \featuredim$, we cannot guarantee a zero bias term since we 
have no access to the true underlying weight vector $\vw_{\rm true}$ in \eqref{equ_linear_obs_model}. In general, the 
bias term decreases for increasing model size $r$ (see Figure \ref{fig_bias_variance}). We also highlight that the bias 
term does not depend on the variance $\sigma^{2}$ of the noise $\varepsilon$ in our toy model \eqref{equ_linear_obs_model}. 

Let us now consider the variance term in \eqref{equ_bias_var_decomp}. Using the properties of our toy model 
(see \eqref{equ_linear_obs_model}, \eqref{equ_toy_model_iid} and \eqref{equ_labels_training_data})
\begin{equation}
\label{equ_variance_term_toy_model}
V = \expect \{ \| \widehat{\vw} - \expect\{ \widehat{\vw} \} \|^{2}_{2} \} = \sigma^{2} \mbox{trace} \big\{ \expect\{(\mX_{r}^{T} \mX_{r})^{-1} \} \big\}.
\end{equation} 
By \eqref{equ_toy_model_iid}, the matrix $(\mX_{r}^{T} \mX_{r})^{-1}$ is random and distributed according to an 
{\bf inverse Wishart distribution} \cite{Mardia1979}. In particular, for $\samplesize_{\rm t} > r +1$, its expectation 
is obtained as 
\begin{equation} 
\label{equ_expr_expect_inv-wishart}
\expect\{(\mX_{r}^{T} \mX_{r})^{-1} \} = 1/(\samplesize_{\rm t}-r-1) \mathbf{I}_{r \times r}.
\end{equation} 
By inserting \eqref{equ_expr_expect_inv-wishart} and $\mbox{trace} \{ \mathbf{I}_{r \times r} \} = r$ into \eqref{equ_variance_term_toy_model}, 
\begin{equation} 
\label{equ_formulae_variance_toy_model}
V= \expect \{ \| \widehat{\vw} - \expect\{ \widehat{\vw} \} \|^{2}_{2} \} = \sigma^{2} r/(\samplesize_{\rm t}-r-1). 
\end{equation} 
As indicated by \eqref{equ_formulae_variance_toy_model}, the variance term increases with increasing model 
complexity $r$ (see Figure \ref{fig_bias_variance}). This behaviour is in stark contrast to the bias term which 
decreases with increasing $r$. The opposite dependency of bias and variance on the model complexity is known 
as the {\bf bias-variance tradeoff}. Thus, the choice of model complexity $r$ (see \eqref{equ_generalization_hypospace_r}) 
has to balance between small variance and small bias term. 

\begin{figure}
\begin{center}
    \begin{tikzpicture}
              \draw [thick, <->] (1,3.5) -- (1,0) -- (10.2,0);
             \draw[red, ultra thick, domain=1:10] plot (\x,  {3*\x^-1});
                 \draw[blue, ultra thick, domain=1:10] plot (\x,  {3*\x/10});
                   \draw[ultra thick, domain=1:10,dashed] plot (\x,  {3*\x^(-1)+3*\x/10});
           \node [right, color=red] at (10.2,0.3) {bias};
                 \node [right,color=blue] at (5.2,1.4) {variance};
               \node [below] at (5,0) {model complexity $r$};
                 \node [above] at (5,2.2) {$\mathcal{E}_{\rm est}$};
     \end{tikzpicture}
\end{center}
\caption{The estimation error $\mathcal{E}_{\rm est}$ incurred by linear regression can be decomposed 
into a bias term $B^{2}$ and a variance term $V$ (see \eqref{equ_bias_var_decomp}). These two 
components depend on the model complexity $r$ in an opposite manner resulting in a bias-variance 
tradeoff.}
\label{fig_bias_variance}
\end{figure}


{\bf Generalization.}
In most ML applications, we are primarily interested in how well a predictor $h^{(\hat{\vw})}$, which has been 
learnt from some training data $\dataset$ (see \eqref{equ_def_ERM_funs}), predicts the label $y$ of a new 
datapoint (which is not contained in the training data $\dataset$) with features $\vx$. Within our linear regression 
model, the prediction (approximation guess or estimate) $\hat{y}$ of the label $y$ is obtained using the learnt 
predictor $h^{(\hat{\vw})}$ via  
\begin{equation}
\label{equ_def_pred_simple_toy_model}
\hat{y} = \widehat{\vw}^{T} \vx. 
\end{equation} 
Note that the prediction $\hat{y}$ is a random variable since (i) the feature vector $\vx$ is modelled as a 
random vector (see \eqref{equ_toy_model_iid}) and (ii) the optimal weight vector $\widehat{\vw}$ (see \eqref{equ_optimal_weight_closed_form}) 
is random. In general, we cannot hope for a perfect prediction but have to face a non-zero prediction error 
\begin{align}
\label{equ_def_pred_error}
e_{\rm pred} & \defeq \hat{y} - y \nonumber \\ 
& \stackrel{\eqref{equ_def_pred_simple_toy_model}}{=}\widehat{\vw}^{T} \vx - y \nonumber \\ 
& \stackrel{\eqref{equ_linear_obs_model}}{=} \widehat{\vw}^{T} \vx - (\vw_{\rm true}^{T}  \vx + \varepsilon) \nonumber \\ 
& = \Delta \vw^{T}  \vx - \varepsilon.
\end{align} 
Note that, within our toy model (see \eqref{equ_linear_obs_model}, \eqref{equ_toy_model_iid} and \eqref{equ_labels_training_data}), 
the prediction error $e_{\rm pred} $ is a random variable since (i) the label $y$ is modelled as a random 
variable (see \eqref{equ_linear_obs_model}) and (ii) the prediction $\hat{y}$ is random. 

Since, within our toy model \eqref{equ_labels_training_data}, $\varepsilon$ is zero-mean and independent of 
$\vx$ and $\widehat{\vw} - \vw_{\rm true}$, we obtain the {\bf average predictor error} as 
\begin{align} 
\label{equ_decomp_E_pred_toy_model}
\mathcal{E}_{\rm pred} & = \expect \{ e_{\rm pred}^{2} \} \nonumber \\
& \stackrel{\eqref{equ_def_pred_error},\eqref{equ_linear_obs_model}}{=} \expect \{ \Delta \vw^{T} \vx \vx^{T} \Delta \vw \} + \sigma^{2}   \nonumber \\
& \stackrel{(a)}{=} \expect \{ \expect \{ \Delta \vw^{T} \vx \vx^{T} \Delta \vw \mid \dataset \} \} + \sigma^{2}  \nonumber \\
& \stackrel{(b)}{=} \expect \{ \Delta \vw^{T} \Delta \vw \}  + \sigma^{2}  \nonumber \\
& \stackrel{\eqref{equ_def_est_error},\eqref{equ_def_est_err}}{=} \mathcal{E}_{\rm est} + \sigma^{2} \nonumber \\
& \stackrel{\eqref{equ_bias_var_decomp}}{=} B^{2} + V + \sigma^{2}. 
\end{align} 
Here, step (a) is due to the law of total expectation \cite{BillingsleyProbMeasure} and step (b) uses that, 
conditioned on the dataset $\dataset$, the feature vector $\vx$ of a new data point (not belonging to 
$\dataset$) has zero mean and covariance matrix $\mathbf{I}$ (see \eqref{equ_toy_model_iid}). 

Thus, as indicated by \eqref{equ_decomp_E_pred_toy_model}, the average (expected) prediction 
error $\mathcal{E}_{\rm pred}$ is the sum of three contributions: (i) the bias $B^{2}$, (ii) the variance 
$V$ and (iii) the noise variance $\sigma^{2}$. The bias and variance, whose sum is the estimation 
error $\mathcal{E}_{\rm est}$, can be influenced by varying the model complexity $r$ (see Figure \ref{fig_bias_variance}) 
which is a design parameter. The noise variance $\sigma^{2}$ is the intrinsic accuracy limit of our toy 
model \eqref{equ_linear_obs_model} and is not under the control of the ML engineer. It is impossible for 
any ML method (no matter how clever it is engineered) to achieve, on average, a small prediction error 
than the noise variance $\sigma^{2}$. 

We finally highlight that our analysis of bias \eqref{equ_def_bias_term}, variance \eqref{equ_formulae_variance_toy_model} 
and the average prediction error \eqref{equ_decomp_E_pred_toy_model} achieved by linear regression only 
applies if the observed data points are well modelled as realizations of random vectors according to 
\eqref{equ_linear_obs_model}, \eqref{equ_toy_model_iid} and \eqref{equ_labels_training_data}. The 
usefulness of this model for the data arising in a particular application has to be verified in practice by 
some validation techniques \cite{Young93,Vasicek76}. 

An alternative approach for analyzing bias, variance and average 
prediction error of linear regression is to use simulations. Here, we 
generate a number of i.i.d.\ copies of the observed data points by 
some random number generator \cite{Andrieu2001}. Using these 
i.i.d.\ copies, we can replace exact computations (expectations) 
by empirical approximations (sample averages).  


\section{Diagnosis} 
Consider a predictor $\hat{h}$ obtained from ERM \eqref{equ_def_ERM_funs} 
with training error $\emperror(\hat{h}| \trainset)$ and validation 
error $\emperror(\hat{h}| \dataset^{(\rm val)})$. By comparing 
the two numbers $\emperror(\hat{h}| \trainset)$ and $\emperror(\hat{h}| \dataset^{(\rm val)})$ 
with some desired or tolerated error $E_{0}$, we can get 
some idea of how to adapt the current ERM approach 
(see \eqref{equ_def_ERM_funs}) to improve performance: 
\begin{itemize} 
\item $\emperror(h| \trainset) \approx \emperror(h| \valset) \approx E_{0}$: 
There is not much to improve regarding prediction accuracy since we 
achieve the desired error on both training and validation set.
\item $\emperror(h| \valset) \gg \emperror(h| \trainset) \approx E_{0}$: 
The ERM \eqref{equ_def_ERM_funs} results in a hypothesis $\hat{h}$ 
with sufficiently small training error but when applied to new (validation) 
data the performance of $\hat{h}$ is significantly worse. This is an 
indicator of overfitting which can be addressed by regularization 
techniques (see Section \ref{sec_regularization}). 
\item $\emperror(h| \trainset) \gg \emperror(h| \valset)$: 
This indicates that the method for solving the ERM \eqref{equ_def_ERM_funs} 
is not working properly. The training error obtained by solving the 
ERM \eqref{equ_def_ERM_funs} should always be smaller than the 
validation error. When using GD for solving ERM, one particular 
reason for $\emperror(h| \trainset) \gg \emperror(h| \valset)$ 
could be that the step size $\alpha$ in the GD step \eqref{equ_def_GD_step} 
is chosen too large (see Figure \ref{fig_small_large_alpha}-(b)). 
\end{itemize}

\section{Exercises} 
\subsection{Validation Set Size} 
Consider a linear regression problem with data points characterized 
by a scalar feature and numeric label. Assume data points are i.i.d. 
Gaussian with zero-mean and covariance $\mathbf{C}$ How many data points 
do we need for a validation set such that the probability that the MSE incurred on the validation 
does not deviate by more than $20$ percent from the average MSE is larger than $0.8$. 



\newpage 
\chapter{Regularization}
\label{ch_overfitting_regularization}

%??????????
%???  maybe Gradient descent method could be explained in slightly details, which is a very important optimization method. also you can add some content of regularized least squares.????
%???? discuss oferfitting in detail for linear regression; the fundamental limit (phase transition) is ``nr of features >= sample size'' ????
%%??????????

A main reason for validating predictors is to detect {\bf overfitting}. The 
phenomenon of overfitting is one of the key obstacles for the successful 
application of ML methods. In case of overfitting, the ERM approach can 
be highly misleading. 

The ERM principle only makes sense if the empirical risk (training error), 
(see \eqref{equ_def_emp_error}) incurred by a predictor when applied to 
some labeled data points (training data) $\dataset= \{ \vz^{(\sampleidx)} = (\vx^{(\sampleidx)},y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize}$, 
is a good indicator for the average prediction error (see \eqref{equ_decomp_E_pred_toy_model}) 
incurred by that predictor on new data points which are different 
from the training data. 

One main pitfall for ERM is the phenomenon of overfitting. 
A predictor $h: \mathbb{R}^{\featuredim} \rightarrow \mathbb{R}$ 
obtained by the ERM is said to {\bf overfit} the training set if it 
has a {\bf small training error} but a {\bf large average prediction error} 
on other data points outside the training set. 

A main cause for overfitting is that the hypothesis space 
$\hypospace$ is chosen too large. If the hypothesis space is 
too large, ML methods based on solving the ERM \eqref{equ_def_ERM_funs} 
can choose from so many different maps $h \in \hypospace$ 
(from features $\vx$ to label $y$) that just ``by luck'' it will find 
a good one for a given training dataset. However, the resulting 
small empirical risk on the training dataset is highly misleading 
since if a predictor was good for the training dataset just 
``by accident'', we can not expect hat it will be any good for 
other data points.

Section \ref{sec_robustness} discusses the relation between 
the tendency of a method to overfit training data and its robustness. 
A ML method is robust if it tolerates small perturbations (errors) 
in the training data. Intuitively, forcing a method to tolerate small 
perturbations in the training error should counteract the tendency 
of the method to overfit the training data. 

\section{Regularized ERM} 
It seems reasonable to avoid overfitting by pruning the 
hypothesis space $\hypospace$, i.e., removing some of 
its elements. In particular, instead of solving \eqref{equ_def_ERM_funs} 
we solve the restricted ERM 
\begin{equation}
\label{equ_ERM_fun_pruned}
   \hat{h} = \argmin_{h \in \hypospace'} \emperror(h|\dataset) \mbox{ with pruned hypothesis space } \hypospace' \!\subset\!\hypospace. 
\end{equation}
Another approach to avoid overfitting is to regularize the ERM \eqref{equ_def_ERM_funs} 
by adding a penalty term $\mathcal{R}(h)$ which somehow measures the complexity or 
non-regularity of a predictor map $h$ using a non-negative number $\mathcal{R}(h) \in \mathbb{R}_{+}$. 
We then obtain the regularized ERM 
\begin{equation}
\label{equ_ERM_fun_regularized}
  \hat{h} = \argmin_{h \in \hypospace} \emperror(h|\dataset)  + \mathcal{R}(h). 
\end{equation} 
The additional term $\mathcal{R}(h)$ aims at approximating (or anticipating) the 
increase in the empirical risk of a predictor $\hat{h}$ when it is applied to new 
data points, which are different from the dataset $\dataset$ used to learn the 
predictor $\hat{h}$ by \eqref{equ_ERM_fun_regularized}. 

The two approaches \eqref{equ_ERM_fun_pruned} and \eqref{equ_ERM_fun_regularized}, for 
making ERM \eqref{equ_def_ERM_funs} robust against overfitting are closely related. In particular, 
these two approaches are, in a certain sense, {\bf dual} to each other: for a given restriction 
$\hypospace' \subset \hypospace$ we can find a penalty $\mathcal{R}(h)$ term such that the 
solutions of \eqref{equ_ERM_fun_pruned} and \eqref{equ_ERM_fun_regularized} coincide. Similarly 
for a many popular types of penalty terms $\mathcal{R}(h)$, we can find a restriction $\hypospace' \subset \hypospace$ 
such that the solutions of \eqref{equ_ERM_fun_pruned} and \eqref{equ_ERM_fun_regularized} coincide. 
This statements can be made precise using the theory of duality for optimization problems (see \cite{BertsekasNonLinProgr}). 



In what follows we will analyze the occurrence of overfitting in Section \ref{sec_overfitting} and then discuss in 
Section \ref{sec_regularization} how to avoid overfitting using regularization. 


\section{Robustness} 
\label{sec_robustness} 

Overfitting is one of main challenges in applying modern ML  methods. Modern ML 
methods use large hypothesis spaces that allow to represent highly non-linear predictor 
maps. Just by pure luck we can find one such predictor map that perfectly fits the 
training set resulting in zero training error and, in turn, solving ERM \eqref{equ_def_ERM_funs}. 

Overfitting is closely related to another property of ML methods: robustness. If a method 
overfits it will typically be not robust to small perturbations in the training data. The robustness 
to small perturbations in the data is almost a mandatory requirement for ML methods  to be 
useful in important application domains. 

The ML methods discussed in Chapter \ref{ch_Optimization} rest on the 
idealizing assumption that we have access to the true label values and feature 
values of a set of data points (the training set). However, the means by which 
the label and feature values are determing are prone to errors. These errors might 
stem from the measurement device itself (hardward failures) or might be 
due to modelling errors. We need ML methods that do not ``break'' if we feed 
it slightly perturbed label values for the training data. 




 \begin{figure}[htbp]
	\centering
	\includegraphics[width=\textwidth]{RobustnessOverfitting.jpg}  
	\caption{Modern ML methods allow to find a predictor map that perfectly fits 
		training data. Such a predictor might perform poorly on a new data point 
		outside the training set. To prevent learning such a predictor 
		map we could require it to be robust against small perturbations in the features of the 
        training data points or the predictor map itself.}
	\label{fig_polyn_training}
\end{figure}

\section{Data Augmentation} 
\label{sec_data_augmentation} 
implement robustness principle by augmenting dataset with random perturbatios of original training data. 



\section{Regularized Linear Regression}
\label{sec_regularization}

As mentioned above, the overfitting of the training data 
$\dataset^{(\rm train)}=\{(\vx^{(\sampleidx)},y^{(\sampleidx)})\}_{\sampleidx=1}^{\samplesize_{\rm t}}$ 
might be caused by choosing the hypothesis space too large. Therefore, we can avoid 
overfitting by making (pruning) the hypothesis space $\hypospace$ smaller to obtain a new 
hypothesis space $\hypospace_{\rm small}$. This smaller hypothesis space $\hypospace_{\rm small}$ 
can be obtained by pruning, i.e., removing certain maps $h$, from $\hypospace$. 

A more general strategy is {\bf regularization}, which amounts to modifying the loss 
function of an ML problem in order to favour a subset of predictor maps. Pruning the 
hypothesis space can be interpreted as an extreme case of regularization, where the 
loss functions become infinite for predictors which do not belong to the smaller 
hypothesis space $\hypospace_{\rm small}$. 

%
%???
%In some sense, regularization provides a soft variant of the model selection method discussed in \ref{}. There, we discussed 
%how to choose the best hypothesis space out of a sequence of increasing hypothesis spaces.  Use interpretation of regu. as 
%anticipating inrease in validation error from Round 4 noteboook ..$
%????
%This 

In order to avoid overfitting, we have to augment our basic ERM approach 
(cf.\ \eqref{equ_def_ERM_funs}) by {\bf regularization techniques}. According 
to \cite{Goodfellow-et-al-2016}, regularization aims at ``any modification we 
make to a learning algorithm that is intended to reduce its generalization error 
but not its training error.'' By generalization error, we mean the average prediction 
error (see \eqref{equ_decomp_E_pred_toy_model}) incurred by a predictor 
when applied to new data points (different from the training set). 

A simple but effective method to regularize the ERM learning principle, is to 
augment the empirical risk \eqref{equ_def_cost_linreg} of linear regression 
by the penalty term $\mathcal{R}(h^{(\vw)}) \defeq \lambda \| \mathbf{w} \|^{2}_{2}$, 
which penalizes overly large weight vectors $\mathbf{w}$. Thus, we arrive 
at {\bf regularized ERM}
\begin{align} 
\label{equ_rerm}
\hat{\vw}^{(\lambda)}  & = \argmin_{h^{(\vw)} \in  \hypospace} \big[ \emperror(h^{(\vw)}|\dataset^{(\rm train)})+ \lambda \| \mathbf{w} \|^{2} \big] \nonumber \\ %  \nonumber \\ 
&  = \argmin_{h^{(\vw)} \in  \hypospace} \big[(1/\samplesize_{\rm t}) \sum_{\sampleidx=1}^{\samplesize_{\rm t}} \loss{(\vx^{(\sampleidx)},y^{(\sampleidx)})}{h^{(\vw)}} + \lambda \| \mathbf{w} \|^{2} \big],
\end{align}
with the regularization parameter $\lambda >0$. The parameter $\lambda$ trades 
a small training error $\emperror(h^{(\vw)}|\dataset)$ against a small norm $\| \mathbf{w} \|$ 
of the weight vector. In particular, if we choose a large value for $\lambda$, then 
weight vectors $\vw$ with a large norm $\| \vw \|$ are ``penalized'' by having a larger 
objective function and are therefore unlikely to be a solution (minimizer) of the 
optimization problem \eqref{equ_rerm}. 

Specialising \eqref{equ_rerm} to the squared error loss and linear predictors 
yields {\bf regularized linear regression} (see \eqref{equ_def_cost_MSE}): 
\begin{equation} 
\label{equ_reg_lin_reg_opt_weight}
\hat{\vw}^{(\lambda)} = \argmin_{\vw \in \mathbb{R}^{\featuredim}} \big[ (1/\samplesize_{\rm t}) \sum_{\sampleidx=1}^{\samplesize_{\rm t}}  (y^{(\sampleidx)} - \vw^{T}\vx^{(\sampleidx)}  )^2 + \lambda \| \vw \|^{2}_{2}\big], 
\end{equation} 
The optimization problem \eqref{equ_reg_lin_reg_opt_weight} is also known 
under the name {\bf ridge regression} \cite{hastie01statisticallearning}. 

%?????
%It is interesting to note that ridge regression \eqref{equ_reg_lin_reg_opt_weight} arises naturally as the optimal Bayes estimator of the weight vector $\vw$ if it is modeled 
%as a Gaussian random vector $\vw \sim \mathcal{N}(\mathbf{0},\sigma_{w}^{2} \mathbf{I}$ etcetcetc. Moreover, the ridge regression problem \eqref{equ_reg_lin_reg_opt_weight} 
%turns out to be almost optimal in terms of minimizing the ???regret??? when implemented as an online learning method \cite[Ch. 11]{PredictionLearningGames}
%MAYBE discuss online learning interpretation of ridge regression in a new section "Online Learning" which comes after Chapter 7 
%??????

Using the feature matrix $\mX=\big(\vx^{(1)},\ldots,\vx^{(\samplesize_{\rm t})}\big)^{T}$ and label vector $\vy=(y^{(1)},\ldots,y^{(\samplesize_{\rm t})})^{T}$, 
we can rewrite \eqref{equ_reg_lin_reg_opt_weight} more compactly as 
\begin{equation} 
\label{equ_def_rlr_w_opt}
\hat{\vw}^{(\lambda)} = \argmin_{\vw \in \mathbb{R}^{\featuredim}} \big[ (1/\samplesize_{\rm t}) \| \vy - \mX \vw \|^{2}_{2} + \lambda \| \vw \|^{2}_{2}\big].
\end{equation} 
The solution of \eqref{equ_def_rlr_w_opt} is given by 
\begin{equation}
\label{equ_close_form_reglinreg}
\hat{\vw}^{(\lambda)} = (1/\samplesize_{\rm t}) ((1/\samplesize_{\rm t}) \mX^{T} \mX + \lambda \mathbf{I})^{-1} \mX^{T} \vy. 
\end{equation}
This reduces to the closed-form expression \eqref{equ_optimal_weight_closed_form} 
when $\lambda=0$ in which case regularized linear regression reduces to ordinary linear 
regression (see \eqref{equ_reg_lin_reg_opt_weight} and \eqref{equ_def_cost_MSE}). It 
is important to note that for $\lambda>0$, the formula \eqref{equ_close_form_reglinreg} 
is always valid, even when $\mX^{T} \mX$ is singular (not invertible). This implies, in turn, 
that for $\lambda> 0$ the optimization problem \eqref{equ_def_rlr_w_opt} (and \eqref{equ_reg_lin_reg_opt_weight}) 
have a unique solution (which is given by \eqref{equ_close_form_reglinreg}). 

Let us now study the effect of regularization on the resulting bias, variance and average 
prediction error incurred by the predictor $h^{(\hat{\vw}^{(\lambda)})}(\vx) = \big(\hat{\vw}^{(\lambda)}\big)^{T} \vx$. 
To this end, we will again invoke the simple probabilistic toy model (see \eqref{equ_linear_obs_model}, \eqref{equ_toy_model_iid} 
and \eqref{equ_labels_training_data}) used already in Section \ref{sec_gen_linreg}. In particular, we interpret 
the training data $\dataset^{(\rm train)} = \{ (\vx^{(\sampleidx)},y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize_{\rm t}}$ 
as realizations of i.i.d.\ random variables according to \eqref{equ_linear_obs_model}, \eqref{equ_toy_model_iid} and \eqref{equ_labels_training_data}. 
%We aim at predicting the label $y$ based on the first $r$ entries of the feature vector $\vx$ (see \eqref{equ_generalization_hypospace_r}). 

As discussed in Section \ref{sec_gen_linreg}, the average prediction error is the sum of three 
components: the bias, the variance and the noise variance $\sigma^{2}$ (see \eqref{equ_decomp_E_pred_toy_model}). 
The bias of regularized linear regression \eqref{equ_reg_lin_reg_opt_weight} is obtained as 
\begin{equation} 
\label{equ_bias_reg_lin_reg}
B^{2} = \big \| (\mathbf{I} - \expect \{ ( \mX^{T} \mX +  \samplesize \lambda \mathbf{I})^{-1} \mX^{T} \mX  \})  \vw_{\rm true} \big  \|^{2}_{2}. 
%B^{2} =    \| (\mathbf{I}-\mX_{r}^{T} \mX_{r} ) \mathbf{I}_{r \times \featuredim} \vw_{\rm true} \}   \} \|^{2}_{2} \geq \sum_{l=r+1}^{\featuredim} w_{{\rm true},l}^2  
\end{equation} 
For sufficiently large sample size $\samplesize_{\rm t}$ we can use the approximation 
\begin{equation} 
\label{equ_approx_Gram_large_N}
\mX^{T} \mX  \approx \samplesize_{\rm t} \mathbf{I} 
\end{equation} 
such that \eqref{equ_bias_reg_lin_reg} can be approximated as 
 \begin{align} 
\label{equ_bias_reg_lin_reg_approx}
B^{2} & \approx \big \| (\mathbf{I}\!-\!(\mathbf{I}\!+\!\lambda \mathbf{I})^{-1} ) \vw_{\rm true} \big  \|^{2}_{2} \nonumber \\
& =  \sum_{l=1}^{\featuredim} \frac{\lambda}{1+\lambda} w_{{\rm true},l}^2 .
\end{align} 
By comparing the (approximate) bias term \eqref{equ_bias_reg_lin_reg_approx} of regularized linear regression 
with the bias term \eqref{equ_def_bias_term} obtained for ordinary linear regression, we see that introducing 
regularization typically increases the bias. The bias increases with larger values of the regularization parameter $\lambda$. 

The variance of regularized linear regression \eqref{equ_reg_lin_reg_opt_weight} satisfies 
\begin{align}
\label{equ_variance_reglinreg}
V & =( \sigma^{2}/\samplesize^{2}_{\rm t}) \times \nonumber \\ 
& \hspace*{-7mm} {\rm trace} \expect \{  ( (1/\samplesize_{\rm t}) \mX^{T} \mX\!+\!\lambda \mathbf{I} )^{-1} \mX^{T} \mX ( (1/\samplesize_{\rm t}) \mX^{T} \mX\!+\!\lambda \mathbf{I} )^{-1}   \}.
\end{align}
Using the approximation \eqref{equ_approx_Gram_large_N}, which is reasonable for sufficiently 
large sample size $\samplesize_{\rm t}$, we can in turn approximate \eqref{equ_variance_reglinreg} as 
\begin{equation}
\label{equ_variance_reglinreg_approx}
V \approx \sigma^{2} (\featuredim/\samplesize_{\rm t}) (1/(1+ \lambda)).
\end{equation} 
According to \eqref{equ_variance_reglinreg_approx}, the variance of regularized linear regression decreases with 
increasing regularization $\lambda$. Thus, as illustrated in Figure \ref{fig_bias_variance_lambda}, the choice of 
$\lambda$ has to balance between the bias $B^{2}$ \eqref{equ_bias_reg_lin_reg_approx} (which increases with 
increasing $\lambda$) and the variance $V$ \eqref{equ_variance_reglinreg_approx} (which decreases with 
increasing $\lambda$). This is another instance of the bias-variance tradeoff (see Figure \ref{fig_bias_variance}). 

\begin{figure}
\begin{center}
    \begin{tikzpicture}
              \draw [thick, <->] (0,3.5) -- (0,0) -- (6.2,0);
             \draw[red, ultra thick, domain=0:5] plot (\x,  {2*\x/(1+\x)}) node [above] {bias of $\hat{\vw}^{(\lambda)}$};
              \draw[blue, ultra thick, domain=0:5] plot (\x,  {2/(1+\x)}) node [above]  {variance of $\hat{\vw}^{(\lambda)}$};
              \node [below] at (5,0) {regularization parameter $\lambda$};
     \end{tikzpicture}
\end{center}
\caption{The bias and variance of regularized linear regression depend on the regularization parameter 
$\lambda$ in an opposite manner resulting in a bias-variance tradeoff.}
\label{fig_bias_variance_lambda}
\end{figure}

%???? 
%Choice of lambda can be guided by validation error 
%???????

So far, we have only discussed the statistical effect of regularization on the resulting ML method (how 
regularization influences bias, variance, average prediction error). However, regularization has also 
an effect on the computational properties of the resulting ML method. Note that the objective function 
in \eqref{equ_def_rlr_w_opt} is a smooth (infinitely often differentiable) convex function. Thus, as for 
linear regression, we can solve the regularization linear regression problem using GD \eqref{equ_squared_loss} 
(see Algorithm \ref{alg:gd_reglinreg}). The effect of adding the regularization term $\lambda \| \vw \|^{2}_{2}$ 
to the objective function within linear regression is a {\bf speed up of GD}. Indeed, we can rewrite \eqref{equ_def_rlr_w_opt} 
as the quadratic problem 
\begin{align} 
\label{equ_quadr_form_reglinreg}
\min_{\vw \in \mathbb{R}^{\featuredim}} & \underbrace{(1/2) \vw^{T} \mQ \vw - \vq^{T}  \vw}_{= f(\vw)} \nonumber \\ 
 & \mbox{ with } \mathbf{Q}= (1/\samplesize) \mX^{T} \mX + \lambda \mathbf{I}, \vq =(1/\samplesize) \mX^{T} \vy. %\mathbf{X}^{T} 
%\min_{\mathbf{w} \in \mathbb{R}^{\featuredim}} \mathbf{w}^{T} \mathbf{Q} \mathbf{w}  \mbox{, with } \mathbf{Q} = \mathbf{X}^{T} \mathbf{X} + \lambda \mathbf{I}.
\end{align} 
This is similar to the quadratic problem \eqref{equ_quadr_form_linreg} underlying linear regression but 
with different matrix $\mQ$. It turns out that the convergence speed of GD (see \eqref{equ_def_GD_step}) 
applied to solving a quadratic problem of the form \eqref{equ_quadr_form_reglinreg} depends crucially 
on the condition number $\kappa(\mQ) \geq 1$ of the psd matrix $\mQ$ \cite{JungFixedPoint}. In particular, 
GD methods are fast if the condition number $\kappa(\mQ)$ is small (close to $1$). 

This condition number is given by $\frac{\lambda_{\rm max}((1/\samplesize) \mX^{T} \mX)} {\lambda_{\rm min}((1/\samplesize) \mX^{T} \mX)}$ for ordinary linear regression 
(see \eqref{equ_quadr_form_linreg}) and given by $\frac{\lambda_{\rm max}((1/\samplesize)\mX^{T} \mX) + \lambda} {\lambda_{\rm min}((1/\samplesize)\mX^{T} \mX)+ \lambda}$ 
for regularized linear regression \eqref{equ_quadr_form_reglinreg}. For increasing regularization parameter $\lambda$, the condition number 
obtained for regularized linear regression \eqref{equ_quadr_form_reglinreg} tends to $1$: 
\begin{equation}
\label{equ_cond_numer_goes_1_rlr}
\lim_{\lambda \rightarrow \infty} \frac{\lambda_{\rm max}((1/\samplesize)\mX^{T} \mX) + \lambda} {\lambda_{\rm min}((1/\samplesize)\mX^{T} \mX)+ \lambda} =1. 
\end{equation} 
Thus, according to \eqref{equ_cond_numer_goes_1_rlr}, the GD implementation of regularized linear 
regression (see Algorithm \ref{alg:gd_reglinreg}) with a large value of the regularization parameter 
$\lambda$ in \eqref{equ_reg_lin_reg_opt_weight} will converge faster compared to GD for linear 
regression (see Algorithm \ref{alg:gd_linreg}). 


\begin{algorithm}[htbp]
\caption{``Regularized Linear Regression via GD''}\label{alg:gd_reglinreg}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\Require   labeled dataset $\dataset=\{ (\vx^{(\sampleidx)}, y^{(\sampleidx)}) \}_{\sampleidx=1}^{\samplesize}$ containing feature vectors 
$\vx^{(\sampleidx)} \in \mathbb{R}^{\featuredim}$ and labels $y^{(\sampleidx)} \in \mathbb{R}$; GD step size $\alpha >0$. 
\Statex\hspace{-6mm}{\bf Initialize:}set $\vw^{(0)}\!\defeq\!\mathbf{0}$; set iteration counter $k\!\defeq\!0$   
\Repeat 
\State $k \defeq k +1$    (increase iteration counter) 
\State  $\vw^{(k)} \defeq (1- \alpha \lambda) \vw^{(k\!-\!1)} + \alpha (2/\samplesize) \sum_{\sampleidx=1}^{\samplesize} (y^{(\sampleidx)} - \big(\vw^{(k\!-\!1)})^{T} \vx^{(\sampleidx)}) \vx^{(\sampleidx)}$  (do a GD step \eqref{equ_def_GD_step})
\Until convergence 
\Ensure $\vw^{(k)}$ (which approximates $\hat{\vw}^{(\lambda)}$ in \eqref{equ_def_rlr_w_opt})
\end{algorithmic}
\end{algorithm}

Let us finally point out a close relation between regularization (which amounts to 
adding the term $\lambda \| \vw \|^{2}$ to the objective function in \eqref{equ_rerm}) 
and model selection (see Section \ref{sec_modsel}). The regularized ERM \eqref{equ_rerm} 
can be shown (see \cite[Ch. 5]{BertsekasNonLinProgr}) to be equivalent to 
\begin{equation} 
\label{equ_restr_ERM}
\hat{\vw}^{(\lambda)} = \argmin_{h^{(\vw)} \in \hypospace^{(\lambda)}}  (1/m_{\rm t}) \sum_{\sampleidx=1}^{\samplesize_{\rm t}}  (y^{(\sampleidx)} - h^{(\vw)}(\vx^{(\sampleidx)})  )^2
\end{equation}  
with the restricted hypothesis space
\begin{align} 
\label{equ_hyposapce_lambda}
\hypospace^{(\lambda)} & \defeq \{ h^{(\vw)}: \mathbb{R}^{\featuredim} \rightarrow \mathbb{R}: h^{(\vw)}(\vx) = \vw^{T} \vx \nonumber \\
& \mbox{, with some } \vw \mbox{ satisfying } \| \vw \|^{2} \leq C(\lambda) \} \subset \hypospace^{(\featuredim)}. 
\end{align} 
For any given value $\lambda$, we can find a bound $C(\lambda)$ such that 
solutions of \eqref{equ_rerm} coincide with the solutions of \eqref{equ_restr_ERM}. 
Thus, by solving the regularized ERM \eqref{equ_rerm} we are performing implicitly 
model selection using a continuous ensemble of hypothesis spaces $\hypospace^{(\lambda)}$ 
given by \eqref{equ_hyposapce_lambda}. In contrast, the simple model selection strategy 
considered in Section \ref{sec_modsel} uses a discrete sequence of hypothesis spaces.   

%\subsection{How To Choose $\lambda$?} 
%????
%Using a validation set. Given a finite set of candidate values of lambda, compute the val error of those 
%candidates and pick the best. Why not pushing this further and derive a close-form expression for val error (for a fixed particular val set!) 
%and then minimize this expression? --> by cannot test too many values of lambda due to limited information content in one particular val set (related to uniform convergence principle) 
%---> moreover, we are not interested in optimal val error for one fixed val set but in an average val error (=generalization error). 
%?????
%Can we use closed form experessions for bias and variance obtained for our toy model ? In general, not possible since bias 
%depends on unknown true weight vector. However, we might have some partial information about weight vector (belongs to some convex set) which 
%would allow to get some idea (minimax?) of how to choose $\lambda$ 
%????????
%???????
%The SURE depends solely on y, without prior knowledge of x0. This can prove very useful as a basis to automatically choose the parameters ? of the estimator, e.g. ? = ? when solving (2), or the smoothing parameters in families of linear estimates [28] such as for ridge regression or smoothing splines. In some settings, it has been shown that it offers better accuracy than GCV and related non-parametric selection techniques [29]. However, compared to GCV, the SURE requires the knowledge of the noise variance ?2  see \url{https://arxiv.org/pdf/1204.3212.pdf}
%??????

\section{Semi-Supervised Learning} 
%%% use unlabeled data to construct regularizers ?? 
Can we use unlabelled data points to construct better regularizers? We could use unlabled data 
to learn some subspace of features that are most relevant ? (relation to feature learning ?)

\section{Multitask Learning} 
%%%% use different tasks to construct regularizers ????
Remember that a formal ML problem is specified by identifying data points, their features and labels, 
a model (hypothesis space) and loss function. Note that we can use the very same raw data, model 
and loss function and still define many different ML problems by using different choices for the label. 
Multitask learning aims at exploiting relations between similar ML problems or tasks. 

Consider the ML problem (task) of predicting the confidence level of a hand-drawing showing an apple. 
To learn such a predictor we might have a collecting of hand-drawings at our disposal. We might now 
for each hand-drawing certain higher-level information such as the object it is showing. This allows us 
to use different choices for the label. We could also use the confidence level of a hand-drawing showing 
an orange. Clearly this problem is related to the problem of predicting the apple confidence. 


The definition (design choice) of the labels corresponds to formulating a particular question we want to 
have answered by an ML method. Some questions (label choices) are more difficult to answer while 
others are easier to answer. 

Consider the ML problem arising from guiding the operation of a mower robot. For a mowing robot, it is 
important to determine if it is currently on grassland or not. Let us assume the mower robot is equipped 
with an on-board camera which allows to take snapshots which are characterized by a feature vector 
$\mathbf{x}$ (see Figure \ref{fig_snapshot_pixels}). We could then define the label as either $y=1$ 
if the snapshot suggests that the mower is on grassland and $y=-1$ if not. However, we might be 
interested in a finer-grained information about the floor type and define the label as $y=1$ for grassland, 
$y=0$ for soil and $y=-1$ for when the mower is on tiles. The latter problem is more difficult 
since we have to distinguish between three different types of floor (``grass'' vs. ``soil'' vs. ``tiles'') 
whereas for the former problem we only have to distinguish between two types of floor (``grass'' vs. ``no grass''). 


\section{Exercises} 
\subsection{Ridge Regression as Quadratic Form} 
\label{ex_2_0}
Consider linear hypothesis space consisting of linear maps parameterized by 
weights $\mathbf{w}$. We try to find the best linear map by minimizing the regularized average 
squared error loss (empirical risk) incurred on some labeled training data points 
$(\vx^{(1)},y^{(1)}),(\vx^{(2)},y^{(2)}),\ldots,(\vx^{(\samplesize)},y^{(\samplesize)})$. 
As regularizer we use $\| \mathbf{w} \|^{2}$, yielding the following learning problem 
$$ \min_{\vw} f(\vw) = \sum_{i=1}^{\samplesize}\ldots + \| \mathbf{w} \|^{2}_{2}$$

Is it possible to write the objective function $f(\vw)$ as 
a convex quadratic form $f(\vw) = \vw^{T} \mathbf{C} \vw + \vb \vw + c$? If this 
is possible, how are the matrix $\mathbf{C}$, vector $\vb$ and constant $c$ related 
to the feature vectors and labels of the training data ? 



\newpage
\chapter{Clustering} 
\label{ch_Clustering}



\begin{figure}[htbp]
\begin{center}
\begin{tikzpicture}[auto,scale=0.8]
\draw [thick] (5,2.5) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(3)}$};
\draw [thick] (4,2) circle (0.1cm)node[anchor=west] {\hspace*{0mm}$\vx^{(4)}$};
\draw [thick] (5,1) circle (0.1cm)node[anchor=west,above] {\hspace*{0mm}$\vx^{(2)}$};
\draw [thick] (1,5) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {\hspace*{0mm}$\vx^{(1)}$};
\draw [thick] (1,3.5) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {\hspace*{0mm}$\vx^{(5)}$};
\draw [thick] (1,2.5) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {\hspace*{0mm}$\vx^{(6)}$};
\draw [thick] (2,4) rectangle ++(0.1cm,0.1cm) node[anchor=west,above] {\hspace*{0mm}$\vx^{(7)}$};
\draw[->] (-0.5,0) -- (6.5,0) node[right] {$x_{\rm g}$};
\draw[->] (0,-0.5) -- (0,6.5) node[above] {$x_{\rm r}$};
%\foreach \y/\ytext in { 1/1,2/2,3/3,4/4,5/5} \draw[shift={(0,\y)}] (2pt,0pt) -- (-2pt,0pt) node[left] {$\ytext/5$};  
%\foreach \x/\xtext in{ 1/1,2/2,3/3,4/4,5/5}\draw[shift={(\x,0)}] (0pt,2pt) -- (0pt,-2pt) node[below] {$\xtext/5$};  
\end{tikzpicture}
\end{center}
\caption{A scatterplot obtained from the features $\vx^{(\sampleidx)}=(x^{(\sampleidx)}_{\rm r},x^{(\sampleidx)}_{\rm g})^{T}$, 
	given by the redness $x_{\rm r}^{(\sampleidx)}$ and greenness $x_{\rm g}^{(\sampleidx)}$, of some snapshots. 
} 
\label{fig_scatterplot_clustering}
\end{figure}

Up to now, we mainly considered ML methods which required some labeled training data in order to 
learn a good predictor or classifier. We will now start to discuss ML methods which do not make use 
of labels. These methods are often referred to as ``unsupervised'' since they do not require a supervisor 
(or teacher) which provides the labels for data points in a training set. 

An important class of unsupervised methods, known as clustering methods, aims at grouping data points 
into few subsets (or {\bf clusters}). While there is no unique formal definition, we understand by cluster a 
subset of data points which are more similar to each other than to the remaining data points (belonging 
to different clusters). Different clustering methods are obtained for different ways to measure the 
``similarity'' between data points. 

In what follows we assume that data points $\vz^{(\sampleidx)}$, for $\sampleidx=1,\ldots,\samplesize$, 
are characterized by feature vectors $\vx^{(\sampleidx)} \in \mathbb{R}^{\featuredim}$ and measure 
similarity between data points using the Euclidean distance $\| \vx^{(\sampleidx)} - \vx^{(j)} \|$.
\footnote{With a slight abuse of notation, we will occasionally denote a data point $\vz^{(\sampleidx)}$ 
using its feature vector $\vx^{(\sampleidx)}$. In general, the feature vector is only a (incomplete) representation 
of a data point but it is costumary in many unsupervised ML methods to identify a data point with its features.} 
Thus, we consider two data points $\vz^{(\sampleidx)}$ and $\vz^{(j)}$ similar if $\| \vx^{(\sampleidx)} - \vx^{(j)} \|$ 
is small. Moreover, we assume the number $k$ of clusters prescribed. 


Consider the scatter plot in Figure \ref{fig_scatterplot_clustering} which is obtained from the redness 
and greenness of several snapshots, which we would like to order into two groups: those snapshots 
which depict mostly grass and those which depict mostly something else than grass. We characterize 
a snapshot using the feature vector $\vx = (x_{r},x_{g})^{T} \in \mathbb{R}^{2}$ and the label $y \in \{ 1,2 \}$ 
with $y=1$ if the snapshot shows grass and $y=2$ otherwise. 

If we were able to group the snapshots based solely on their features $\vx^{(\sampleidx)}$ into $k=2$ 
groups $\mathcal{C}_{1}$ (which correspond to snapshots of grass) and $\mathcal{C}_{2}$ (no grass), 
we would need to acquire the label of one data point only. Indeed, given a perfect clustering, if we know 
the label of one data point, we immediately know the label of all other data points in the same cluster. 
Moreover, we would also know the labels of the data points in the other cluster, since there 
are only two possible label values.  

There are two main flavours of clustering methods: 
\begin{itemize} 
\item hard clustering (see Section \ref{sec_hard_clustering}) 
\item and soft clustering methods (see Section \ref{sec_soft_clustering}).
\end{itemize}
Within hard clustering, each data point $\vx^{(\sampleidx)}$ belongs to one and only one cluster. In contrast, 
soft clustering methods assign a data point $\vx^{(\sampleidx)}$ to several different clusters with varying degree 
of belonging (confidence). 

Clustering methods determine for each data point $\vz^{(\sampleidx)}$ a cluster assignment $y^{(\sampleidx)}$. 
The cluster assignment $y^{(\sampleidx)}$ encodes the cluster to which the data point $\vx^{(\sampleidx)}$ is assigned. 
For hard clustering with a prescribed number of $k$ clusters, the cluster assignments $y^{(\sampleidx)} \in \{1,\ldots,k\}$ 
represent the index of the cluster to which $\vx^{(\sampleidx)}$ belongs. 

In contrast, soft clustering methods allow each data point to belong to several different clusters. The degree with which 
data point $\vx^{(\sampleidx)}$ belongs to cluster $c \in \{1,\ldots,k\}$ is represented by the degree of belonging $y_{c}^{(\sampleidx)} \in [0,1]$, 
which we stack into the vector $\vy^{(\sampleidx)} = \big(y_{1}^{(\sampleidx)},\ldots,y_{k}^{(\sampleidx)}\big)^{T} \in [0,1]^{k}$. 
Thus, while hard clustering generates non-overlapping clusters, the clusters produced by soft clustering methods may overlap. 

We intentionally used the same symbol $y^{(\sampleidx)}$ used to denote the cluster assignments of a data point $\vx^{(\sampleidx)}$ as 
we used to denote an associated label $y^{(\sampleidx)}$ in earlier sections. There is a strong conceptual link between clustering and classification. 
\vspace*{2mm}
\begin{center}
\framebox[0.96\textwidth]{
 \parbox{0.9\textwidth}{
We can interpret clustering as an extreme case of classification without having access to any labeled training data, i.e., we do not 
know the label of any data point.
}}
\end{center}
Thus, in order to have any chance to find the correct labels (cluster assignments) $y_{c}^{(\sampleidx)}$ we have 
to rely solely on the intrinsic geometry of the data points which is given by the locations of the feature vectors $\vx^{(\sampleidx)}$. 





\newpage
\section{Hard Clustering}
\label{sec_hard_clustering}

A simple method for hard clustering is  the ``$k$-means'' algorithm which requires the number $k$ of 
clusters to specified before-hand. The idea underlying $k$-means is quite simple: First, given a current 
guess for the cluster assignments $y^{(\sampleidx)}$, determine the cluster means $\vm^{(c)} = \frac{1}{|\{i: y^{(\sampleidx)}= c\}|} \sum\limits_{i: y^{(\sampleidx)}=c} \vx^{(\sampleidx)}$ 
for each cluster. Then, in a second step, update the cluster assignments $y^{(\sampleidx)} \in \{1,\ldots,k\}$ 
for each data point $\vx^{(\sampleidx)}$ based on the nearest cluster mean. By iterating these two steps 
we obtain Algorithm \ref{alg:kmeans}. 


\begin{algorithm}[htbp]
\caption{``$k$-means''}\label{alg:kmeans}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\Require   dataset $\dataset=\{ \vx^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize}$; number $k$ of clusters. 
\Statex\hspace{-6mm}{\bf Initialize:} choose initial cluster means $\vm^{(c)}$ for $c=1,\ldots,k$.    
\Repeat
\State for each data point $\vx^{(\sampleidx)}$, $\sampleidx\!=\!1,\ldots,\samplesize$, do 
\begin{equation} 
\label{equ_cluster_assign_update}
y^{(\sampleidx)} \in \argmin\limits_{\clusteridx' \in \{1,\ldots,\nrcluster\}} \| \vx^{(\sampleidx)} - \vm^{(\clusteridx')} \|  \mbox{  (update cluster assignments)} 
\end{equation}
\State for each cluster $\clusteridx\!=\!1,\ldots,\nrcluster$ do 
\begin{equation}
\label{equ_cluster_mean_update} 
\vm^{(\clusteridx)}= \frac{1}{|\{\sampleidx: y^{(\sampleidx)}= \clusteridx\}|}  \sum_{\sampleidx: y^{(\sampleidx)} = \clusteridx} \vx^{(\sampleidx)}   \mbox{  (update cluster means) } 
\end{equation} 
\Until convergence 
\Ensure cluster assignments $y^{(\sampleidx)} \in \{1,\ldots,\nrcluster\}$
\end{algorithmic}
\end{algorithm}

In \eqref{equ_cluster_assign_update} we denote by $\argmin\limits_{\clusteridx' \in \{1,\ldots,\nrcluster\}} \| \vx^{(\sampleidx)} - \vm^{(\clusteridx')} \|$ the 
set of all cluster indices $\clusteridx \in \{1,\ldots,\nrcluster\}$ such that $ \| \vx^{(\sampleidx)} - \vm^{(\clusteridx)} \| =  \min_{\clusteridx' \in \{1,\ldots,\nrcluster\}} \| \vx^{(\sampleidx)} - \vm^{(\clusteridx')} \|$. 


The $k$-means algorithm requires the specification of initial choices for the cluster means $\vm^{(c)}$, for $c=1,\ldots,k$. 
There is no unique optimal strategy for the initialization but several heuristic strategies can be used. 
One option is to initialize the cluster means with i.i.d.\ realizations of a random vector $\vm$ whose 
distribution is matched to the dataset $\dataset = \{ \vx^{(\sampleidx)} \}_{\sampleidx=1}^{\samplesize}$, e.g., $\vm \sim \mathcal{N}( \hat{\vm}, \widehat{\mathbf{C}})$ 
with sample mean $\hat{\vm} = (1/\samplesize) \sum_{i=1}^{\samplesize} \vx^{(\sampleidx)}$ and the sample covariance 
$\widehat{\mathbf{C}} = (1/\samplesize) \sum_{i=1}^{\samplesize} (\vx^{(\sampleidx)}\!-\!\hat{\vm}) (\vx^{(\sampleidx)}\!-\!\hat{\vm})^{T}$. 
Another option is to choose the cluster means $\vm^{(c)}$ by randomly selecting $k$ different data points $\vx^{(\sampleidx)}$. 
The cluster means might also be chosen by evenly partitioning the principal component of the dataset 
(see Chapter \ref{ch_FeatureLearning}). 

It can be shown that $k$-means implements a variant of empirical risk minimization. 
To this end we define the empirical risk (or ``clustering error'')
\begin{equation}
\label{equ_def_emp_risk_kmeans}
\vspace*{-2mm}
 \emperror \big( \{\vm^{(\clusteridx)}\}_{\clusteridx=1}^{\nrcluster},\{y^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize} \mid \dataset \big)
=(1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} {\left\|\vx^{(\sampleidx)}-\vm^{(y^{(\sampleidx)})}\right\|^2}.
\end{equation} 
Note that the empirical risk \eqref{equ_def_emp_risk_kmeans} depends on the current guess for the cluster means 
$\{ \vm^{(\clusteridx)} \}_{\clusteridx=1}^{\nrcluster}$ and cluster assignments $\{ y^{(\sampleidx)} \}_{\sampleidx=1}^{\samplesize}$. 
%The optimal 
%cluster assignments and means minimize the empirical risk $\emperror \big( \{\vm^{(c)}\}_{c=1}^{k},\{y^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize} \mid \dataset \big)$. 

Finding the global optimum of the function \eqref{equ_def_emp_risk_kmeans} over all possible cluster 
means $\{\vm^{(\clusteridx)}\}_{\clusteridx=1}^{\nrcluster}$ and cluster assignments $\{y^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize}$ is difficult as the function is non-convex. 
However, minimizing \eqref{equ_def_emp_risk_kmeans} only with respect to the cluster assignments $\{y^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize}$ 
but with the cluster means $\{\vm^{(\clusteridx)}\}_{\clusteridx=1}^{\nrcluster}$ held fixed is easy. 
Similarly, minimizing \eqref{equ_def_emp_risk_kmeans} over the choices of cluster means with 
the cluster assignments held fixed is also straightforward. This observation is used by Algorithm 
\ref{alg:kmeans}:  it is alternatively minimizing $\emperror$ over all cluster means with the assignments 
$\{ y^{(\sampleidx)} \}_{\sampleidx=1}^{\samplesize}$ held fixed and minimizing $\emperror$ over all 
cluster assignments with the cluster means $\{ \vm^{(\clusteridx)} \}_{\clusteridx=1}^{\nrcluster}$ held fixed. 

The interpretation of Algorithm \ref{alg:kmeans} as a method for minimizing the cost function \eqref{equ_def_emp_risk_kmeans} is 
useful for convergence diagnosis. In particular, we might terminate Algorithm \ref{alg:kmeans} if the decrease of 
the objective function $ \emperror$ is below a prescribed (small) threshold. 
 
A practical implementation of Algorithm \ref{alg:kmeans} needs to fix three issues: 
\begin{itemize} 
\item Issue 1: We need to specify a ``tie-breaking strategy'' to handle the case when several 
different cluster indices $\clusteridx\!\in\!\{1,\ldots,\nrcluster\}$ achieve the minimum value in \eqref{equ_cluster_assign_update}. 
\item Issue 2: We need to specify how to handle the situation when after a cluster assignment 
update  \eqref{equ_cluster_assign_update}, there is a cluster $\clusteridx$ with no data points are 
associated with it, i.e., $|\{ \sampleidx: y^{(\sampleidx)} = \clusteridx \}|=0$. In this case, the cluster 
means update \eqref{equ_cluster_mean_update} would be not well defined for the cluster $\clusteridx$. 
\item Issue 3: We need to specify a stopping criterion (``checking convergence''). 
\end{itemize}
The following algorithm fixes those three issues in a particular way \cite{Gray1980}.  
\begin{algorithm}[htbp]
\caption{``$k$-Means II'' (a reformulation of the ``Fixed Point Algorithm'' presented in \cite{Gray1980})}\label{alg:kmeansimpl}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\Require   dataset  $\dataset=\{ \vx^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize}$; 
number $k$ of clusters; prescribed tolerance $\varepsilon \geq 0$. 
\Statex\hspace{-6mm}{\bf Initialize:} choose initial cluster means 
$\big\{ \vm^{(\clusteridx)} \big\}_{\clusteridx=1}^{\nrcluster}$ and 
cluster assignments $\big\{ y^{(\sampleidx)} \big\}_{\sampleidx=1}^{\samplesize}$;  
set iteration counter $r \defeq 0$; compute  $E^{(r)} = \emperror \big( \{\vm^{(\clusteridx)}\}_{\clusteridx=1}^{\nrcluster},\{y^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize} \mid \dataset \big)$;
\Repeat 
\State for all data points $i\!=\!1,\ldots,\samplesize$, update cluster assignment
\begin{equation} 
\label{equ_cluster_assign_update2}
y^{(\sampleidx)} \defeq \min \{  \argmin\limits_{\clusteridx' \in \{1,\ldots,\nrcluster\}} \| \vx^{(\sampleidx)} - \vm^{(\clusteridx')} \| \} \mbox{  (update cluster assignments)} 
\end{equation}
\State for all clusters $\clusteridx\!=\!1,\ldots,\nrcluster$, update activity indicator  $b^{(\clusteridx)} = \begin{cases} 1 & \mbox{ if } |\{i: y^{(\sampleidx)}= \clusteridx\}| > 0 \\ 0 & \mbox{ else.} \end{cases}$
\State for all $\clusteridx\!=\!1,\ldots,\nrcluster$ with $b^{(\clusteridx)}=1$, update cluster means 
\begin{equation}
\label{equ_cluster_mean_update2} 
\vm^{(\clusteridx)}= \frac{1}{|\{i: y^{(\sampleidx)}= \clusteridx\}|}  \sum_{i: y^{(\sampleidx)} = \clusteridx} \vx^{(\sampleidx)}  \mbox{  (update cluster means) } 
\end{equation} 
\State $r \defeq r +1$  (increment iteration counter)
\State $E^{(r)} = \emperror \big( \{\vm^{(\clusteridx)}\}_{\clusteridx=1}^{\nrcluster},\{y^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize} \mid \dataset \big)$  (see \eqref{equ_def_emp_risk_kmeans})
\Until $E^{(r-1)} - E^{(r)} > \varepsilon$
\Ensure cluster assignments $y^{(\sampleidx)}\!\in\!\{1,\ldots,\nrcluster\}$ and cluster means $\vm^{(\clusteridx)}$ %and active flags $b^{(c)}$ for $c\!=\!1,\ldots,k$
\end{algorithmic}
\end{algorithm}

The variables $b^{(c)} \in \{0,1\}$ indicate if cluster $c$ is active ($b^{(c)}= 1$) or cluster 
$c$ is inactive ($b^{(c)}=0$), in the sense of having no data points assigned to it during the preceding 
cluster assignment step \eqref{equ_cluster_assign_update2}. Using these additional variables 
allows to ensure the cluster mean update step \eqref{equ_cluster_mean_update2} to be well defined, 
since it is only applied to cluster $c$ associated with at least one data point $\vx^{(\sampleidx)}$. 

It can be shown that Algorithm \ref{alg:kmeansimpl} amounts to a fixed-point iteration 
\begin{equation}
\label{equ_fixed_point_clustering}
\{ y^{(\sampleidx)} \}_{i=1}^{\samplesize} \mapsto \mathcal{P}  \{ y^{(\sampleidx)} \}_{i=1}^{\samplesize}
\end{equation}
with a particular operator $\mathcal{P}$ (which depends on the dataset $\dataset$). Each iteration of 
Algorithm \ref{alg:kmeansimpl} updates the cluster assignments $y^{(\sampleidx)}$ by applying the 
operator $\mathcal{P}$. By interpreting Algorithm \ref{alg:kmeansimpl} as a fixed-point iteration 
\eqref{equ_fixed_point_clustering}, the authors of \cite[Thm. 2]{Gray1980} present an elegant 
proof of the convergence of Algorithm \ref{alg:kmeansimpl} within a finite number of iterations 
(even for $\varepsilon = 0$). What is more, after running Algorithm \ref{alg:kmeansimpl} for a finite 
number of iterations the cluster assignments $\{ y^{(\sampleidx)} \}_{i=1}^{\samplesize}$ do not change anymore.   

We illustrate the operation of Algorithm \ref{alg:kmeansimpl} in Figure \ref{fig:first_iter_kmeans}. Each column 
corresponds to one iteration of Algorithm \ref{alg:kmeansimpl}. The upper picture in each column depicts the 
update of cluster means while the lower picture shows the update of the cluster assignments during each iteration. 

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=10cm]{EvolKmeans.jpg}  
\vspace*{-6mm}
\end{center}
\caption{Evolution of cluster means and cluster assignments within $k$-means.}
\label{fig:first_iter_kmeans}
\end{figure}

While Algorithm \ref{alg:kmeansimpl} is guaranteed to terminate after a finite number of iterations, the delivered 
cluster assignments and cluster means might only be (approximations) of local minima of the clustering error 
\eqref{equ_def_emp_risk_kmeans} (see Figure \ref{fig_emp_risk_k_means}). In order to escape local minima, 
it is useful to run Algorithm \ref{alg:kmeansimpl} several times, using different initializations for the cluster means, 
and picking the cluster assignments $\{ y^{(\sampleidx)} \}_{\sampleidx=1}^{\samplesize}$ with smallest clustering 
error \eqref{equ_def_emp_risk_kmeans}. 

\begin{figure}[htbp]
\begin{center}
%\includegraphics[height=0.2\textheight]{images/ellbow.pdf}
    \begin{tikzpicture}
               \draw[  thick, domain=-1:2.5] plot (2*\x,  {\x^2-3*\x^3+\x+\x^4}) node [right] {$\emperror \big( \{\vm^{(c)}\}_{c=1}^{k},\{y^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize} \mid \dataset \big)$}; 
               \draw[  thick,fill=blue]  (-1.1,-0.45) circle (0.05cm) node[below] {local minimum}; 
     \end{tikzpicture}
\end{center}
\caption{The clustering error $\emperror \big( \{\vm^{(\clusteridx)}\}_{\clusteridx=1}^{\nrcluster},\{y^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize} \mid \dataset \big)$ (see \eqref{equ_def_emp_risk_kmeans}), 
which is minimized by $k$-means, is a non-convex function of the cluster means and assignments. 
It is therefore possible for $k$-means to get trapped around a local minimum.} \label{fig_emp_risk_k_means}
\end{figure}

Up till now, we have assumed the number $k$ of clusters to be given before hand. However, in some 
applications it is not clear what a good choice for $k$ can be. One approach to choosing the value of
$k$ is if the clustering method acts as a sub-module within an overall supervised ML system, which 
allows to implement some sort of validation. We could then try out different values of the number $k$ 
and determine validation errors for each choice. Then, we pick the choice of $k$ which results in the 
smallest validation error. 

Another approach to choosing $k$ is the so-called ``elbow-method''. This approach amounts to running 
$k$-means Algorithm \ref{alg:kmeansimpl} for different values of $k$ resulting in the (approximate) optimum empirical error
$\mathcal{E}^{(k)} = \emperror \big( \{\vm^{(c)}\}_{c=1}^{k},\{y^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize} \mid \dataset \big)$. 
%where ${\vm_{c}\}_{c=1}^{k},\{y^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize}$ 
%is the output obtained from for a particular choice of $k$. 
We then plot the minimum empirical error $\mathcal{E}^{(k)}$ as a function of the number 
$k$ of clusters. This plot typically looks like Figure \ref{fig_ellbow}, i.e., 
a steep decrease for small values of $k$ and then flattening out for larger values of $k$. 
Finally, the choice of $k$ might be guided by some probabilistic model which penalizes 
larger values of $k$. 


\begin{figure}
\begin{center}
%\includegraphics[height=0.2\textheight]{images/ellbow.pdf}
    \begin{tikzpicture}
          \begin{axis}
          [ylabel=$\mathcal{E}^{(k)}$,
              xlabel=$k$]
           %   \addplot[domain=-11:-5.1] {1/(x+5)};
             % \addplot[domain=-4.9:0] {-1/(x+5)};
              \addplot[domain=1:2, ultra thick] {10-x^3};
              \addplot[domain=2:10, ultra thick] {2+1/5-x/10};
         %     \addplot[mark=*,only marks] coordinates {(-3,4)(0,-3)(2,10)};
         %     \addplot[mark=*,fill=white,only marks] coordinates {(-3,-.5)
           %        (0,-.2)(0,2)(2,-2)};
          \end{axis}
     \end{tikzpicture}
\end{center}
\caption{The clustering error $\mathcal{E}^{(k)}$ achieved by $k$-means for increasing number $k$ of clusters.}
\label{fig_ellbow}
\end{figure}




\section{Soft Clustering}
\label{sec_soft_clustering}

The cluster assignments obtained from hard-clustering methods, such as Algorithm \ref{alg:kmeansimpl}, provide 
rather coarse-grained information. Indeed, even if two data points $\vx^{(\sampleidx)}, \vx^{(j)}$ are assigned to the 
same cluster $c$, their distances to the cluster mean $\vm^{(c)}$ might be very different. For some applications, we 
would like to have a more fine-grained information about the cluster assignments. 

%In particular, we are interested in the degree by which a data point belongs to a particular cluster. 
Soft-clustering methods provide such fine-grained information by explicitly modelling the degree (or confidence) by which 
a particular data point belongs to a particular cluster. More precisely, soft-clustering methods track for each data 
point $\vx^{(\sampleidx)}$ the degree of belonging to each of the clusters $\clusteridx \in \{1,\ldots,\nrcluster\}$.  

A principled approach to modelling a degree of belonging to different clusters uses a probabilistic (generative) model for 
the dataset $\dataset = \{ \vx^{(\sampleidx)} \}_{\sampleidx=1}^{\samplesize}$. Within this model, we represent % the cluster assignment $\clusteridx^{(\sampleidx)}$ 
%In what follows, we use a probabilistic model for the observed data points to motivate a particular soft-clustering method. 
%As mentioned above, soft clustering methods associate each data point $\vx^{(\sampleidx)}$ with a vector 
%$\vy^{(\sampleidx)} = \big(y_{1}^{(\sampleidx)},\ldots,y_{k}^{(\sampleidx)}\big)^{T}$. The entry $y_{c}^{(\sampleidx)} \in [0,1]$ represents 
%the degree of belonging (or our confidence) that data point $\vx^{(\sampleidx)}$ belongs to cluster $c$. 
%In order to develop a soft-clustering method, it is useful to represent 
eacc cluster by a probability distribution. 
One popular choice for this distribution is the multivariate normal distribution 
\begin{equation}
\label{equ_def_mvn_distribution}
 \mathcal{N} (\vx ; {\bm \mu}, {\bm \Sigma}) = \frac{1}{\sqrt{{\rm det } \{ 2 \pi  {\bm \Sigma} \} }} \exp\big( - (1/2) (\vx\!-\!{\bm \mu})^{T}  {\bm \Sigma}^{-1} (\vx\!-\!{\bm \mu})  \big)
\end{equation} 
of a Gaussian random vector with mean ${\bm \mu}$ and (invertible) covariance matrix ${\bf \Sigma}$.\footnote{Note that the distribution \eqref{equ_def_mvn_distribution} 
is only defined for an invertible (non-singular) covariance matrix ${\bm \Sigma}$.}
We maintain a separate distribution of the form \eqref{equ_def_mvn_distribution} for each of the $\nrcluster$ clusters. 
Thus, each cluster $\clusteridx \in \{1,\ldots,\nrcluster\}$ is represented by a distribution of the form \eqref{equ_def_mvn_distribution} 
with a cluster-specific mean ${\bm \mu}^{(\clusteridx)} \in \mathbb{R}^{\featuredim}$ and cluster-specific covariance matrix ${\bm \Sigma}^{(\clusteridx)} \in \mathbb{R}^{\featuredim \times \featuredim}$. 

Since we do not know the correct cluster $\clusteridx^{(\sampleidx)}$ of the data point $\vx^{(\sampleidx)}$, we model the 
cluster assignment $\clusteridx^{(\sampleidx)}$ as a random variable with probability distribution  
\begin{equation} 
\label{equ_def_cluster_prob}
p_{\clusteridx} \defeq \prob{c^{(\sampleidx)}=\clusteridx} \mbox{ for } \clusteridx=1,\ldots,\nrcluster.
\end{equation}  
Note that the (prior) probabilities $p_{c}$ are unknown and therefore have to be estimated somehow by the soft-clustering method. 
The random cluster assignment $\clusteridx^{(\sampleidx)}$ selects the cluster-specific distribution \eqref{equ_def_mvn_distribution} of the random 
data point $\vx^{(\sampleidx)}$: %Given a particular value for the cluster $\clusteridx \in \{1,\ldots,k\}$ is represented by a conditional probability distribution 
\begin{equation}
\label{equ_cond_dist_GMM}
 \prob{\vx^{(\sampleidx)} | \clusteridx^{(\sampleidx)} } = \mathcal{N} (\vx; {\bm \mu}^{(\clusteridx^{(\sampleidx)})}, {\bm \Sigma}^{(\clusteridx^{(\sampleidx)})})
\end{equation} 
with mean vector ${\bm \mu}^{(c)}$ and covariance matrix ${\bm \Sigma}^{(c)}$. 
%The probability distribution of the random vector depends on the (hidden) cluster index $c^{(\sampleidx)}\in \{1,\ldots,k\}$, 
%such that 
%with some (unknown!) cluster-specific mean vector ${\bm \mu}^{(c)}$ and covariance matrix ${\bm \Sigma}^{(c)}$ associated with cluster $c$. 
%Thus, we consider $\vx^{(\sampleidx)}$ drawn from some cluster, with index $c^{(\sampleidx)} \in \{1,\ldots,k\}$. Each cluster $c \in \{1,\ldots,k\}$ 
%corresponds to a multivariate Gaussian distribution with mean ${\bm \mu}^{(c^)}$ and covariance matrix ${\bm \Sigma}^{(c)}$. 

The modelling of cluster assignments $\clusteridx^{(\sampleidx)}$ as (unobserved) random variables lends naturally  
to a rigorous definition for the notion of the degree $y^{(\sampleidx)}_{c}$ by which data point $\vx^{(\sampleidx)}$ belongs to cluster $c$. 
In particular, we define the degree $y^{(\sampleidx)}_{c}$ of data point $\vx^{(\sampleidx)}$ belonging to cluster $c$ as the ``a-posteriori'' probability 
of the cluster assignment $\clusteridx^{(\sampleidx)}$ being equal to a particular cluster index $\clusteridx \in \{1,\ldots,\nrcluster\}$: 
\begin{equation}
\label{equ_def_deg_belonging_prob}
y^{(\sampleidx)}_{\clusteridx} \defeq \prob{ \clusteridx^{(\sampleidx)} = \clusteridx | \dataset}.
%y^{(\sampleidx)}_{\clusteridx} \defeq \prob{ \clusteridx^{(\sampleidx)}= \clusterfidx | \dataset}
\end{equation} 
By their very definition \eqref{equ_def_deg_belonging_prob}, the degrees of belonging $y^{(\sampleidx)}_{\clusteridx}$ have to sum 
to one: 
\begin{equation} 
\label{equ_dob_sum_to_one}
\sum_{\clusteridx=1}^{\nrcluster} y^{(\sampleidx)}_{\clusteridx}=1 \mbox{ for each } \sampleidx=1,\ldots,\samplesize.
\end{equation}  
It is important to note that we use the conditional cluster probability \eqref{equ_def_deg_belonging_prob}, conditioned on the 
dataset, for defining the degree of belonging $y^{(\sampleidx)}_{\clusteridx}$. This is reasonable since the degree of belonging 
$y^{(\sampleidx)}_{\clusteridx}$ depends on the overall (cluster) geometry of the data set $\dataset$.

A probabilistic model for the observed data points $\vx^{(\sampleidx)}$ is obtained 
by considering each data point $\vx^{(\sampleidx)}$ being the result of a random 
draw from the distribution $\mathcal{N} (\vx ; {\bm \mu}^{(c^{(\sampleidx)})}, {\bm \Sigma}^{(c^{(\sampleidx)})})$ 
with some cluster $c^{(\sampleidx)}$. Since the cluster indices $c^{(\sampleidx)}$ are 
unknown,\footnote{After all, the goal of soft-clustering is to estimate the 
cluster indices $c^{(\sampleidx)}$.} we model them as random variables. 
In particular, we model the cluster indices $c^{(\sampleidx)}$ as i.i.d. with probabilities
$p_{\clusteridx} = \prob{ c^{(\sampleidx)} = \clusteridx}.$ 

The overall probabilistic model \eqref{equ_cond_dist_GMM}, \eqref{equ_def_cluster_prob} 
amounts to a {\bf Gaussian mixture model} (GMM). Indeed, using the law of total probability, 
the marginal distribution $\prob{\vx^{(\sampleidx)}}$ (which is the same for all data points 
$\vx^{(\sampleidx)}$) is a (additive) mixture of multivariate Gaussian distributions: 
\begin{equation} 
\label{equ_def_GMM}
 \prob{\vx^{(\sampleidx)}} = \sum_{c=1}^{k} \underbrace{\prob{c^{(\sampleidx)}=c}}_{p_{c}}  \underbrace{\prob{\vx^{(\sampleidx)} | c^{(\sampleidx)}=c}}_{\mathcal{N}(\vx^{(\sampleidx)};{\bm \mu}^{(c)}, {\bm \Sigma}^{(c)})}. 
\end{equation} 
Within our statistical model for the dataset $\dataset$, the cluster assignments $c^{(\sampleidx)}$ are 
hidden (unobserved) random variables. We thus have to infer or estimate these variables from the 
observed data points $\mathbf{x}^{(\sampleidx)}$ which are i.i.d.\ realizations of the GMM \eqref{equ_def_GMM}. 

\begin{figure}
\begin{center}
\begin{tikzpicture}[scale=0.4]
\draw [thick] \boundellipse{0,0}{10}{5} node[right]  {${\bm \mu^{(1)}}$};
 \fill (0,0) circle (2pt) ; 
  \node [right] at (0,5.5) {$ {\bm \Sigma}^{(1)}$} ; 
\draw [thick] \boundellipse{11,1}{-2}{4} node[right]  {${\bm \mu^{(2)}}$};
 \fill (11,1) circle (2pt) ; 
   \node [right] at (11,5.5) {$ {\bm \Sigma}^{(2)}$} ; 
\draw [thick] \boundellipse{-9,4}{2}{3} node[left,xshift=3mm,yshift=3mm]  {$\,\,{\bm \mu^{(3)}}$}; 
 \fill (-9,4) circle (2pt) ; 
 \node [right] at (-9,8) {$ {\bm \Sigma}^{(3)}$} ; 
\end{tikzpicture}
\end{center}
\label{fig_GMM_elippses}
\caption{The GMM \eqref{equ_def_GMM} yields a probability density function which 
is a weighted sum of multivariate normal distributions $\mathcal{N}({\bm \mu}^{(c)}, {\bm \Sigma}^{(c)})$. 
The weight of the $c$-th component is the cluster probability $ \prob{c^{(\sampleidx)}=c}$.}
\end{figure}
%\end{document}

Using the GMM \eqref{equ_def_GMM} for explaining the observed data points $\vx^{(\sampleidx)}$ 
turns the clustering problem into a {\bf statistical inference} or {\bf parameter estimation problem} \cite{kay,LC}. 
The estimation problem is to estimate the true underlying cluster probabilities $p_{\clusteridx}$ 
(see \eqref{equ_def_cluster_prob}), cluster means ${\bm \mu}^{(\clusteridx)}$ and cluster 
covariance matrices ${\bm \Sigma}^{(\clusteridx)}$ (see \eqref{equ_cond_dist_GMM}) from 
the observed data points $\dataset = \{ \vx^{(\sampleidx)} \}_{\sampleidx=1}^{\samplesize}$, 
which are drawn from the probability distribution \eqref{equ_def_GMM}. 
%Having estimates for the GMM parameters allows us to compute an estimate (approximation) of the ``a-posteriori'' probability 
%$\prob\{ c^{(\sampleidx)}= c | \dataset \}$, which we consider as the ``true'' degree of data point $\vx^{(\sampleidx)}$ belonging to cluster $c$. 

We denote the estimates for the GMM parameters by $\hat{p}_{\clusteridx} (\approx p_{\clusteridx})$, $\vm^{(\clusteridx)} (\approx {\bm \mu}^{(\clusteridx)})$ and 
$\mathbf{C}^{(\clusteridx)} (\approx {\bm \Sigma}^{(\clusteridx)})$, respectively. 
Based on these estimates, we can then compute an estimate $\hat{y}_{\clusteridx}^{(\sampleidx)}$ 
of the (a-posterior) probability 
\begin{equation} 
y_{\clusteridx}^{(\sampleidx)} = \prob{c^{(\sampleidx)} = \clusteridx \mid \dataset }
\end{equation} 
of the $\sampleidx$-th data point $\vx^{(\sampleidx)}$ belonging to cluster $\clusteridx$, given the observed dataset $\dataset$. 

It turns out that this estimation problem becomes significantly easier by operating in an alternating 
fashion: In each iteration, we first compute a new estimate $\hat{p}_{c}$ of the cluster probabilities 
$p_{\clusteridx}$, given the current estimate $\vm^{(\clusteridx)}, \mathbf{C}^{(\clusteridx)}$ for the 
cluster means and covariance matrices. Then, using this new estimate $\hat{p}_{\clusteridx}$ for the 
cluster probabilities, we update the estimates $\vm^{(\clusteridx)}, \mathbf{C}^{(\clusteridx)}$ of the 
cluster means and covariance matrices. Then, using the new estimates $\vm^{(\clusteridx)}, \mathbf{C}^{(\clusteridx)}$, 
we compute a new estimate $\hat{p}_{\clusteridx}$ and so on. By repeating these two update steps, 
we obtain an iterative soft-clustering method which is summarized in Algorithm \ref{alg:softclustering}. 

%Let us, for ease of exposition, restrict to the case of $k\!=\!2$ clusters $\clusteridx\!\in\!\{1,2\}$. The generalization to $k\!>\!2$ clusters is easy. 
%For $k=2$, we only need to track the degree of belonging $y_{1}^{(\sampleidx)}$ to cluster $\clusteridx=1$ since $y_{2}^{(\sampleidx)}\!=\!1\!-\!y_{1}^{(\sampleidx)}$ (see \eqref{equ_dob_sum_to_one}). 
%The estimates of the probabilities can be written as $\hat{p}_{1} = \samplesize_{1}/\samplesize$ and $\hat{p}_{2} = \samplesize_{2}/\samplesize$ 
%with $\samplesize_{1} = \sum_{\sampleidx=1}^{\samplesize} y^{(\sampleidx)}_{1}$ and $\samplesize_{1} = \sum_{\sampleidx=1}^{\samplesize} y^{(\sampleidx)}_{2}$. 
%The quantities $\samplesize_{1}, \samplesize_{2}$ can be interpreted as {\bf effective cluster sizes}. 
\begin{algorithm}[htbp]
\caption{``A Soft-Clustering Algorithm'' \cite{BishopBook}}\label{alg:softclustering}

\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\Require   dataset $\dataset=\{ \vx^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize}$; number $k$ of clusters. 
\Statex\hspace{-6mm}{\bf Initialize:} use initial guess for GMM parameters $\{\mathbf{m}^{(\clusteridx)},\mathbf{C}^{(\clusteridx)},\hat{p}_{\clusteridx}\}_{\clusteridx=1}^{\nrcluster}$ 
\Repeat
\vspace*{2mm}
\State for each data point $\vx^{(\sampleidx)}$ and cluster $\clusteridx \in \{1,\ldots,\nrcluster\}$, update degrees of belonging
\vspace*{-1mm}
\begin{align}
\label{equ_update_soft_cluster_assignm}
y_{c}^{(\sampleidx)} & =  \frac{\hat{p}_{\clusteridx} \mathcal{N}(\vx^{(\sampleidx)};\mathbf{m}^{(\clusteridx)},\mathbf{C}^{(\clusteridx)})}{\sum_{\clusteridx'=1}^{\nrcluster} \hat{p}_{\clusteridx'}\mathcal{N}(\vx^{(\sampleidx)};\mathbf{m}^{(\clusteridx')},\mathbf{C}^{(\clusteridx')})} %\nonumber \\
%y_{2}^{(\sampleidx)} & = 1- y_{1}^{(\sampleidx)} 
\end{align}
\State for each cluster $\clusteridx \in \{1,\ldots,\nrcluster\}$, update estimates of GMM parameters: 
\begin{itemize} 
\item cluster probability $\hat{p}_{\clusteridx}\!=\!\samplesize_{\clusteridx}/\samplesize$, with effective cluster size $\samplesize_{\clusteridx}\!=\!\sum\limits_{\sampleidx=1}^{\samplesize} y_{\clusteridx}^{(\sampleidx)}$
\item cluster mean $\mathbf{m}^{(\clusteridx)} = (1/\samplesize_{\clusteridx}) \sum\limits_{\sampleidx=1}^{\samplesize} y_{\clusteridx}^{(\sampleidx)} \vx^{(\sampleidx)}$ 
\item cluster covariance matrix $\mathbf{C}^{(\clusteridx)}  = (1/\samplesize_{\clusteridx}) {\sum\limits_{\sampleidx=1}^{\samplesize} y_{\clusteridx}^{(\sampleidx)} \big(\vx^{(\sampleidx)}\!-\!\mathbf{m}^{(\clusteridx)}\big)   \big(\vx^{(\sampleidx)}\!-\!\mathbf{m}^{(\clusteridx)}\big)^{T} }$
\end{itemize}
\vspace*{1mm}
\Until convergence
\Ensure soft cluster assignments $\vy^{(\sampleidx)}=(y_{1}^{(\sampleidx)},\ldots,y_{k}^{(\sampleidx)})^{T}$ for each data point $\vx^{(\sampleidx)}$ 
\end{algorithmic}
\end{algorithm}

As for hard clustering, we can interpret the soft clustering problem as an instance 
of the ERM principle (Chapter \ref{ch_Optimization}). In particular, Algorithm \ref{alg:softclustering} 
aims at minimizing the empirical risk 
\begin{equation} 
\label{equ_def_emp_risk_soft_clustering}
\emperror \big( \{ \vm^{(\clusteridx)}, \mathbf{C}^{(\clusteridx)}, \hat{p}_{\clusteridx} \}_{\clusteridx=1}^{\nrcluster} \mid \dataset \big)=
 - \log {\rm Prob} \big\{\dataset; \{ \vm^{(\clusteridx)}, \mathbf{C}^{(\clusteridx)}, \hat{p}_{\clusteridx} \}_{\clusteridx=1}^{\nrcluster}\big\}. 
\end{equation} 
The interpretation of Algorithm \ref{alg:softclustering} as a method for minimizing the empirical 
risk \eqref{equ_def_emp_risk_soft_clustering} suggests to monitor the decrease of the empirical 
risk $ - \log {\rm Prob} \big\{\dataset; \{ \vm^{(\clusteridx)}, \mathbf{C}^{(\clusteridx)}, \hat{p}_{\clusteridx} \}_{\clusteridx=1}^{\nrcluster}\big\}$ 
to decide when to stop iterating (``convergence test''). % the termination criterion. % Algorithm \ref{alg:softclustering}. 

Similar to $k$-means Algorithm \ref{alg:kmeans}, also the soft clustering Algorithm \ref{alg:softclustering} 
suffers from the problem of getting stuck in local minima of the empirical risk \eqref{equ_def_emp_risk_soft_clustering}. 
As for $k$-means, a simple strategy to overcome this issue is to run Algorithm \ref{alg:softclustering} 
several times, each time with a different initialization for the GMM parameter estimates 
$ \{ \vm^{(c)}, \mathbf{C}^{(c)}, \hat{p}_{c} \}_{c=1}^{k}$ and then picking the result which yields the 
smallest empirical risk \eqref{equ_def_emp_risk_soft_clustering}.  

We note that the empirical risk \eqref{equ_def_emp_risk_soft_clustering} underlying the 
soft-clustering Algorithm \ref{alg:softclustering} is essentially a {\bf log-likelihood function}. 
Thus, Algorithm \ref{alg:softclustering} can be interpreted as an {\bf approximate maximum 
likelihood} estimator for the true underlying GMM parameters $\{{\bm \mu}^{(c)},{\bm \Sigma}^{(c)},p_{c}\}_{c=1}^{k}$. 
In particular, Algorithm \ref{alg:softclustering} is an instance of a generic approximate maximum likelihood 
technique referred to as {\bf expectation maximization} (EM) (see \cite[Chap. 8.5]{hastie01statisticallearning} for more details). 
The interpretation of Algorithm \ref{alg:softclustering} as a special case of EM allows to characterize the 
behaviour of Algorithm \ref{alg:softclustering} using existing convergence results for EM methods \cite{XuJordan1996}. 
%???? Discuss EM principle in more detail; point out relation to recent work of Arora on Factorization methods such as Dictionary Learning, proving global convergence 
%New Algorithms for Learning Incoherent and Overcomplete Dictionaries, Sanjeev Arora, Rong Ge, Ankur Moitra
%https://arxiv.org/pdf/1510.06096.pdf
%
%Arora, Sanjeev, Rong Ge, Ankur Moitra, and Sushant Sachdeva.
%?Provable ICA with Unknown Gaussian Noise, and Implications
%for Gaussian Mixtures and Autoencoders.? Algorithmica 72, no. 1
%(March 4, 2015): 215?236.
%??????

We finally note that $k$-means hard clustering can be interpreted as an extreme case of 
soft-clustering Algorithm \ref{alg:softclustering}. Indeed, consider fixing the cluster 
covariance matrices $ {\bm \Sigma}^{(c)}$ within the GMM \eqref{equ_cond_dist_GMM} 
to be the scaled identity: 
\begin{equation}
\label{equ_def_special_case}
 {\bm \Sigma}^{(c)}= \sigma^{2} \mathbf{I} \mbox{ for all } c \in \{1,\ldots,k\}.  
\end{equation} 
Here, we assume the covariance matrix \eqref{equ_def_special_case}, with a particular value 
for $\sigma^{2}$, to be the actual ``correct'' covariance matrix for cluster $c$. The estimates 
$\mathbf{C}^{(c)}$ for the covariance matrices are then trivially given by $\mathbf{C}^{(c)} =  {\bm \Sigma}^{(c)}$, 
i.e., we can omit the covariance matrix updates in Algorithm \ref{alg:softclustering}. Moreover, when 
choosing a very small variance $\sigma^{2}$ in \eqref{equ_def_special_case}), the update 
\eqref{equ_update_soft_cluster_assignm} tends to enforce $y_{c}^{(\sampleidx)} \in \{0,1\}$, i.e., 
each data point $\vx^{(\sampleidx)}$ is associated to exactly one cluster $c$, whose cluster mean 
$\vm^{(c)}$ is closest to the data point $\vx^{(\sampleidx)}$. Thus, for $\sigma^{2} \rightarrow 0$, 
the soft-clustering update \eqref{equ_update_soft_cluster_assignm} reduces to the hard cluster 
assignment update \eqref{equ_cluster_assign_update} of the $k$-means Algorithm \ref{alg:kmeans}. 

\newpage
\chapter{Feature Learning} 
\label{ch_FeatureLearning}

\begin{quote}
``Solving Problems By Changing the Viewpoint.''
\end{quote}

\begin{figure}[htbp]
\begin{center}
\includegraphics[width=0.7\textwidth]{DimRed.jpg}  
\end{center}
\caption{Dimensionality reduction methods aim at finding a map $h$ which maximally 
	compresses the raw data while still allowing to accurately reconstruct the original 
	data point from a small number of features $x_{1},\ldots,x_{\featuredim}$.}
\label{fig_dimred}
\end{figure}

%????? Discuss relation to Information Bottleneck and universal source coding ??????

%????? Discuss a simple method for dim. red. for binary data ????

Roughly speaking, ML methods exploit the intrinsic geometry of (large) sets of 
data points to compute predictions. By definition, we represent these data points 
as elements of the feature space $\featurespace$. Note that the features are 
a design choice so we can shape the intrinsic geometry of the data points by 
using different choices for the features (and feature space). Feature learning 
methods automate the choice of finding a good feature space for a given 
data set. A subclass of feature learning methods are dimensionality reduction methods, 
where the new feature space has a (much) smaller dimension than the original 
feature space (see Section \ref{sec_dim_red}). However, sometimes it might be 
useful to change to a higher-dimensional feature space (see Section \ref{sec_dim_increas}). 


??? Develop feature learning as an approximation problem. The raw data is the vector to be approximated. 
The approximation has to be in a (small) subspace which is spanned by all possible low-dimensional feature vectors???

%Supervised ML methods often 
%rely on the principle that near-by data points should have similar label values. 
%There is a great deal of ways to define this notion of ``near-by''.  
\section{Dimensionality Reduction} 
\label{sec_dim_red}

Consider a ML method that aims at predicting the label $y$  of a data point $\vz$ 
based on some features $\vx$ which characterize the data point $\vz$. Intuitively, it 
should be beneficial to use as many features as possible. Indeed, the more features 
of a data point we know, the more we should know about its label $y$. 
%since having more information 
%about a data point can only help us in the task of predicting the label $y$. 

There are, however, two pitfalls in using an unnecessarily large number of features. 
The first one is a {\bf computational pitfall} and the second one is a {\bf statistical pitfall}. 
The larger the feature vector $\vx \in \mathbb{R}^{\featuredim}$ (with large $\featuredim$), 
the more computation (and storage) is required for executing the resulting ML method. 
Moreover, using a large number of features makes the resulting ML methods more prone to 
overfitting. Indeed, linear regression will overfit when using feature vectors $\vx\!\in\!\mathbb{R}^{\featuredim}$ 
whose length $\featuredim$ exceeds the number $\samplesize$ of labeled data points used 
for training (see Chapter \ref{ch_overfitting_regularization}). 

Thus, both from a computational and statistical perspective, it is beneficial to use 
only the maximum necessary amount of relevant features. A key challenge here is 
to select those features which carry most of the relevant information required for 
the prediction of the label $y$. Beside coping with overfitting and limited computational 
resources, dimensionality reduction can also be useful for data visualization. Indeed, if 
the resulting feature vector has length $d=2$, we can use scatter plots to depict datasets. 

The basic idea behind most dimensionality reduction methods is quite simple. As 
illustrated in Figure \ref{fig_dimred}, these methods aim at learning (finding) a ``compression'' 
map that transforms a raw data point $\vz$ to a (short) feature vector $\vx=(x_{1},\ldots,x_{\featuredim})^{T}$ 
in such a way that it is possible to find (learn) a ``reconstruction'' map which allows to accurately 
reconstruct the original data point from the features $\vx$. The compression and reconstruction 
map is typically constrained to belong some set of computationally feasible maps or hypothesis 
space (see Chapter \ref{ch_some_examples} for different examples of hypothesis spaces). In what 
follows we restrict ourselves to using only linear maps for compression and reconstruction leading 
to principal component analysis. The extension to non-linear maps using deep neural networks 
is known as {\bf deep autoencoders} \cite[Ch. 14]{Goodfellow-et-al-2016}. 


\section{Principal Component Analysis} 
\label{sec_pca}

%We will now discuss some basic but powerful methods for efficiently constructing relevant features $\vx$ for a data point $\vz$. 
Consider a data point $\vz \in \mathbb{R}^{D}$ which is represented by a (typically very long) 
vector of length $D$. The length $D$ of the raw feature vector might be easily on the order of 
millions. To obtain a small set of relevant features $\vx \in \mathbb{R}^{\featuredim}$, we apply 
a linear transformation to the data point: 
\begin{equation} 
\label{equ_feat_learning_matrix}
\vx = \mathbf{W} \vz.
\end{equation}
Here, the ``compression'' matrix $\mathbf{W} \in \mathbb{R}^{\featuredim \times D}$ maps 
(in a linear fashion) the large vector $\vz \in \mathbb{R}^{D}$ to a smaller feature vector $\vx \in \mathbb{R}^{\featuredim}$. %In what follows, we consider particular choice for the matrix $\mathbf{W}$. 


It is reasonable to choose the compression matrix $\mathbf{W} \in \mathbb{R}^{\featuredim \times D}$ 
in \eqref{equ_feat_learning_matrix} such that the resulting features $\vx \in \mathbb{R}^{\featuredim}$ 
allow to approximate the original data point $\vz \in \mathbb{R}^{D}$ as accurate as possible. We can 
approximate (or recover) the data point $\vz \in \mathbb{R}^{D}$ back from the features $\vx$ by 
applying a reconstruction operator $\mathbf{R} \in \mathbb{R}^{D \times \featuredim}$, which is 
chosen such that 
\begin{equation} 
\label{equ_approx_linear_PCA}
\vz \approx \mathbf{R} \vx \stackrel{\eqref{equ_feat_learning_matrix}}{=} \mathbf{R} \mathbf{W} \vz. 
\end{equation} 

The approximation error $\emperror \big( \mathbf{W},\mathbf{R} \mid \dataset\big)$ resulting 
when \eqref{equ_approx_linear_PCA} is applied to each data point in a 
dataset $\dataset=  \{ \vz^{(\sampleidx)}\}_{\sampleidx=1}^{\samplesize}$ is then
\begin{equation} 
\label{equ_app_error_PCA}
\emperror \big( \mathbf{W},\mathbf{R} \mid \dataset\big) = (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \| \vz^{(\sampleidx)} - \mathbf{R} \mathbf{W} \vz^{(\sampleidx)} \|. 
\end{equation} 

One can verify that the approximation error $\emperror \big( \mathbf{W},\mathbf{R} \mid \dataset\big)$ 
can only by minimal if the compression matrix $\mathbf{W}$ is of the form
\begin{equation}
\label{equ_def_PCA_W}
\mathbf{W} = \mathbf{W}_{\rm PCA} \defeq \big( \mathbf{u}^{(1)},\ldots, \mathbf{u}^{(\featuredim)} \big)^{T} \in \mathbb{R}^{\featuredim \times D}, 
\end{equation} 
with $\featuredim$ orthonormal vectors $\mathbf{u}^{(l)}$ which correspond to the 
$\featuredim$ largest eigenvalues of the {\bf sample covariance matrix} 
\begin{equation} 
\label{equ_def_Q_PCA}
\mathbf{Q} \defeq (1/\samplesize) \mathbf{Z}^{T} \mathbf{Z} \in \mathbb{R}^{D \times D} 
\end{equation}
with data matrix $\mathbf{Z}\!=\!\big( \vz^{(1)}, \ldots, \vz^{(\samplesize)} \big)^{T}\!\in\!\mathbb{R}^{\samplesize \times D}$.
\footnote{Some authors define the data matrix as $\mZ\!=\!\big( \widetilde{\vz}^{(1)}, \ldots, \widetilde{\vz}^{(\samplesize)} \big)^{T}\!\in\!\mathbb{R}^{\samplesize \times D}$  using ``centered'' data points $\widetilde{\vz}^{(\sampleidx)} - \widehat{\vm}$ 
	obtained by subtracting the average $\widehat{\vm} = (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \vz^{(\sampleidx)}$.}
By its very definition \eqref{equ_def_Q_PCA}, the matrix $\mathbf{Q}$ is positive semi-definite so that it allows 
for an eigenvalue decomposition (EVD) of the form \cite{Strang2007}
\vspace*{-2mm}
\begin{equation}
\label{equ_EVD_Q_PCA}
 \mQ=\big(\eigvecCov^{(1)},\ldots,\eigvecCov^{(D)}\big)  \begin{pmatrix}\lambda^{(1)} &  \ldots & 0 \\ 0 & \ddots & 0  \\ 0 & \ldots & \lambda^{(D)} 
 \end{pmatrix} \big(\eigvecCov^{(1)},\ldots,\eigvecCov^{(D)}\big)^{T} \nonumber
\end{equation} 
with real-valued eigenvalues $\lambda^{(1)} \geq \lambda^{(2)}\geq \ldots \geq \lambda^{(D)} \geq 0$ and orthonormal 
eigenvectors $\{ \eigvecCov_{r} \}_{r=1}^{D}$. 

The features $\vx^{(\sampleidx)}$, obtained by applying the compression matrix $\mathbf{W}_{\rm PCA}$ 
\eqref{equ_def_PCA_W} to the raw data points $\vz^{(\sampleidx)}$, are referred to as {\bf principal components (PC)}. 
The overall procedure of determining the compression matrix \eqref{equ_def_PCA_W} and, in turn, computing 
the PC vectors $\vx^{(\sampleidx)}$ is known as {\bf principal component analysis (PCA)} and summarized 
in Algorithm \ref{alg_PCA}. 
\begin{algorithm}[htbp]
\caption{Principal Component Analysis (PCA)}\label{alg_PCA}
\begin{algorithmic}[1]
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\Require  dataset  $\dataset=\{ \vz^{(\sampleidx)} \in \mathbb{R}^{D} \}_{\sampleidx=1}^{\samplesize}$; number $\featuredim$ of PCs. 
%\Statex\hspace{-6mm}{\bf Initialize:} choose initial cluster means $\vm_{c}$ for $c=1,\ldots,k$.    
%\Repeat
%\State for all $i\!=\!1,\ldots,\samplesize$, do 
\State compute EVD \eqref{equ_EVD_Q_PCA} to obtain orthonormal eigenvectors $\big(\eigvecCov^{(1)},\ldots,\eigvecCov^{(D)}\big)$ 
corresponding to (decreasingly ordered) eigenvalues $\lambda^{(1)} \geq \lambda^{(2)}\geq \ldots \geq \lambda^{(D)} \geq 0$
\vspace*{2mm}
\State construct compression matrix $\mW_{\rm PCA}  \defeq \big( \mathbf{u}^{(1)},\ldots, \mathbf{u}^{(\featuredim)} \big)^{T} \in \mathbb{R}^{\featuredim \times D}$ %  $\vx^{(\sampleidx)} = \mW_{\rm PCA} \vz^{(\sampleidx)}$ with compression matrix \eqref{equ_def_PCA_W}
\vspace*{2mm}
\State compute feature vector $\vx^{(\sampleidx)} = \mW_{\rm PCA} \vz^{(\sampleidx)}$ whose entries are PC of $\vz^{(\sampleidx)}$ %with compression matrix \eqref{equ_def_PCA_W}
%\Until convergence 
\vspace*{2mm}
\State compute approximation error $\emperror^{(\rm PCA)} = \sum_{r = \featuredim+1}^{D} \lambda^{(r)}$ (see \eqref{equ_approx_error_PCA}). 
\vspace*{2mm}
\Ensure $\vx^{(\sampleidx)}$, for $\sampleidx=1,\ldots,\samplesize$, and the approximation error $\emperror^{(\rm PCA)}$. 
\end{algorithmic}
\end{algorithm}
Note that the length $n$ of the feature vectors $\vx$, which is also the number of PCs used, is an 
input parameter of Algorithm \ref{alg_PCA}. The number $n$ can be chosen between $n=0$ and $n=D$. 
However, it can be shown that PCA for $n>m$ is not well-defined. In particular, the orthonormal eigenvectors 
$ \mathbf{u}^{(\featuredim+1)},\ldots, \mathbf{u}^{(D)}$ are not unique. 


From a computational perspective, Algorithm \ref{alg_PCA} essentially amounts to performing an EVD of the sample covariance 
matrix $\mathbf{Q}$ (see \eqref{equ_def_Q_PCA}). Indeed, the EVD of $\mQ$ provides not only the optimal compression matrix 
$\mathbf{W}_{\rm PCA}$ but also the measure $\emperror^{(\rm PCA)}$ for the information loss incurred by replacing the original 
data points $\vz^{(\sampleidx)} \in \mathbb{R}^{D}$ with the smaller feature vector $\vx^{(\sampleidx)} \in \mathbb{R}^{\featuredim}$. 
In particular, this information loss is measured by the approximation error (obtained for the optimal reconstruction matrix 
$\mathbf{R}_{\rm opt} = \mathbf{W}_{\rm PCA}^{T}$)
%$\emperror \big( \mathbf{W}_{\rm PCA},\mathbf{R}\mid \dataset\big)$ \eqref{equ_app_error_PCA}) which is given as 
\begin{equation} 
\label{equ_approx_error_PCA}
\emperror^{(\rm PCA)} \defeq \emperror \big( \mathbf{W}_{\rm PCA},\underbrace{\mathbf{R}_{\rm opt}}_{=\mathbf{W}_{\rm PCA}^{T}}\mid \dataset\big) = \sum_{r = \featuredim+1}^{D} \lambda^{(r)}. 
\end{equation} 
As depicted in Figure \ref{fig_ellbow_PCA}, the approximation error  $\emperror^{(\rm PCA)}$ decreases with increasing 
number $\featuredim$ of PCs used for the new features \eqref{equ_feat_learning_matrix}. The maximum error 
$\emperror^{(\rm PCA)} = (1/\samplesize) \sum_{\sampleidx=1}^{\samplesize} \| \vz^{(\sampleidx)} \|^{2}$ is obtained for 
$\featuredim\!=\!0$, which amounts to completely ignoring the data points $\vz^{(\sampleidx)}$. In the other extreme case 
where $\featuredim\!=\!D$ and $\vx^{(\sampleidx)}\!=\!\vz^{(\sampleidx)}$, which amounts to no compression at all, the 
approximation error is zero $\emperror^{(\rm PCA)}\!=\!0$. 

\begin{figure}[htbp]
\begin{center}
%\includegraphics[height=0.2\textheight]{images/ellbow.pdf}
    \begin{tikzpicture}
          \begin{axis}
          [ylabel=$\emperror^{(\rm PCA)}$,
              xlabel=$\featuredim$]
           %   \addplot[domain=-11:-5.1] {1/(x+5)};
             % \addplot[domain=-4.9:0] {-1/(x+5)};
              \addplot[domain=1:2, ultra thick] {10-x^3};
              \addplot[domain=2:10, ultra thick] {2+1/5-x/10};
         %     \addplot[mark=*,only marks] coordinates {(-3,4)(0,-3)(2,10)};
         %     \addplot[mark=*,fill=white,only marks] coordinates {(-3,-.5)
           %        (0,-.2)(0,2)(2,-2)};
          \end{axis}
     \end{tikzpicture}
\end{center}
\caption{Reconstruction error $\emperror^{(\rm PCA)}$ (see \eqref{equ_approx_error_PCA}) of PCA for varying number $\featuredim$ of PCs.}
\label{fig_ellbow_PCA}
\end{figure}

\subsection{Combining PCA with Linear Regression} 

One important use case of PCA is as a pre-processing step within an overall ML problem 
such as linear regression (see Section \ref{sec_lin_regression}). As discussed in Chapter 
\ref{ch_overfitting_regularization}, linear regression methods are prone to overfitting 
whenever the data points are characterized by feature vectors whose length $D$ exceeds 
the number $\samplesize$ of labeled data points used for training. One simple but powerful 
strategy to avoid overfitting is to preprocess the original feature vectors (they are considered 
as the raw data points $\vz^{(\sampleidx)}\in \mathbb{R}^{D}$) by applying PCA in order to obtain 
smaller feature vectors $\vx^{(\sampleidx)} \in \mathbb{R}^{\featuredim}$ with $\featuredim < \samplesize$. 

%We will discuss how using PCA avoids overfitting for settings where the raw data points $\vz^{(\sampleidx)}$ are high-dimensional, i.e., 
%$\vz^{(\sampleidx)} \in \mathbb{R}^{D}$ with $D$ very large. %????DISCUSS REGRESSION PROBLEM USING WEBCAM SNAPSHOTS?????


\subsection{How To Choose Number of PC?} 
There are several aspects which can guide the choice for the number $\featuredim$ of PCs to be used as features. 
\begin{itemize}
\item for data visualization: use either $\featuredim=2$ or $\featuredim=3$ 
\item computational budget: choose $\featuredim$ sufficiently small such that computational complexity of overall 
ML method fits the available computational resources.  
\item statistical budget: consider using PCA as a pre-processing step within a linear 
regression problem (see Section \ref{sec_lin_regression}). Thus, we use the output 
$\vx^{(\sampleidx)}$ of PCA as the feature vectors in linear regression. In order to avoid 
overfitting, we should choose $\featuredim < \samplesize$ (see Chapter \ref{ch_overfitting_regularization}). 
\item elbow method: choose $\featuredim$ large enough such that approximation 
error $\emperror^{(\rm PCA)}$ is reasonably small (see Figure \ref{fig_ellbow_PCA}). 
\end{itemize} 



\subsection{Data Visualisation}

If we use PCA with $\featuredim=2$ PC, we obtain feature vectors 
$\vx^{(\sampleidx)} = \mathbf{W} \vz^{(\sampleidx)}$ (see \eqref{equ_feat_learning_matrix}) 
which can be depicted as points in a scatter plot (see Section \ref{equ_subsection_scatterplot}). 
As an example we consider data points $\vz^{(\sampleidx)}$ obtained  from historic recordings of 
Bitcoin statistics. Each data point $\vz^{(\sampleidx)} \in \mathbb{R}^{6}$ 
is a vector of length $D=6$. It is difficult to visualise points in an Euclidean space $\mathbb{R}^{D}$ of dimension $D > 2$. 
It is then helpful to apply PCA with $\featuredim=2$ which results in feature vectors $\vx^{(\sampleidx)} \in \mathbb{R}^{2}$ 
which can be depicted conveniently as a scatter plot (see Figure \ref{fig_scatterplot_visualization}). 

\begin{figure}[htbp]
\begin{center}
\begin{tikzpicture}
    \begin{axis}[
            axis x line=middle,
            axis y line=middle,
            xmax=5000,
            enlarge y limits=true,
            enlarge x limits=true,
            width=10cm, height=8cm,    
            grid = major,
            grid style={dashed, gray!30},
            ylabel=first PC $x_{1}$,
            xlabel=second PC $x_{2}$,
         ]        
          \addplot[only marks,mark=+] table [x=pc1, y=pc2, col sep = comma] {twopc.csv};
    \end{axis}
\end{tikzpicture}
\end{center}
\caption{A scatter plot of feature vectors $\vx^{(\sampleidx)} = \big(x_{1}^{(\sampleidx)},x_{2}^{(\sampleidx)}\big)^{T}$ 
	whose entries are the first two PCs of the Bitcoin statistics $\vz^{(\sampleidx)}$ of the $\sampleidx$-th day.} 
\label{fig_scatterplot_visualization}
\end{figure}



\subsection{Extensions of PCA}
There have been proposed several extensions of the basic PCA method: 
\begin{itemize}
 \item {\bf kernel PCA \cite[Ch.14.5.4]{hastie01statisticallearning}:} combines PCA with a non-linear feature map (see Section \ref{sec_kernel_methods}). % {\bf non-linear} subspaces by combining PCA with feature map obtained from a kernel function. 
 \vspace*{2mm}
 \item {\bf robust PCA \cite{RobustPCA}:} modifies PCA to better cope with {\bf outliers} in the dataset.
 \vspace*{2mm}
 \item {\bf sparse PCA \cite[Ch.14.5.5]{hastie01statisticallearning}:} requires each PC to depend only on a small number of data attributes $z_{j}$. 
 \vspace*{2mm}
 \item {\bf probabilistic PCA \cite{Roweis98emalgorithms,probabilistic-principal-component-analysis}:} generalizes PCA by using a  {\bf probabilistic (generative) model} for the data. 
\end{itemize}

\section{Linear Discriminant Analysis}

Dimensionality reduction is typically used as a preprocessing step within some overall ML problem such as 
regression or classification. It can then be useful to exploit the availability of labeled data for the design 
of the compression matrix $\mW$ in \eqref{equ_feat_learning_matrix}. However, plain PCA (see Algorithm 
\ref{alg_PCA}) does not make use of any label information provided additionally for the raw data points 
$\vz^{(\sampleidx)} \in \mathbb{R}^{D}$. Therefore, the compression matrix $\mW_{\rm PCA}$ delivered 
by PCA can be highly suboptimal as a pre-processing step for labeled data points. A principled approach 
for choosing the compression matrix $\mW$ such that data points with different labels are well separated 
is {\bf linear discriminant analysis} \cite{hastie01statisticallearning}. 

\section{Random Projections} 
Note that PCA amounts to computing an EVD of the sample covariance matrix 
$\mathbf{Q} = (1/\samplesize) \mathbf{Z} \mathbf{Z}^{T}$ with the data matrix 
$\mathbf{Z} = \big(\vz^{(1)},\ldots,\vz^{(\samplesize)}\big)^{T}$ containing the 
data points $\vz^{(\sampleidx)} \in \mathbb{R}^{D}$ as its columns. The computational 
complexity (amount of multiplications and additions) for computing this PCA is lower 
bounded by $\min\{ D^{2}, \samplesize^{2} \}$ \cite{Du08low-complexityprincipal,Sharma2007}. 
This computational complexity can be prohibitive for ML applications with $\featuredim$ 
and $\samplesize$ being on the order of millions (which is already the case if the features 
are pixel values of a $512 \times 512$ RGB bitmap, see Section \ref{sec_feature_space}). 
There is a surprisingly cheap alternative to PCA for finding a good choice for the 
compression matrix $\mathbf{W}$ in \eqref{equ_feat_learning_matrix}. Indeed, a randomly 
chosen matrix $\mathbf{W}$ with entries drawn i.i.d.\ from a suitable probability distribution 
(such as Bernoulli or Gaussian) yields a good compression matrix $\mW$ (see \eqref{equ_feat_learning_matrix}) 
with high probability \cite{Bingham01randomprojection,jung-specesttit}. 

%???? mention LightOn technology https://arxiv.org/abs/1510.06664?????

\section{Information Bottleneck}
We can use information bottleneck for feature learning. Using Gaussian process model, we even 
get closed-form solutions of Gaussian Information Bottleneck. 


\section{Dimensionality Increase} 
\label{sec_dim_increas} 

Feature learning methods are mainly dimensionality reduction methods. However, it might be 
beneficial to also consider feature learning methods that produce new feature vectors which 
are longer than the raw feature vectors. An extreme example for such a feature map are kernel 
methods which map finite length vector to infinite dimensional spaces. 

Mapping raw feature vectors into higher-dimensional spaces might be useful 
if the intrinsic geometry of the data points is simpler when looked at in the 
higher-dimensional space. Consider a binary classification problem where 
data points are highly inter-winded in the original feature space. By mapping 
into higher-dimensional feature space we might "even-out" this non-linear 
geometry such that we can use linear classifiers in the higher-dimensional space. 

%??? add section on kernel methods which take the other direction, i.e., create longer feature vectors than 
%original raw feature vectors ???

\chapter{Privacy-Preserving ML}
\label{chap_privacy_preserving_ML}

Many ML applications involve data points representing individual humans. 
These data points might include sensitive data, such as medical records, 
which is subject to privacy protection. This chapter discusses some techniques 
for preprocessing the raw data to protect privacy of individuals while still allowing 
to solve the overall ML task. We will illustrate these techniques using a stylized 
healthcare application. 

\begin{figure}[htbp]
	\centering
	\includegraphics[width=8cm]{PrivacyPreservingCartoon.png}
	\caption{Data points represent humans. We are interested in the fruit 
		preference of humans. Their gender is considered sensitive information 
		and should not be revealed to ML methods.}
	\label{fig:ppmlcartoon}
\end{figure}

A key challenge for health-care are pandemics. To optimally manage pandemics 
it is important to have accurate information about the dynamics. We can model 
this as a ML problem with data points representing humans. One key feature of 
data points is if it represents an infected human or not. This data is sensitive 
and typically only available to public health-care institutes. 

Consider the patient database of a hospital which should provide information 
about the average number of infected patients. Instead of directly forward 
the patient files, the hospital must only forward the fraction of infected patients. 
This is an example of privacy-preserving data processing. For a sufficiently large 
number of patients at the hospital (say, more than $1000$), we cannot infer much 
about individual patients just form the fraction of infected patients treated in that 
hospital. 

\section{Privacy-Preserving Feature Learning (Operating on level of individual data points)}

Privacy-preserving ML can be implemented using modification of feature learning 
methods discussed in Chapter \ref{ch_FeatureLearning}. Generic feature learning 
methods aim at learning a compressed representation of the raw data points which 
contain as much information as possible about the quantity of interest. In contrast, 
privacy-preserving ML does not aim at compression but rather obscuring the raw 
data such that it does not reveal sensible information about data points. 


\subsection{Privacy-Preserving Information Bottleneck} %?? info bottleneck with privacy constraints??? 


\subsection{Privacy-Preserving Feature Selection} 
?? ignore features which are sensitive (name, social ID) but not very relevant for actual task (e.g. predicting income).    ???

\subsection{Privacy-Preserving Random Projections} 
?? cheap form: random projections/compressed sensing. random projections blur features of individual 
data points but still allow to learn a sparse linear model using e.g. Lasso ???






\section{Federated Learning (Operates on level of local datasets)}
FL method only exchange model parameter updates; no raw local data is revealed; 

 
\chapter{Explainable ML}
\label{chap_explainable_ML}

A key challenge for the successful deployment of ML methods to 
many (critical) application domain is their explainability. Human users 
of ML seem to have a strong desire to get explanations that resolve 
the uncertainty about predictions and decisions obtained from ML 
methods. Explainable ML enables the user to better predict the 
outcomes of ML methods. 

Explainable ML is challenging since explanations must be tailored 
(personalized) to individual users with varying backgrounds. Some 
users might have received university-level education in ML, while 
other users might have no formal training in linear algebra. Linear 
regression with few features might be perfectly interpretable for 
the first group but might be considered a black-box by the latter. 


?????? 
discuss relation between finding good explanations and active 
learning. Active learning aims at finding data points (by their features) 
which provide most information about the true model parameters. 
XML aims at finding explanations (e.g. data points from training set) 
which provide most information about the prediction provided by 
some black-box ML method. 
?????????????
discuss relation between XML and feature learning. XML can be 
obtained from feature learning methods by learning those subset 
of features which provide most information about the prediction 
(not about the label itself)
?????????????? 


\section{A Model Agnostic Method}
\label{sec_model_agn_xml}

We propose a simple probabilistic model for the predictions and 
user knowledge. This model allows to study explainable ML using 
information theory. Explaining is here considered as the task of 
reducing the ``surprise'' incurred by a prediction. We quantify 
the effect of an explanation by the conditional mutual information 
between the explanation and prediction, given the user background. 

\section{Explainable Empirical Risk Minimization} 
The approach discussed in Section \ref{sec_model_agn_xml} constructs explanations 
for any given ML method such that the user is able to better predict 
the outcome of this ML method. Instead of providing an explanation 
we could also try to make the ML method itself more predictable for 
a user. 


\chapter{Lists of Symbols}

\begin{align} 
&\timeidx  &\quad &\mbox{A discrete time index.} \nonumber \\
&\sampleidx  &\quad &\mbox{Generic index used to enumrate data points in a list of data points. } \nonumber \\
&\samplesize  &\quad &\mbox{The number of different data points in the training set.} \nonumber \\ 
&h(\cdot)  &\quad &\mbox{A predictor that maps a feature vector $\vx$ of a data point to a predicted label $\hat{y}=h(\vx)$.} \nonumber \\ 
&y  &\quad &\mbox{Label of some data point.} \nonumber \\ 
&y^{(\sampleidx)}  &\quad &\mbox{Label of the $i$th data point.} \nonumber \\ 
&\vx  &\quad &\mbox{Feature vector whose entries are the features of some data point.} \nonumber \\ 
&\vx^{(\sampleidx)}  &\quad &\mbox{Feature vector whose entries are the features of the $i$th data point.} \nonumber \\ 
&\featurelen &\quad &\mbox{The number of (real-valued) features of a single data point.} \nonumber
\end{align} 


\chapter{Glossary} 


% \printglossaries 
\begin{itemize}
\item {\bf classification problem}: an ML problem involving a discrete label space $\labelspace$ 
such as $\labelspace\!=\!\{-1,1\}$ for binary classification, or $\labelspace\!=\!\{1,2,\ldots,K\}$ 
with $K\!>\!2$ for multi-class classification. 

\item {\bf classifier}: a hypothesis map $h: \featurespace \rightarrow \labelspace$ with discrete 
label space (e.g., $\labelspace=\{-1,1\}$). 

\item {\bf condition number } $\kappa(\mathbf{Q})$ of a matrix $\mathbf{Q}$: the ratio of largest 
to smallest eigenvalue of a psd matrix $\mathbf{Q}$.

\item {\bf data point}: an elementary unit of information such as a single pixel, a single image, 
a particular audio recording, a letter, a text document or an entire social network user profile. 
\item {\bf dataset}: a collection of data points. 

\item {\bf eigenvalue/eigenvector}: for a square matrix $\mathbf{A} \in \mathbb{R}^{\featuredim \times \featuredim}$ 
we call a non-zero vector $\vx \in \mathbb{R}^{\featuredim}$ an eigenvector of $\mathbf{A}$ if $\mathbf{A} \vx = \lambda \vx$ 
with some $\lambda \in \mathbb{R}$, which we call an eigenvalue of $\mA$. 

\item {\bf features}: any measurements (or quantities) used to characterize a data point (e.g., the 
maximum amplitude of a sound recoding or the greenness of an RGB image). In principle, we 
can use as a feature any quantity which can be measured or computed easily in an automated fashion.  

\item {\bf hypothesis map}: a map (or function) $h: \featurespace \rightarrow \labelspace$ from the 
feature space $\featurespace$ to the label space $\labelspace$. Given a data point with features 
$\vx$ we use a hypothesis map to estimate (or approximate) the label $y$ using the predicted 
label $\hat{y} = h(\vx)$. ML is about automating the search for a good hypothesis 
map such that the error $y - h(\vx)$ is small. 

\item {\bf hypothesis space}: a set of computationally feasible (predictor) maps $h: \featurespace \rightarrow \labelspace$. 

\item {\bf i.i.d.}: independent and identically distributed; e.g., ``$x,y,z$ are i.i.d. random variables'' 
means that the joint probability distribution $p(x,y,z)$ of the random variables $x,y,z$ factors into 
the product $p(x)p(y)p(z)$ with the marginal probability distribution $p(\cdot)$ which is the same 
for all three variables $x,y,z$. 

\item {\bf label}: some property of a data point which is of interest, such as the fact if a webcam 
snapshot shows a forest fire or not. In contrast to features, labels are properties of a data points 
that cannot be measured or computed easily in an automated fashion. Instead, acquiring accurate 
label information often involves human expert labor. Many ML methods aim at learning accurate 
predictor maps that allow to guess or approximate the label of a data point based on its features. 

\item {\bf loss function}: a function which associates a given data point $(\vx,y)$ with features $\vx$ 
and label $y$ and hypothesis map $h$ a number that quantifies the prediction error $y-h(\vx)$. 

\item {\bf positive semi-definite (psd) matrix}: a positive semidefinite matrix $\mQ$, i.e., a symmetric 
matrix $\mQ = \mQ^{T}$ such that $\vx^{T} \mQ \vx \geq 0$ holds for every vector $\vx$. 

\item {\bf predictor}: a hypothesis map $h: \featurespace \rightarrow \labelspace$ with continuous 
label space (e.g., $\labelspace = \mathbb{R}$). 

\item {\bf regression problem}: an ML problem involving a continuous label space $\labelspace$ 
(such as $\labelspace = \mathbb{R}$). 
%which belongs to discrete label space (e.g., $\labelspace = \{-1,1\}$) 

\item {\bf training data}: a dataset which is used for finding a good hypothesis map $h \in \hypospace$ 
out of a hypothesis space $\hypospace$, e.g., via empirical risk minimization (see Chapter \ref{ch_Optimization}). 

\item {\bf validation data}: a dataset which is used for evaluating the quality of a predictor which 
has been learnt using some other (training) data. 
%\item ``$\labelspace^{\featurespace}$'': the set of all maps $h: \featurespace \rightarrow \labelspace$ from the feature-space $\featurespace$ to the label-space $\labelspace$
%is spanned by the columns of $\mA$, i.e., ${\rm span} \mA$
\end{itemize} 
                                    






\bibliographystyle{plain}
\bibliography{/Users/alexanderjung/Literature}

%\bibliography{LitLink}
%BACKMATTER SEE DOCUMENTATION
%\backmatter  % references, restarts sample

%\printbibliography

\end{document}
